<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Week 6: Harmony and melody - Sensing Sound and Music</title><meta property="og:title" content="Week 6: Harmony and melody - Sensing Sound and Music"/><meta name="generator" content="mystmd"/><meta name="description" content="This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python."/><meta property="og:description" content="This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python."/><meta name="keywords" content="sensing, sound, music, psychology, technology"/><meta name="image" content="/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png"/><meta property="og:image" content="/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png"/><link rel="stylesheet" href="/sensingsoundandmusic/build/_assets/app-5WKS5EPQ.css"/><link rel="stylesheet" href="/sensingsoundandmusic/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/sensingsoundandmusic/favicon.ico"/><link rel="stylesheet" href="/sensingsoundandmusic/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/sensingsoundandmusic/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Sensing Sound and Music" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/sensingsoundandmusic/">Sensing Sound and Music</a><a title="Week 1: Tuning in" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week1">Week 1: Tuning in</a><a title="Week 2: Listening" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week2">Week 2: Listening</a><a title="Week 3: Acoustics" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week3">Week 3: Acoustics</a><a title="Week 4: Psychoacoustics" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week4">Week 4: Psychoacoustics</a><a title="Week 5: Time and Rhythm" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week5">Week 5: Time and Rhythm</a><a title="Week 6: Harmony and melody" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week6">Week 6: Harmony and melody</a><a title="Week 7: Body Motion" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week7">Week 7: Body Motion</a><a title="Week 8: The Brain" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week8">Week 8: The Brain</a><a title="Week 9: Vision" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week9">Week 9: Vision</a><a title="Week 10: Physiology" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week10">Week 10: Physiology</a><a title="Week 11: Machine Listening" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/sensingsoundandmusic/week11">Week 11: Machine Listening</a></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><article class="article content article-grid grid-gap"><main class="article-grid subgrid-gap col-screen"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center h-6 mb-5 text-sm font-light"><div class="flex-grow"></div><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="opacity-50 hover:opacity-100 text-inherit hover:text-inherit" aria-label="Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mx-1"><title>Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)</title><path d="M12 2.2c2.7 0 5 1 7 2.9.9.9 1.6 2 2.1 3.1.5 1.2.7 2.4.7 3.8 0 1.3-.2 2.6-.7 3.8-.5 1.2-1.2 2.2-2.1 3.1-1 .9-2 1.7-3.2 2.2-1.2.5-2.5.7-3.7.7s-2.6-.3-3.8-.8c-1.2-.5-2.2-1.2-3.2-2.1s-1.6-2-2.1-3.2-.8-2.4-.8-3.7c0-1.3.2-2.5.7-3.7S4.2 6 5.1 5.1C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C5.6 7.1 5 8 4.6 9c-.4 1-.6 2-.6 3s.2 2.1.6 3c.4 1 1 1.8 1.8 2.6S8 19 9 19.4c1 .4 2 .6 3 .6s2.1-.2 3-.6c1-.4 1.9-1 2.7-1.8 1.5-1.5 2.3-3.3 2.3-5.6 0-1.1-.2-2.1-.6-3.1-.4-1-1-1.8-1.7-2.6C16.1 4.8 14.2 4 12 4zm-.1 6.4l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.5.3-1 .4-1.5.4-.9 0-1.6-.3-2.1-.8-.5-.6-.8-1.3-.8-2.3 0-.9.3-1.7.8-2.2.6-.6 1.3-.8 2.1-.8 1.2 0 2.1.4 2.6 1.4zm5.6 0l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.4.2-.9.3-1.4.3-.9 0-1.6-.3-2.1-.8s-.8-1.3-.8-2.2c0-.9.3-1.7.8-2.2.5-.5 1.2-.8 2-.8 1.2 0 2.1.4 2.6 1.4z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1"><title>Credit must be given to the creator</title><path d="M12 2.2c2.7 0 5 .9 6.9 2.8 1.9 1.9 2.8 4.2 2.8 6.9s-.9 5-2.8 6.8c-2 1.9-4.3 2.9-7 2.9-2.6 0-4.9-1-6.9-2.9-1.8-1.7-2.8-4-2.8-6.7s1-5 2.9-6.9C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C4.8 8 4 9.9 4 12c0 2.2.8 4 2.4 5.6C8 19.2 9.8 20 12 20c2.2 0 4.1-.8 5.7-2.4 1.5-1.5 2.3-3.3 2.3-5.6 0-2.2-.8-4.1-2.3-5.7C16.1 4.8 14.2 4 12 4zm2.6 5.6v4h-1.1v4.7h-3v-4.7H9.4v-4c0-.2.1-.3.2-.4.1-.2.2-.2.4-.2h4c.2 0 .3.1.4.2.2.1.2.2.2.4zm-4-2.5c0-.9.5-1.4 1.4-1.4s1.4.5 1.4 1.4c0 .9-.5 1.4-1.4 1.4s-1.4-.5-1.4-1.4z"></path></svg></a><a href="https://en.wikipedia.org/wiki/Open_access" target="_blank" rel="noopener noreferrer" title="Open Access" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="mr-1 inline-block opacity-60 hover:opacity-100 hover:text-[#E18435]"><path d="M17.1 12.6h-2V7.5c0-1.7-1.4-3.1-3-3.1-.8 0-1.6.3-2.2.9-.6.5-.9 1.3-.9 2.2v.7H7v-.7c0-1.4.5-2.7 1.5-3.7s2.2-1.5 3.6-1.5 2.6.5 3.6 1.5 1.5 2.3 1.5 3.7v5.1z"></path><path d="M12 21.8c-.8 0-1.6-.2-2.3-.5-.7-.3-1.4-.8-1.9-1.3-.6-.6-1-1.2-1.3-2-.3-.8-.5-1.6-.5-2.4s.2-1.6.5-2.4c.3-.7.7-1.4 1.3-2s1.2-1 1.9-1.3c.7-.3 1.5-.5 2.3-.5.8 0 1.6.2 2.3.5.7.3 1.4.8 1.9 1.3.6.6 1 1.2 1.3 2 .3.8.5 1.6.5 2.4s-.2 1.6-.5 2.4c-.3.7-.7 1.4-1.3 2-.6.6-1.2 1-1.9 1.3-.7.3-1.5.5-2.3.5zm0-10.3c-2.2 0-4 1.8-4 4.1s1.8 4.1 4 4.1 4-1.8 4-4.1-1.8-4.1-4-4.1z"></path><circle cx="12" cy="15.6" r="1.7"></circle></svg></a><a href="https://github.com/fourMs/sensingsoundandmusic" title="GitHub Repository: fourMs/sensingsoundandmusic" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><div class="inline-block mr-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block"><title>Jupyter Notebook</title><path d="M20.2 1.7c0 .8-.5 1.4-1.3 1.5-.8 0-1.4-.5-1.5-1.3 0-.8.5-1.4 1.3-1.5.8-.1 1.5.5 1.5 1.3zM12 17.9c-3.7 0-7-1.3-8.7-3.3 1.8 4.8 7.1 7.3 11.9 5.5 2.5-.9 4.5-2.9 5.5-5.5-1.7 2-4.9 3.3-8.7 3.3zM12 5.1c3.7 0 7 1.3 8.7 3.3-1.8-4.8-7.1-7.3-11.9-5.5-2.5.9-4.5 2.9-5.5 5.5 1.7-2 5-3.3 8.7-3.3zM6.9 21.8c.1 1-.7 1.8-1.7 1.9-1 .1-1.8-.7-1.9-1.7 0-1 .7-1.8 1.7-1.9 1-.1 1.8.7 1.9 1.7zM3.7 4.6c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1c0 .5-.4 1-1 1z"></path></svg></div><a href="https://github.com/fourMs/sensingsoundandmusic/edit/main/book/week6.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Week 6: Harmony and melody</h1><p class="mt-2 mb-0 lead text-zinc-600 dark:text-zinc-400">Exploring harmony, melody, and musical structure</p><header class="mt-4 not-prose"><div class="grid grid-cols-1 sm:grid-cols-2 gap-y-1"><div><span class="font-semibold text-sm"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R2t8top:" data-state="closed">Alexander Refsum Jensenius</button></span></div><div class="text-sm"><div>University of Oslo<!-- --> </div></div></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="c5SXQlLeTE" class="relative group/block"><p>Last week, we explored the importance of timing and rhythm in sound and music. This week, our focus shifts to how frequencies interact both “horizontally” and “vertically.” Horizontal organization of frequencies gives rise to melodies; sequences of pitches unfolding over time, often shaped by rhythmic patterns. Vertical organization, on the other hand, leads to the formation of intervals, harmonies, and textures, as multiple pitches sound together or overlap. Understanding these dimensions is essential for analyzing how music conveys structure, emotion, and complexity. First, however, we need to define the concept of “tone”.</p><h2 id="tones" class="relative group"><span class="heading-text">Tones</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#tones" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>In music psychology, a <em>tone</em> is understood as a sound with a specific frequency and timbral quality that the auditory system interprets as having a definite pitch. Our perception of tones is shaped by both the physical properties of the sound wave (such as frequency, amplitude, and harmonic content) and the way our brains process these signals. Tones are the building blocks of musical perception, allowing us to distinguish melodies, harmonies, and timbres.</p><p>From a technological perspective, tones can be generated, analyzed, and manipulated using digital tools. Synthesizers create tones by combining waveforms, while audio analysis software can extract pitch and timbre features from recordings. Technologies such as pitch detection algorithms and spectral analysis are essential for applications in music information retrieval, automatic transcription, and digital instrument design.</p></div><div id="IwEmgengrf" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><details class="myst-dropdown rounded-md my-5 shadow dark:shadow-2xl dark:shadow-neutral-900 overflow-hidden bg-gray-50 dark:bg-stone-800"><summary class="myst-dropdown-header m-0 text-lg font-medium py-1 min-h-[2em] pl-3 cursor-pointer hover:shadow-[inset_0_0_0px_30px_#00000003] dark:hover:shadow-[inset_0_0_0px_30px_#FFFFFF03] bg-gray-100 dark:bg-slate-900"><span class="myst-dropdown-header-title text-neutral-900 dark:text-white"><span class="block float-right text-sm font-thin text-neutral-700 dark:text-neutral-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-dropdown-header-icon inline-block pl-2 mr-2 -translate-y-[1px] details-toggle transition-transform"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></span>Source</span></summary><div class="myst-dropdown-body px-4 py-1 details-body"><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import numpy as np

sr = 22050  # or whatever sample rate you are using
duration = 2.0  # seconds, or whatever duration you want
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Define a fundamental frequency
f0 = 440  # A4, 440 Hz

# Pure tone (sine wave)
pure = np.sin(2 * np.pi * f0 * t)

# Harmonics (sum of first 5 harmonics)
harmonics = sum([np.sin(2 * np.pi * f0 * (n+1) * t) / (n+1) for n in range(5)])

# Complex tone (sum of odd harmonics, like a square wave)
complex_tone = sum([np.sin(2 * np.pi * f0 * (2*n+1) * t) / (2*n+1) for n in range(5)])

# Set new envelope times (in seconds)
attack_time = 0.01   # very rapid attack
decay_time = 0.02    # very rapid decay
sustain_time = 0.9   # much longer sustain
release_time = 0.2   # longer release

# Convert times to sample lengths
attack_len = int(attack_time * sr)
decay_len = int(decay_time * sr)
sustain_len = int(sustain_time * sr)
release_len = int(release_time * sr)

# Create envelope with decay going down to 0.2
attack = np.linspace(0, 1, attack_len, endpoint=False)                    # Attack
decay = np.linspace(1, 0.2, decay_len, endpoint=False)                    # Decay (to 0.2)
sustain = np.full(sustain_len, 0.2)                                       # Sustain
release = np.linspace(0.2, 0, release_len, endpoint=True)                 # Release

envelope = np.concatenate([attack, decay, sustain, release])

# Ensure envelope matches signal length
if len(envelope) &lt; len(t):
    envelope = np.concatenate([envelope, np.zeros(len(t) - len(envelope))])
else:
    envelope = envelope[:len(t)]

# Apply envelope to signals (assuming pure, harmonics, complex_tone are numpy arrays)
pure_env = pure * envelope
harmonics_env = harmonics * envelope
complex_tone_env = complex_tone * envelope
signals_env = [pure_env, harmonics_env, complex_tone_env]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div></div></details><div data-mdast-node-id="Eb2Z0ZpgWQaNbZJaDjW2p" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="APO2i8TrHM" class="relative group/block"><p>Tones are not the same as notes: while a <em>note</em> refers to a symbolic representation in musical notation (with defined pitch, duration, and sometimes dynamics), a <em>tone</em> refers to the auditory experience itself.</p><aside class="myst-admonition myst-admonition-note my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden myst-admonition-default rounded border-l-4 border-blue-500"><div class="myst-admonition-header m-0 font-medium py-1 flex min-w-0 text-lg text-blue-600 bg-blue-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="myst-admonition-header-icon inline-block pl-2 mr-2 self-center flex-none text-blue-600"><path stroke-linecap="round" stroke-linejoin="round" d="m11.25 11.25.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Zm-9-3.75h.008v.008H12V8.25Z"></path></svg><div class="myst-admonition-header-text text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="myst-admonition-body px-4 py-1"><p>In the following, we will use an <a href="https://en.wikipedia.org/wiki/Analysis_by_synthesis" class="italic" target="_blank" rel="noreferrer" data-state="closed">analysis-by-synthesis</a> technique to understand more about the different concepts. This is a method used in sound and music research to understand auditory perception by recreating sounds and analyzing their properties. This approach is widely used in areas like speech synthesis, sound design, and music analysis.</p></div></aside></div><div id="dblvLhqqRx" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><details class="myst-dropdown rounded-md my-5 shadow dark:shadow-2xl dark:shadow-neutral-900 overflow-hidden bg-gray-50 dark:bg-stone-800"><summary class="myst-dropdown-header m-0 text-lg font-medium py-1 min-h-[2em] pl-3 cursor-pointer hover:shadow-[inset_0_0_0px_30px_#00000003] dark:hover:shadow-[inset_0_0_0px_30px_#FFFFFF03] bg-gray-100 dark:bg-slate-900"><span class="myst-dropdown-header-title text-neutral-900 dark:text-white"><span class="block float-right text-sm font-thin text-neutral-700 dark:text-neutral-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-dropdown-header-icon inline-block pl-2 mr-2 -translate-y-[1px] details-toggle transition-transform"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></span>Source</span></summary><div class="myst-dropdown-body px-4 py-1 details-body"><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import music21

import matplotlib.pyplot as plt

# Plot the complex tone waveform
fig, axs = plt.subplots(1, 2, figsize=(12, 3), gridspec_kw={&#x27;width_ratios&#x27;: [2, 1]})

# Waveform plot
axs[0].plot(t, complex_tone_env)
axs[0].set_title(&#x27;Complex Tone (with Envelope)&#x27;)
axs[0].set_xlabel(&#x27;Time (s)&#x27;)
axs[0].set_ylabel(&#x27;Amplitude&#x27;)

# Create a single musical note (e.g., A4)
note = music21.note.Note(&#x27;A4&#x27;)
note.quarterLength = 1.0
stream = music21.stream.Stream([note])

# Render the musical note in the second subplot (as an image)
# Save the note as an image and display it
img_path = stream.write(&#x27;lily.png&#x27;)
img = plt.imread(img_path)
axs[1].imshow(img)
axs[1].axis(&#x27;off&#x27;)
axs[1].set_title(&#x27;Musical Note (A4)&#x27;)

plt.tight_layout()
plt.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div></div></details><div data-mdast-node-id="scpdopmrVkqUG9dhSv3ia" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Changing working directory to: `/tmp/music21&#x27;
Processing `/tmp/music21/tmplwsgg5xf.ly&#x27;
Parsing...
/tmp/music21/tmplwsgg5xf.ly:25:5: error: unknown escaped string: `\RemoveEmptyStaffContext&#x27;
    
    \RemoveEmptyStaffContext
/tmp/music21/tmplwsgg5xf.ly:26:5: error: syntax error, unexpected \override, expecting &#x27;=&#x27;
    
    \override VerticalAxisGroup #&#x27;remove-first = ##t
/tmp/music21/tmplwsgg5xf.ly:26:33: warning: deprecated: missing `.&#x27; in property path VerticalAxisGroup.remove-first
    \override VerticalAxisGroup 
                                #&#x27;remove-first = ##t
/tmp/music21/tmplwsgg5xf.ly:28:2: error: syntax error, unexpected &#x27;}&#x27;
 
 }
/tmp/music21/tmplwsgg5xf.ly:28:3: error: Unfinished main input
 }
  
Interpreting music...
Preprocessing graphical objects...
Calculating line breaks... 
Drawing systems... 
Converting to PNG...
fatal error: failed files: &quot;/tmp/music21/tmplwsgg5xf.ly&quot;
</span></code></pre></div><img src="/sensingsoundandmusic/build/57f4854fc995277d7cccbf6ae6db6492.png" alt="&lt;Figure size 1200x300 with 2 Axes&gt;"/></div></div><div id="dc9d2O9dEP" class="relative group/block"><h2 id="pitch" class="relative group"><span class="heading-text">Pitch</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#pitch" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p><em>Pitch</em> is the psycho-physiological correlate of frequency that lets us hear one sound as higher or lower than another. It is closely related to frequency---the rate at which a waveform repeats---but the relationship is not one-to-one. As you may recall from the Acoustics chapter, most instrument tones are not pure sine waves. Musical, ‘pitched’ instruments generally produce a fundamental frequency and many additional frequencies (overtones). This is because pitched musical instruments are often based on an acoustic resonator, which oscillates at numerous frequencies simultaneously, mostly limited to integer multiples, or harmonics, of the lowest (fundamental) frequency, and such multiples form a harmonic series. However, the individual partials of a complex sound are typically not perceived as separate; our perceptual system fuses them together, leading us to experience a unitary sound</p><p>The pitch of harmonic tones generally corresponds to the fundamental frequency (f₀). However, the brain can infer a fundamental frequency (and thus perceive pitch) from complex tones even when a fundamental component is absent. This is called <em>virtual pitch</em> or the <em>missing fundamental</em> and it is related to the phenomenon whereby one’s brain extracts tones from everyday signals, even if parts of the signal are masked by other sounds</p><p>Below is a sequence of simple (sine) tones distinct at different frequencies (<a target="_blank" rel="noreferrer" href="https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2&amp;dl=0" class="">[Audio Ex. 1]{.underline}</a>).</p><img id="HQymENylfp" style="margin:0 auto" src="/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png" data-canonical-url="figures/week6_image2.png" class=""/><p>If we add some partials (multiples) below each tone (<a target="_blank" rel="noreferrer" href="https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2&amp;dl=0" class="">Audio example 2</a>), can you start to hear a familiar melody?</p><img id="hYZOe7Vw2A" style="margin:0 auto" src="/sensingsoundandmusic/build/week6_image4-fac934d9fdb26d8b5be4765f37a3b3f3.png" data-canonical-url="figures/week6_image4.png" class=""/><p>If we then rearrange these partials slightly (<a target="_blank" rel="noreferrer" href="https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2&amp;dl=0" class="">[Audio Example 3]{.underline}</a>), we can induce an even stronger sense of virtual pitch---various missing fundamental frequencies which are not physically present in the tones themselves:</p><img id="CnPiw6Bjzs" style="margin:0 auto" src="/sensingsoundandmusic/build/week6_image1-d21c6b7d42de5162c042527722e83ce7.png" data-canonical-url="figures/week6_image1.png" class=""/><p><em>Image source</em>: Toiviainen, P. (2015). Lecture materials for Music Perception. University of Jyväskylä.</p><h3 id="pitch-class" class="relative group"><span class="heading-text">Pitch class</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#pitch-class" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>In music, a pitch class (or “chroma”) is a set of pitches that are a whole number of octaves apart, e.g., the pitch class C consists of the Cs in all octaves. Humans perceive the notes in a tonal scale as repeating once per octave. This provides the basis for producing &amp; perceiving melodic patterns based on relative pitch relationships---that is, <em>relative</em> to a pitch class. Absolute pitch ability, on the other hand, is the ability to recognize (or reproduce) specific pitches without the help of a reference pitch and pitch class.</p><img id="OH0okrnHfX" style="margin:0 auto" src="/sensingsoundandmusic/build/week6_image3-fb606222ac409a6c1a24a8e81bcdc20b.png" data-canonical-url="figures/week6_image3.png" class=""/><p><em>Image Source</em>: Trainor, Laurel &amp; Unrau, A.J.. (2012). Development of pitch and music perception. Springer Handbook of Auditory Research: Human Auditory Development. 223-254.</p><h3 id="tonality-and-harmonic-expectation" class="relative group"><span class="heading-text">Tonality and harmonic expectation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#tonality-and-harmonic-expectation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><em>Tonality</em> refers to the tendency for tones to resolve to a fundamental tonic note---a hierarchy of tones centered on a tonic (scale). It is pne of the main conceptual categories in Western music &amp; musical thought, and corresponds to <em>key signatures</em> in musical notation (C major, C minor). Some tones feel stable in a melodic-harmonic sequence or scale, others seem to want to resolve. Knowledge of tonality does not require formal training in music theory---it is learned implicitly, by virtue of exposure to the music of one’s culture. This learning-by-exposure causes us to store knowledge about key as a cognitive schema. Tonality is also supported by acoustics, in the sense that the most crucial notes in a scale (3rd and 5th) tend to share partials with the tonic.</p></div><div id="ysWGWKwI7k" class="relative group/block"><h2 id="harmony" class="relative group"><span class="heading-text">Harmony</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#harmony" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p><a href="https://en.wikipedia.org/wiki/Harmony" class="italic" target="_blank" rel="noreferrer" data-state="closed">Harmony</a> involves the combination of discernible tones to create intervals and chords. It is the simultaneous sounding of different pitches, which can evoke a wide range of emotional responses. Harmony plays a crucial role in the emotional tone and complexity of music. Harmony encompasses the interaction of pitches through intervals and chords, shaped by timbre and texture.</p><h3 id="intervals" class="relative group"><span class="heading-text">Intervals</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#intervals" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>An <em>interval</em> is the distance between two pitches, measured in steps or frequency ratios. Intervals are the building blocks of harmony, as they define the relationships between notes played together or in succession. The human brain is sensitive to the relationships between pitches, perceiving certain combinations as consonant (pleasant or stable) and others as dissonant (tense or unstable). These perceptual responses are influenced by cultural exposure, musical training, and innate auditory processing mechanisms.</p><ul><li><p><strong>Types of Intervals:</strong> Intervals are named by counting the number of letter names from the lower to the higher note (e.g., C to E is a third). They can be major, minor, perfect, augmented, or diminished.</p></li><li><p><strong>Consonance and Dissonance:</strong> Some intervals, like octaves and perfect fifths, are perceived as consonant, while others, like minor seconds or tritones, are more dissonant.</p></li><li><p><strong>Role in Harmony:</strong> Intervals form the basis for chords and harmonic progressions. The combination of intervals within a chord determines its character and function.</p></li></ul><h3 id="chords" class="relative group"><span class="heading-text">Chords</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#chords" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>A <em>chord</em> is a group of three or more notes played simultaneously. Chords are the foundation of Western harmony and are used to create progressions that define the structure and mood of a piece.</p><ul><li><p><strong>Triads:</strong> The most basic chords, consisting of three notes (root, third, fifth). Types include major, minor, diminished, and augmented triads.</p></li><li><p><strong>Seventh Chords and Extensions:</strong> Adding more notes (such as sevenths, ninths, elevenths, and thirteenths) creates richer harmonies.</p></li><li><p><strong>Chord Progressions:</strong> Sequences of chords that create movement and tension-resolution patterns in music (e.g., I–IV–V–I in classical music, or ii–V–I in jazz).</p></li><li><p><strong>Functional Harmony:</strong> Chords have roles (tonic, dominant, subdominant) that guide the listener’s expectations.</p></li></ul><h3 id="timbre" class="relative group"><span class="heading-text">Timbre</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#timbre" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Recall that <a href="https://en.wikipedia.org/wiki/Timbre" class="italic" target="_blank" rel="noreferrer" data-state="closed">timbre</a>, often called “sound color,” is the quality of a sound that distinguishes different instruments or voices, even when they produce the same pitch and loudness.</p><ul><li><p><strong>“Sound Color”:</strong> The unique quality that makes a violin sound different from a flute, even if both play the same note at the same loudness.</p></li><li><p><strong>Spectral Content:</strong> Timbre is shaped by the harmonic content (overtones) and the way energy is distributed across frequencies.</p></li><li><p><strong>Temporal Alignment:</strong> The timing of sound waves, including attack, decay, sustain, and release, contributes to timbre perception.</p></li><li><p><strong>Just Noticeable Differences (JND):</strong> The smallest change in a sound property (such as frequency, amplitude, or spectral content) that can be perceived.</p></li></ul><h3 id="texture" class="relative group"><span class="heading-text">Texture</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#texture" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/Texture_(music)" class="italic" target="_blank" rel="noreferrer" data-state="closed">Texture</a> describes how multiple layers of sound interact in a musical composition. It ranges from monophonic (a single melody) to polyphonic (multiple independent melodies).</p><ul><li><p><strong>Monophonic Texture:</strong> A single melodic line without accompaniment (e.g., solo singing).</p></li><li><p><strong>Homophonic Texture:</strong> A main melody supported by chords or accompaniment (e.g., singer with guitar).</p></li><li><p><strong>Polyphonic Texture:</strong> Two or more independent melodies played simultaneously (e.g., fugues, counterpoint).</p></li><li><p><strong>Heterophonic Texture:</strong> Multiple performers play variations of the same melody at the same time.</p></li><li><p><strong>Combination of Timbres:</strong> The blending of different sound qualities to create a rich texture.</p></li></ul></div><div id="q1QBFW3upT" class="relative group/block"><h2 id="melody" class="relative group"><span class="heading-text">Melody</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#melody" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p><a href="https://en.wikipedia.org/wiki/Melody" class="italic" target="_blank" rel="noreferrer" data-state="closed">Melody</a> is the sequence of musical notes that are perceived as a single entity. It is often the most recognizable and memorable aspect of a musical piece. Melody plays a crucial role in emotional engagement and memory recall. Tools like MIDI editors and pitch detection algorithms are used to analyze and manipulate melodies.</p></div><div id="pf1T34b8AI" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import music21
music21.environment.UserSettings()[&#x27;musescoreDirectPNGPath&#x27;] = &#x27;/usr/bin/mscore3&#x27;

# Create a simple melody: C D E F G F E D C
melody_notes = [&#x27;C4&#x27;, &#x27;D4&#x27;, &#x27;E4&#x27;, &#x27;F4&#x27;, &#x27;G4&#x27;, &#x27;F4&#x27;, &#x27;E4&#x27;, &#x27;D4&#x27;, &#x27;C4&#x27;]
melody = music21.stream.Stream()
for n in melody_notes:
    melody.append(music21.note.Note(n, quarterLength=0.5))

# Show the musical score (this will render in Jupyter if MuseScore or similar is installed)
melody.show()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="y6RWV3Fyscy7wEzhK0YmM" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/sensingsoundandmusic/build/fcec387afe67a7ab7a8a4d0a0ac747ff.png" alt="&lt;IPython.core.display.Image object&gt;"/></div></div><div id="mGDx6yn9ZC" class="relative group/block"><p>From a psychological perspective, <em>melody</em> is the perception of a coherent sequence of tones that form a recognizable musical line. Melodies are central to musical memory and emotional response, as the brain tracks pitch contours, intervals, and rhythmic patterns to identify and recall tunes. Research in music cognition explores how listeners segment, remember, and anticipate melodic sequences.</p><p>Technological advances have enabled detailed analysis and manipulation of melody. Pitch tracking algorithms extract melodic lines from audio, while MIDI editors and music notation software allow for precise editing and visualization. In music generation and AI composition, models learn melodic patterns from large datasets to create new, stylistically consistent melodies. Melody extraction and similarity algorithms are also used in music search and recommendation systems.</p><h3 id="auditory-stream-segregation" class="relative group"><span class="heading-text">Auditory stream segregation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#auditory-stream-segregation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Auditory stream segregation is the process by which the human auditory system organizes complex mixtures of sounds into perceptually meaningful elements, or “streams.” In music, this allows listeners to distinguish between different melodic lines, instruments, or voices, even when they are played simultaneously. This perceptual organization is influenced by factors such as pitch, timbre, spatial location, and timing. For example, melodies that move in different pitch ranges or have distinct timbres are more likely to be perceived as separate streams. Understanding auditory stream segregation is essential for analyzing polyphonic music, designing effective music information retrieval systems, and developing algorithms for source separation and automatic transcription. Advances in computational modeling and machine learning have enabled researchers to simulate and study how the brain separates and tracks multiple musical streams in real-world listening scenarios.</p><p>The foundational work of psychologist Albert S. Bregman, particularly his book <em>Auditory Scene Analysis</em> (1990), established the theoretical framework for understanding how the auditory system parses complex acoustic environments. Bregman introduced the concept of “auditory scene analysis” (ASA), describing how the brain groups and segregates sounds based on cues such as frequency proximity, temporal continuity, common onset/offset, and timbral similarity. According to Bregman, these grouping principles allow us to perceive coherent musical lines and separate voices in polyphonic music, even when their acoustic signals overlap. His research has had a profound influence on music psychology, cognitive science, and the development of computational models for music and audio processing.</p><h3 id="gestalt-theory" class="relative group"><span class="heading-text">Gestalt theory</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#gestalt-theory" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Gestalt theory, originating from early 20th-century psychology, describes how humans naturally organize sensory information into meaningful patterns and wholes. In music perception, Gestalt principles help explain how listeners group sequences of notes into coherent melodies, phrases, and motifs, even when the underlying acoustic signals are complex or ambiguous.</p><p>Key Gestalt principles relevant to music include:</p><ul><li><p><strong>Proximity:</strong> Notes that are close together in time or pitch are perceived as belonging to the same melodic group.</p></li><li><p><strong>Similarity:</strong> Notes with similar timbre, loudness, or articulation are grouped together.</p></li><li><p><strong>Continuity:</strong> The brain tends to perceive smooth, continuous melodic lines rather than abrupt jumps.</p></li><li><p><strong>Closure:</strong> Listeners mentally “fill in” gaps to perceive complete musical phrases, even if some notes are missing.</p></li><li><p><strong>Figure-Ground:</strong> The ability to focus on a primary melody (figure) while treating accompaniment or background sounds as secondary (ground).</p></li></ul><p>These principles interact with auditory stream segregation, enabling us to follow individual voices in polyphonic music, recognize recurring themes, and make sense of complex musical textures. Gestalt theory has influenced both music psychology and computational models for music analysis, providing a framework for understanding how we perceive structure and form in music.</p><h3 id="auditory-illusions" class="relative group"><span class="heading-text">Auditory illusions</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#auditory-illusions" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/Diana_Deutsch" class="italic" target="_blank" rel="noreferrer" data-state="closed">Diana Deutsch</a> is a renowned psychologist and researcher who has made significant contributions to the study of auditory illusions and the psychology of music. Her experiments have uncovered a variety of perceptual phenomena that reveal how our brains organize and interpret complex sound patterns. Some of her most famous auditory illusions include:</p><ul><li><p><strong>The Tritone Paradox</strong>: When two tones separated by a tritone (half an octave) are played in succession, some listeners perceive the sequence as ascending in pitch, while others hear it as descending. The direction of the perceived pitch change can vary depending on the listener’s linguistic background and even their geographical origin, suggesting that pitch perception is influenced by both biology and experience.</p></li><li><p><strong>The Octave Illusion</strong>: When two tones an octave apart are alternately played to each ear (for example, high tone to the right ear, low tone to the left, then switching), listeners often perceive a single tone that alternates between ears and changes pitch, even though both tones are always present. This illusion demonstrates how the brain integrates and separates auditory information from both ears.</p></li><li><p><strong>The Scale Illusion</strong>: When ascending and descending musical scales are split between the two ears (with some notes sent to the left ear and others to the right), listeners tend to perceive coherent melodic lines that do not correspond to the actual physical input. The brain “reconstructs” the most plausible musical pattern, illustrating its tendency to organize sounds into familiar structures.</p></li><li><p><strong>Phantom Words</strong>: In this illusion, repeating ambiguous speech sounds can cause listeners to “hear” words or phrases that are not actually present. The specific words perceived can vary between individuals, highlighting the role of expectation, language, and context in auditory perception.</p></li></ul><p>Deutsch’s work demonstrates that auditory perception is not a simple reflection of the physical properties of sound, but an active process shaped by cognitive, cultural, and neural factors. Her illusions are widely used in research, education, and demonstrations to illustrate the complexities of how we hear and interpret sound.</p><p>For more examples and audio demonstrations, visit <a target="_blank" rel="noreferrer" href="https://deutsch.ucsd.edu/psychology/pages.php?i=201" class="">Diana Deutsch’s Auditory Illusions website</a>.</p><div style="text-align:center" class="leading-[0]"><div class="relative inline-block" style="padding-bottom:60%;width:min(max(100%, 500px), 100%)"><iframe width="100%" height="100%" src="https://www.youtube.com/embed/oF0g-UUqzgg?si=d5Id2ihd_KKVuGjt" allowfullscreen="" allow="autoplay" style="width:100%;height:100%;position:absolute;top:0;left:0;border:none"></iframe></div></div></div><div id="HT25G7sDPd" class="relative group/block"><h2 id="audio-visualizations" class="relative group"><span class="heading-text">Audio Visualizations</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#audio-visualizations" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>We have previously looked at <a href="https://en.wikipedia.org/wiki/Waveform" class="italic" target="_blank" rel="noreferrer" data-state="closed">waveform</a> displays and <a href="https://en.wikipedia.org/wiki/Spectrogram" class="italic" target="_blank" rel="noreferrer" data-state="closed">spectrograms</a>. However, there are also several other visualization forms that try to better capture what humans hear.</p><p>Visualizing audio is crucial for understanding both the physical properties of sound and how humans perceive it. Different representations highlight various aspects of the audio signal, making them useful for analysis, classification, and creative applications.</p><h3 id="waveform" class="relative group"><span class="heading-text">Waveform</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#waveform" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>A waveform is a simple plot of amplitude versus time. It shows how the air pressure (or voltage, in digital audio) changes over time. While useful for seeing the overall shape and dynamics of a sound, it does not provide detailed information about frequency content.</p><h3 id="spectrogram" class="relative group"><span class="heading-text">Spectrogram</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#spectrogram" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>A spectrogram displays how the frequency content of a signal changes over time. It is created by applying the Short-Time Fourier Transform (STFT) to the audio, resulting in a 2D image where the x-axis is time, the y-axis is frequency, and the color represents amplitude (often in decibels). Spectrograms are widely used for audio analysis, speech recognition, and music research.</p><h3 id="log-mel-spectrogram" class="relative group"><span class="heading-text">Log Mel Spectrogram</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#log-mel-spectrogram" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>The <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" class="italic" target="_blank" rel="noreferrer" data-state="closed">Log Mel Spectrogram</a> mimics human hearing by applying the STFT, mapping the frequencies to the Mel scale (which is more perceptually relevant), and then applying a logarithmic transformation to represent the amplitude on a decibel scale. This representation compresses the frequency axis to better match how humans perceive pitch differences, making it especially useful in machine learning and audio classification tasks.</p><h3 id="mfccs-mel-frequency-cepstral-coefficients" class="relative group"><span class="heading-text">MFCCs (Mel-Frequency Cepstral Coefficients)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#mfccs-mel-frequency-cepstral-coefficients" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" class="italic" target="_blank" rel="noreferrer" data-state="closed">MFCCs</a> are a compact representation of the spectral envelope of a sound. They are computed by taking the Log Mel Spectrogram and applying the Discrete Cosine Transform (DCT), which decorrelates the features and compresses the information. MFCCs are widely used in speech recognition, music classification, and audio similarity tasks because they capture timbral characteristics that are important for distinguishing different sounds.</p><h3 id="cqt-constant-q-transform" class="relative group"><span class="heading-text">CQT (Constant-Q Transform)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#cqt-constant-q-transform" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>The <a href="https://en.wikipedia.org/wiki/Constant-Q_transform" class="italic" target="_blank" rel="noreferrer" data-state="closed">Constant-Q Transform</a> uses a logarithmic frequency scale, with exponentially spaced center frequencies and varying filter bandwidths. This makes it ideal for musical applications, as it aligns with the way musical notes are spaced (e.g., each octave is divided into equal steps). The CQT is particularly useful for tasks like pitch tracking, chord recognition, and music transcription, as it provides a more musically meaningful frequency representation than the linear STFT.</p><h3 id="summary-table" class="relative group"><span class="heading-text">Summary Table</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#summary-table" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><table class=""><tbody><tr class=""><th class="">Visualization</th><th class="">What it shows</th><th class="">Typical Use Cases</th></tr><tr class=""><td class="">Waveform</td><td class="">Amplitude vs. time</td><td class="">Editing, dynamics, onset detection</td></tr><tr class=""><td class="">Spectrogram</td><td class="">Frequency vs. time (linear)</td><td class="">Audio analysis, speech/music research</td></tr><tr class=""><td class="">Log Mel Spectrogram</td><td class="">Perceptual frequency vs. time</td><td class="">Machine learning, audio classification</td></tr><tr class=""><td class="">MFCCs</td><td class="">Compressed spectral envelope</td><td class="">Speech/music recognition, feature extraction</td></tr><tr class=""><td class="">CQT</td><td class="">Musical pitch vs. time</td><td class="">Pitch tracking, chord recognition, MIR</td></tr></tbody></table><p>These visualizations are implemented in Python using libraries such as <code>librosa</code> and <code>matplotlib</code>, as shown in the code below. Each representation provides unique insights into the structure and content of audio signals, supporting both scientific analysis and creative exploration.</p></div><div id="Zz8pBeKRF8" class="relative group/block"><div class="flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><details class="myst-dropdown rounded-md my-5 shadow dark:shadow-2xl dark:shadow-neutral-900 overflow-hidden bg-gray-50 dark:bg-stone-800"><summary class="myst-dropdown-header m-0 text-lg font-medium py-1 min-h-[2em] pl-3 cursor-pointer hover:shadow-[inset_0_0_0px_30px_#00000003] dark:hover:shadow-[inset_0_0_0px_30px_#FFFFFF03] bg-gray-100 dark:bg-slate-900"><span class="myst-dropdown-header-title text-neutral-900 dark:text-white"><span class="block float-right text-sm font-thin text-neutral-700 dark:text-neutral-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-dropdown-header-icon inline-block pl-2 mr-2 -translate-y-[1px] details-toggle transition-transform"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></span>Source</span></summary><div class="myst-dropdown-body px-4 py-1 details-body"><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import numpy as np
import librosa

import matplotlib.pyplot as plt
import librosa.display

# Generate a test audio signal (sine wave + harmonics)
sr = 22050  # sample rate
duration = 2.0  # seconds
t = np.linspace(0, duration, int(sr * duration), endpoint=False)
f_start = 0
f_end = 20000
audio = 0.5 * np.sin(2 * np.pi * ((f_start + (f_end - f_start) * t / duration) * t))

fig, axs = plt.subplots(3, 2, figsize=(14, 12))
fig.suptitle(&#x27;Audio Representations&#x27;, fontsize=16)

# Waveform
librosa.display.waveshow(audio, sr=sr, ax=axs[0, 0])
axs[0, 0].set_title(&#x27;Waveform&#x27;)
axs[0, 0].set_xlabel(&#x27;&#x27;)
axs[0, 0].set_ylabel(&#x27;Amplitude&#x27;)

# Spectrogram
S = np.abs(librosa.stft(audio, n_fft=1024, hop_length=256))
librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), sr=sr, hop_length=256, x_axis=&#x27;time&#x27;, y_axis=&#x27;hz&#x27;, ax=axs[0, 1])
axs[0, 1].set_title(&#x27;Spectrogram (dB)&#x27;)

# Mel Spectrogram
S_mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=256, n_mels=64)
librosa.display.specshow(librosa.power_to_db(S_mel, ref=np.max), sr=sr, hop_length=256, x_axis=&#x27;time&#x27;, y_axis=&#x27;mel&#x27;, ax=axs[1, 0])
axs[1, 0].set_title(&#x27;Mel Spectrogram (dB)&#x27;)

# MFCC
mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=1024, hop_length=256)
librosa.display.specshow(mfccs, x_axis=&#x27;time&#x27;, ax=axs[1, 1])
axs[1, 1].set_title(&#x27;MFCC&#x27;)

# CQT
C = np.abs(librosa.cqt(audio, sr=sr, hop_length=256, n_bins=60))
librosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max), sr=sr, hop_length=256, x_axis=&#x27;time&#x27;, y_axis=&#x27;cqt_note&#x27;, ax=axs[2, 0])
axs[2, 0].set_title(&#x27;CQT (dB)&#x27;)

# Hide the last empty subplot
axs[2, 1].axis(&#x27;off&#x27;)

plt.tight_layout(rect=[0, 0, 1, 0.97])
plt.show()
</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div></div></details><div data-mdast-node-id="ixtVQGhxOgQ_hc2q01KiS" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/sensingsoundandmusic/build/49b5229f35e1e416b5299b7568d49a2d.png" alt="&lt;Figure size 1400x1200 with 6 Axes&gt;"/></div></div><div id="riyk16F1m1" class="relative group/block"><h2 id="symbolic-representations" class="relative group"><span class="heading-text">Symbolic Representations</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#symbolic-representations" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Symbolic representations are structured, human- and machine-readable formats that encode musical information such as pitch, rhythm, dynamics, and articulation. These representations are essential for music analysis, composition, generation, and interoperability between software tools. Below are some of the most widely used symbolic formats:</p><h3 id="midi" class="relative group"><span class="heading-text">MIDI</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#midi" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/MIDI" class="italic" target="_blank" rel="noreferrer" data-state="closed">MIDI</a> (Musical Instrument Digital Interface) is a standard protocol for communicating musical performance data between electronic instruments and computers. It encodes information such as note pitch, velocity (how hard a note is played), duration, instrument type, and control changes (e.g., modulation, sustain pedal). MIDI files do not contain actual audio but rather instructions for how music should be played, making them compact and widely compatible. MIDI is the backbone of most digital music production environments and is used for sequencing, editing, and playback.</p><p><strong>Key features:</strong></p><ul><li><p>Encodes note events (on/off), pitch, velocity, and timing.</p></li><li><p>Supports multiple channels (instruments) and tracks.</p></li><li><p>Widely supported by DAWs, synthesizers, and notation software.</p></li></ul><h3 id="abc-notation" class="relative group"><span class="heading-text">ABC Notation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#abc-notation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/ABC_notation" class="italic" target="_blank" rel="noreferrer" data-state="closed">ABC Notation</a> is a text-based music notation system that uses ASCII characters to represent musical scores. It is especially popular for folk and traditional music due to its simplicity and ease of sharing via plain text. ABC notation can encode melody, rhythm, lyrics, and basic chords.</p><p><strong>Key features:</strong></p><ul><li><p>Human-readable and easy to edit in any text editor.</p></li><li><p>Supports simple melodies, chords, and lyrics.</p></li><li><p>Many online tools exist for converting ABC to sheet music or MIDI.</p></li></ul><h3 id="remi" class="relative group"><span class="heading-text">REMI</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#remi" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2002.00212" class="">REMI</a> (REvamped MIDI-derived events) is an enhanced representation of MIDI data designed for deep learning-based music generation. REMI introduces additional event types such as Note Duration, Bar, Position, and Tempo, allowing for a more structured and musically meaningful encoding of rhythm and meter.</p><p><strong>Key features:</strong></p><ul><li><p>Designed for symbolic music generation with neural networks.</p></li><li><p>Encodes timing, structure, and expressive elements.</p></li><li><p>Facilitates learning of musical form and rhythm.</p></li></ul><h3 id="musicxml" class="relative group"><span class="heading-text">MusicXML</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#musicxml" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p><a href="https://en.wikipedia.org/wiki/MusicXML" class="italic" target="_blank" rel="noreferrer" data-state="closed">MusicXML</a> is an XML-based format for representing Western music notation. It encodes detailed musical elements such as notes, rests, articulations, dynamics, lyrics, and layout information. MusicXML is ideal for sharing, analyzing, and archiving sheet music, and is supported by most notation software.</p><p><strong>Key features:</strong></p><ul><li><p>Encodes full sheet music, including layout and expressive markings.</p></li><li><p>Supports complex scores with multiple staves, voices, and instruments.</p></li><li><p>Enables interoperability between notation programs (e.g., Finale, Sibelius, MuseScore).</p></li></ul><h3 id="piano-roll" class="relative group"><span class="heading-text">Piano Roll</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#piano-roll" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>The <a href="https://en.wikipedia.org/wiki/Piano_roll" class="italic" target="_blank" rel="noreferrer" data-state="closed">Piano Roll</a> is a visual representation of music, where time is displayed on the horizontal axis and pitch on the vertical axis. Notes are shown as rectangles, with their position indicating onset, their length indicating duration, and their vertical placement indicating pitch. Piano rolls are commonly used in DAWs for editing MIDI data and visualizing performances.</p><p><strong>Key features:</strong></p><ul><li><p>Intuitive, graphical interface for editing MIDI notes.</p></li><li><p>Useful for sequencing, quantization, and visualization.</p></li><li><p>Does not encode expressive markings or complex notation.</p></li></ul><h3 id="note-graph" class="relative group"><span class="heading-text">Note Graph</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#note-graph" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>A <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2006.05417" class="">Note Graph</a> is a graph-based representation of musical scores, where nodes represent notes and edges capture relationships such as sequence, onset, and sustain. This approach provides a structured way to analyze and model complex musical relationships, such as polyphony, voice leading, and harmonic context.</p><p><strong>Key features:</strong></p><ul><li><p>Captures relationships between notes beyond simple sequences.</p></li><li><p>Useful for music analysis, generation, and machine learning.</p></li><li><p>Enables modeling of complex structures like counterpoint and harmony.</p></li></ul><hr class="py-2 my-5 translate-y-2"/><p><strong>Summary Table</strong></p><table class=""><tbody><tr class=""><th class="">Representation</th><th class="">Type</th><th class="">Typical Use Cases</th><th class="">Strengths</th></tr><tr class=""><td class="">MIDI</td><td class="">Event-based</td><td class="">Sequencing, playback, editing</td><td class="">Compact, widely supported</td></tr><tr class=""><td class="">ABC Notation</td><td class="">Text-based</td><td class="">Folk/traditional music, sharing</td><td class="">Simple, human-readable</td></tr><tr class=""><td class="">REMI</td><td class="">Event-based</td><td class="">AI music generation</td><td class="">Structured, rhythm-aware</td></tr><tr class=""><td class="">MusicXML</td><td class="">XML-based</td><td class="">Sheet music, notation, analysis</td><td class="">Detailed, expressive, interoperable</td></tr><tr class=""><td class="">Piano Roll</td><td class="">Visual</td><td class="">MIDI editing, sequencing</td><td class="">Intuitive, easy to manipulate</td></tr><tr class=""><td class="">Note Graph</td><td class="">Graph-based</td><td class="">Analysis, AI, complex relationships</td><td class="">Captures structure and context</td></tr></tbody></table></div><div id="knVtjAZT9g" class="relative group/block"><h2 id="questions" class="relative group"><span class="heading-text">Questions</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#questions" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><ol start="1"><li><p>What is the difference between a tone and a note in music psychology?</p></li><li><p>How does a spectrogram differ from a waveform in audio visualization?</p></li><li><p>What is the purpose of the Mel Spectrogram and why is it perceptually relevant?</p></li><li><p>Describe the concept of harmony and how technology can be used to analyze it.</p></li><li><p>What are the main differences between symbolic representations such as MIDI, ABC Notation, and MusicXML?</p></li></ol></div><div></div><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/sensingsoundandmusic/week5"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">Sensing Sound and Music</div>Week 5: Time and Rhythm</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/sensingsoundandmusic/week7"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Sensing Sound and Music</div>Week 7: Body Motion</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></main></article><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/sensingsoundandmusic/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-JSE36H2O.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-C7FW3E47.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-ND43KHSX.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/root-IB5726YR.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/_shared/chunk-NBON2RSI.js"/><link rel="modulepreload" href="/sensingsoundandmusic/build/routes/$-LXLHKVOR.js"/><script>window.__remixContext = {"url":"/week6","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.1","options":{"favicon":"/sensingsoundandmusic/build/favicon-2720eef16358a9679c7ec109a6d05906.ico"},"nav":[],"actions":[],"projects":[{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"exports":[],"title":"Sensing Sound and Music","description":"An interdisciplinary exploration of how humans perceive, experience, and create sound and music through psychological and technological perspectives.","authors":[{"id":"MUS2640","name":"MUS2640"},{"id":"University of Oslo","name":"University of Oslo"}],"github":"https://github.com/fourMs/sensingsoundandmusic","keywords":["sensing","sound","music","psychology","technology"],"id":"11cc8b5a-b0a2-4516-8397-a7dbad782f82","toc":[{"file":"intro.md"},{"file":"week1.md"},{"file":"week2.md"},{"file":"week3.ipynb"},{"file":"week4.ipynb"},{"file":"week5.md"},{"file":"week6.ipynb"},{"file":"week7.md"},{"file":"week8.md"},{"file":"week9.md"},{"file":"week10.md"},{"file":"week11.md"}],"bibliography":[],"index":"index","pages":[{"slug":"week1","title":"Week 1: Tuning in","description":"This page introduces the foundational concepts of music psychology and technology, exploring how humans perceive, experience, and create sound and music through both psychological and technological perspectives.","date":"","thumbnail":"/sensingsoundandmusic/build/c20d5f224f701120b83307814eab6564.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week2","title":"Week 2: Listening","description":"This chapter explores the art and science of listening, focusing on how sounds and soundscapes are described, understood, and analyzed across disciplines. It introduces influential theories and thinkers, practical listening exercises, and tools for engaging with the sonic environment.","date":"","thumbnail":"/sensingsoundandmusic/build/impulsive-sustained--c0c48158496bcc681fba65df66fa6f2f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week3","title":"Week 3: Acoustics","description":"This document provides an introduction to the fundamentals of acoustics, covering the physics of sound, the behavior of sound in rooms and instruments, and the basics of digital audio. It includes explanations, visualizations, and practical exercises to help you understand how sound is produced, transmitted, and perceived in various contexts.","date":"","thumbnail":"/sensingsoundandmusic/build/695b2b8faecb94212205e4e80f715504.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week4","title":"Week 4: Psychoacoustics","description":"This notebook introduces the fundamentals of psychoacoustics—the science of how humans perceive and interpret sound. It covers the anatomy of the ear, principles of loudness and pitch perception, auditory illusions, masking effects, and the application of psychoacoustic concepts in audio technology and music analysis.","date":"","thumbnail":"/sensingsoundandmusic/build/c03cf3c207d0b46d253830651ea7ba28.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week5","title":"Week 5: Time and Rhythm","description":"This chapter explores the foundations of musical time and rhythm, covering concepts such as onset timing, perceptual centers, meter, microrhythm, groove, and entrainment. It examines how rhythm is structured, perceived, and performed, highlighting the roles of technology and analysis tools in understanding the nuances of timing and groove in various musical styles.","date":"","thumbnail":"/sensingsoundandmusic/build/week5_image6-42ef9fdd71a421c1baf682d65217062b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week6","title":"Week 6: Harmony and melody","description":"This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python.","date":"","thumbnail":"/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week7","title":"Week 7: Body Motion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week8","title":"Week 8: The Brain","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/0698af3bcaf829b93eb28d09596d0541.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week9","title":"Week 9: Vision","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/undefined","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week10","title":"Week 10: Physiology","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week11","title":"Week 11: Machine Listening","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static","BASE_URL":"/sensingsoundandmusic"},"routes/$":{"config":{"version":2,"myst":"1.6.1","options":{"favicon":"/sensingsoundandmusic/build/favicon-2720eef16358a9679c7ec109a6d05906.ico"},"nav":[],"actions":[],"projects":[{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"exports":[],"title":"Sensing Sound and Music","description":"An interdisciplinary exploration of how humans perceive, experience, and create sound and music through psychological and technological perspectives.","authors":[{"id":"MUS2640","name":"MUS2640"},{"id":"University of Oslo","name":"University of Oslo"}],"github":"https://github.com/fourMs/sensingsoundandmusic","keywords":["sensing","sound","music","psychology","technology"],"id":"11cc8b5a-b0a2-4516-8397-a7dbad782f82","toc":[{"file":"intro.md"},{"file":"week1.md"},{"file":"week2.md"},{"file":"week3.ipynb"},{"file":"week4.ipynb"},{"file":"week5.md"},{"file":"week6.ipynb"},{"file":"week7.md"},{"file":"week8.md"},{"file":"week9.md"},{"file":"week10.md"},{"file":"week11.md"}],"bibliography":[],"index":"index","pages":[{"slug":"week1","title":"Week 1: Tuning in","description":"This page introduces the foundational concepts of music psychology and technology, exploring how humans perceive, experience, and create sound and music through both psychological and technological perspectives.","date":"","thumbnail":"/sensingsoundandmusic/build/c20d5f224f701120b83307814eab6564.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week2","title":"Week 2: Listening","description":"This chapter explores the art and science of listening, focusing on how sounds and soundscapes are described, understood, and analyzed across disciplines. It introduces influential theories and thinkers, practical listening exercises, and tools for engaging with the sonic environment.","date":"","thumbnail":"/sensingsoundandmusic/build/impulsive-sustained--c0c48158496bcc681fba65df66fa6f2f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week3","title":"Week 3: Acoustics","description":"This document provides an introduction to the fundamentals of acoustics, covering the physics of sound, the behavior of sound in rooms and instruments, and the basics of digital audio. It includes explanations, visualizations, and practical exercises to help you understand how sound is produced, transmitted, and perceived in various contexts.","date":"","thumbnail":"/sensingsoundandmusic/build/695b2b8faecb94212205e4e80f715504.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week4","title":"Week 4: Psychoacoustics","description":"This notebook introduces the fundamentals of psychoacoustics—the science of how humans perceive and interpret sound. It covers the anatomy of the ear, principles of loudness and pitch perception, auditory illusions, masking effects, and the application of psychoacoustic concepts in audio technology and music analysis.","date":"","thumbnail":"/sensingsoundandmusic/build/c03cf3c207d0b46d253830651ea7ba28.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week5","title":"Week 5: Time and Rhythm","description":"This chapter explores the foundations of musical time and rhythm, covering concepts such as onset timing, perceptual centers, meter, microrhythm, groove, and entrainment. It examines how rhythm is structured, perceived, and performed, highlighting the roles of technology and analysis tools in understanding the nuances of timing and groove in various musical styles.","date":"","thumbnail":"/sensingsoundandmusic/build/week5_image6-42ef9fdd71a421c1baf682d65217062b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week6","title":"Week 6: Harmony and melody","description":"This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python.","date":"","thumbnail":"/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week7","title":"Week 7: Body Motion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week8","title":"Week 8: The Brain","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/0698af3bcaf829b93eb28d09596d0541.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week9","title":"Week 9: Vision","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/undefined","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week10","title":"Week 10: Physiology","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week11","title":"Week 11: Machine Listening","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"page":{"version":2,"kind":"Notebook","sha256":"3c99062d0f10904d5cfbd56aabd76c5431aab281080d580a67cb3b6c63c58c51","slug":"week6","location":"/week6.ipynb","dependencies":[],"frontmatter":{"title":"Week 6: Harmony and melody","description":"This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python.","subtitle":"Exploring harmony, melody, and musical structure","authors":[{"nameParsed":{"literal":"Alexander Refsum Jensenius","given":"Alexander Refsum","family":"Jensenius"},"name":"Alexander Refsum Jensenius","affiliations":["University of Oslo"],"id":"contributors-week6-generated-uid-0"}],"affiliations":[{"id":"University of Oslo","name":"University of Oslo"}],"exports":[{"format":"ipynb","filename":"week6.ipynb","url":"/sensingsoundandmusic/build/week6-f46a4733518703ebe5195049b92bb93e.ipynb"}],"kernelspec":{"name":"python3","display_name":"venv","language":"python"},"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"github":"https://github.com/fourMs/sensingsoundandmusic","keywords":["sensing","sound","music","psychology","technology"],"source_url":"https://github.com/fourMs/sensingsoundandmusic/blob/main/book/week6.ipynb","edit_url":"https://github.com/fourMs/sensingsoundandmusic/edit/main/book/week6.ipynb","thumbnail":"/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png"},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Last week, we explored the importance of timing and rhythm in sound and music. This week, our focus shifts to how frequencies interact both “horizontally” and “vertically.” Horizontal organization of frequencies gives rise to melodies; sequences of pitches unfolding over time, often shaped by rhythmic patterns. Vertical organization, on the other hand, leads to the formation of intervals, harmonies, and textures, as multiple pitches sound together or overlap. Understanding these dimensions is essential for analyzing how music conveys structure, emotion, and complexity. First, however, we need to define the concept of “tone”.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G3FU4lL1E9"}],"key":"ATo6kPZbER"},{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Tones","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QaJY3GAgPn"}],"identifier":"tones","label":"Tones","html_id":"tones","implicit":true,"key":"BQoEVooJr9"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"In music psychology, a ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"o5oJmfPkmJ"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"tone","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SWYe9adqh1"}],"key":"cHRbDjk2tz"},{"type":"text","value":" is understood as a sound with a specific frequency and timbral quality that the auditory system interprets as having a definite pitch. Our perception of tones is shaped by both the physical properties of the sound wave (such as frequency, amplitude, and harmonic content) and the way our brains process these signals. Tones are the building blocks of musical perception, allowing us to distinguish melodies, harmonies, and timbres.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TORy9O74Cx"}],"key":"AfapUPVPP4"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"From a technological perspective, tones can be generated, analyzed, and manipulated using digital tools. Synthesizers create tones by combining waveforms, while audio analysis software can extract pitch and timbre features from recordings. Technologies such as pitch detection algorithms and spectral analysis are essential for applications in music information retrieval, automatic transcription, and digital instrument design.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"GgDsvtII1s"}],"key":"SoKIiaBGQC"}],"key":"c5SXQlLeTE"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\n\nsr = 22050  # or whatever sample rate you are using\nduration = 2.0  # seconds, or whatever duration you want\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\n# Define a fundamental frequency\nf0 = 440  # A4, 440 Hz\n\n# Pure tone (sine wave)\npure = np.sin(2 * np.pi * f0 * t)\n\n# Harmonics (sum of first 5 harmonics)\nharmonics = sum([np.sin(2 * np.pi * f0 * (n+1) * t) / (n+1) for n in range(5)])\n\n# Complex tone (sum of odd harmonics, like a square wave)\ncomplex_tone = sum([np.sin(2 * np.pi * f0 * (2*n+1) * t) / (2*n+1) for n in range(5)])\n\n# Set new envelope times (in seconds)\nattack_time = 0.01   # very rapid attack\ndecay_time = 0.02    # very rapid decay\nsustain_time = 0.9   # much longer sustain\nrelease_time = 0.2   # longer release\n\n# Convert times to sample lengths\nattack_len = int(attack_time * sr)\ndecay_len = int(decay_time * sr)\nsustain_len = int(sustain_time * sr)\nrelease_len = int(release_time * sr)\n\n# Create envelope with decay going down to 0.2\nattack = np.linspace(0, 1, attack_len, endpoint=False)                    # Attack\ndecay = np.linspace(1, 0.2, decay_len, endpoint=False)                    # Decay (to 0.2)\nsustain = np.full(sustain_len, 0.2)                                       # Sustain\nrelease = np.linspace(0.2, 0, release_len, endpoint=True)                 # Release\n\nenvelope = np.concatenate([attack, decay, sustain, release])\n\n# Ensure envelope matches signal length\nif len(envelope) \u003c len(t):\n    envelope = np.concatenate([envelope, np.zeros(len(t) - len(envelope))])\nelse:\n    envelope = envelope[:len(t)]\n\n# Apply envelope to signals (assuming pure, harmonics, complex_tone are numpy arrays)\npure_env = pure * envelope\nharmonics_env = harmonics * envelope\ncomplex_tone_env = complex_tone * envelope\nsignals_env = [pure_env, harmonics_env, complex_tone_env]","visibility":"hide","key":"UDSjAvHEwY"},{"type":"output","id":"Eb2Z0ZpgWQaNbZJaDjW2p","data":[],"visibility":"show","key":"i6D6Kzv1PP"}],"visibility":"show","key":"IwEmgengrf"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Tones are not the same as notes: while a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q9aaBgzGxP"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"note","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"elrJICoa7S"}],"key":"CKBdX1YTvu"},{"type":"text","value":" refers to a symbolic representation in musical notation (with defined pitch, duration, and sometimes dynamics), a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fZ3O1RI3hc"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"tone","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iyvbKlSdW6"}],"key":"xzEeuvNoRW"},{"type":"text","value":" refers to the auditory experience itself.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C6ypZYv9DX"}],"key":"rejqrbG5sI"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"N7fibbMzmh"}],"key":"a1NrtwZtd9"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"In the following, we will use an ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Qbp1vRvet4"},{"type":"link","url":"https://en.wikipedia.org/wiki/Analysis_by_synthesis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"analysis-by-synthesis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"zxC0QOOMAh"}],"urlSource":"https://en.wikipedia.org/wiki/Analysis_by_synthesis","data":{"page":"Analysis_by_synthesis","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"uTS8t6IkJI"},{"type":"text","value":" technique to understand more about the different concepts. This is a method used in sound and music research to understand auditory perception by recreating sounds and analyzing their properties. This approach is widely used in areas like speech synthesis, sound design, and music analysis.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"MOBAgsYsXT"}],"key":"bHXMzwGUKw"}],"key":"jvthbag10U"}],"key":"APO2i8TrHM"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import music21\n\nimport matplotlib.pyplot as plt\n\n# Plot the complex tone waveform\nfig, axs = plt.subplots(1, 2, figsize=(12, 3), gridspec_kw={'width_ratios': [2, 1]})\n\n# Waveform plot\naxs[0].plot(t, complex_tone_env)\naxs[0].set_title('Complex Tone (with Envelope)')\naxs[0].set_xlabel('Time (s)')\naxs[0].set_ylabel('Amplitude')\n\n# Create a single musical note (e.g., A4)\nnote = music21.note.Note('A4')\nnote.quarterLength = 1.0\nstream = music21.stream.Stream([note])\n\n# Render the musical note in the second subplot (as an image)\n# Save the note as an image and display it\nimg_path = stream.write('lily.png')\nimg = plt.imread(img_path)\naxs[1].imshow(img)\naxs[1].axis('off')\naxs[1].set_title('Musical Note (A4)')\n\nplt.tight_layout()\nplt.show()","visibility":"hide","key":"epjkRz8MRk"},{"type":"output","id":"scpdopmrVkqUG9dhSv3ia","data":[{"name":"stderr","output_type":"stream","text":"Changing working directory to: `/tmp/music21'\nProcessing `/tmp/music21/tmplwsgg5xf.ly'\nParsing...\n/tmp/music21/tmplwsgg5xf.ly:25:5: error: unknown escaped string: `\\RemoveEmptyStaffContext'\n    \n    \\RemoveEmptyStaffContext\n/tmp/music21/tmplwsgg5xf.ly:26:5: error: syntax error, unexpected \\override, expecting '='\n    \n    \\override VerticalAxisGroup #'remove-first = ##t\n/tmp/music21/tmplwsgg5xf.ly:26:33: warning: deprecated: missing `.' in property path VerticalAxisGroup.remove-first\n    \\override VerticalAxisGroup \n                                #'remove-first = ##t\n/tmp/music21/tmplwsgg5xf.ly:28:2: error: syntax error, unexpected '}'\n \n }\n/tmp/music21/tmplwsgg5xf.ly:28:3: error: Unfinished main input\n }\n  \nInterpreting music...\nPreprocessing graphical objects...\nCalculating line breaks... \nDrawing systems... \nConverting to PNG...\nfatal error: failed files: \"/tmp/music21/tmplwsgg5xf.ly\"\n"},{"output_type":"display_data","metadata":{"image/png":{"height":289,"width":1072}},"data":{"image/png":{"content_type":"image/png","hash":"57f4854fc995277d7cccbf6ae6db6492","path":"/sensingsoundandmusic/build/57f4854fc995277d7cccbf6ae6db6492.png"},"text/plain":{"content":"\u003cFigure size 1200x300 with 2 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"mXvZHavCYc"}],"visibility":"show","key":"dblvLhqqRx"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Pitch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZVx4o1NxvQ"}],"identifier":"pitch","label":"Pitch","html_id":"pitch","implicit":true,"key":"TvIWHAizzL"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Pitch","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xytMHMZnwB"}],"key":"UiJvyvxx1k"},{"type":"text","value":" is the psycho-physiological correlate of frequency that lets us hear one sound as higher or lower than another. It is closely related to frequency---the rate at which a waveform repeats---but the relationship is not one-to-one. As you may recall from the Acoustics chapter, most instrument tones are not pure sine waves. Musical, ‘pitched’ instruments generally produce a fundamental frequency and many additional frequencies (overtones). This is because pitched musical instruments are often based on an acoustic resonator, which oscillates at numerous frequencies simultaneously, mostly limited to integer multiples, or harmonics, of the lowest (fundamental) frequency, and such multiples form a harmonic series. However, the individual partials of a complex sound are typically not perceived as separate; our perceptual system fuses them together, leading us to experience a unitary sound","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"eIu7etTtSZ"}],"key":"xr8bCFGnI5"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The pitch of harmonic tones generally corresponds to the fundamental frequency (f₀). However, the brain can infer a fundamental frequency (and thus perceive pitch) from complex tones even when a fundamental component is absent. This is called ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"BmT8guHCZY"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"virtual pitch","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UQQIV74V3n"}],"key":"zmQt8N8fuh"},{"type":"text","value":" or the ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ScNKNsDQES"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"missing fundamental","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"CNrl9Inj5a"}],"key":"oBCIwF3InI"},{"type":"text","value":" and it is related to the phenomenon whereby one’s brain extracts tones from everyday signals, even if parts of the signal are masked by other sounds","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"sZqShORvr7"}],"key":"los4g1LouT"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Below is a sequence of simple (sine) tones distinct at different frequencies (","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Y19zLkyYHS"},{"type":"link","url":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"[Audio Ex. 1]{.underline}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"QuBvX7AdJT"}],"urlSource":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","key":"JYtj994UMP"},{"type":"text","value":").","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KSWuaNoYyK"}],"key":"rciOgHtvkV"},{"type":"image","url":"/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"HQymENylfp","urlSource":"figures/week6_image2.png"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"If we add some partials (multiples) below each tone (","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jhmp2rFVnF"},{"type":"link","url":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Audio example 2","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"c1x96iIh1H"}],"urlSource":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","key":"u1wGnAeGKW"},{"type":"text","value":"), can you start to hear a familiar melody?","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Ugyzyj6JRx"}],"key":"Ee1srdj2Le"},{"type":"image","url":"/sensingsoundandmusic/build/week6_image4-fac934d9fdb26d8b5be4765f37a3b3f3.png","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"hYZOe7Vw2A","urlSource":"figures/week6_image4.png"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"If we then rearrange these partials slightly (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"lMjIvFfVLL"},{"type":"link","url":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"[Audio Example 3]{.underline}","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"U4P5AkFFqD"}],"urlSource":"https://www.dropbox.com/scl/fo/8qfuvsu1s5cl8odnbdo0x/AAQZOn7PITdCyytloI_yAnI?rlkey=gv9isblfijwhavfqd6u2uapt2\u0026dl=0","key":"vhfafnVTDn"},{"type":"text","value":"), we can induce an even stronger sense of virtual pitch---various missing fundamental frequencies which are not physically present in the tones themselves:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"TdGmxmAo2x"}],"key":"mEDFsaJ8Gz"},{"type":"image","url":"/sensingsoundandmusic/build/week6_image1-d21c6b7d42de5162c042527722e83ce7.png","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"CnPiw6Bjzs","urlSource":"figures/week6_image1.png"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Image source","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"bMmHNrkg9G"}],"key":"KPFsDlXM6R"},{"type":"text","value":": Toiviainen, P. (2015). Lecture materials for Music Perception. University of Jyväskylä.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"UMJBIbHzaH"}],"key":"Npa3opWTUb"},{"type":"heading","depth":3,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Pitch class","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"CE7QDNFIQu"}],"identifier":"pitch-class","label":"Pitch class","html_id":"pitch-class","implicit":true,"key":"RM6Rt06YnF"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"In music, a pitch class (or “chroma”) is a set of pitches that are a whole number of octaves apart, e.g., the pitch class C consists of the Cs in all octaves. Humans perceive the notes in a tonal scale as repeating once per octave. This provides the basis for producing \u0026 perceiving melodic patterns based on relative pitch relationships---that is, ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"VrIMgI8dP1"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"relative","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"iWqRrmFmDW"}],"key":"QLSAJTSdqU"},{"type":"text","value":" to a pitch class. Absolute pitch ability, on the other hand, is the ability to recognize (or reproduce) specific pitches without the help of a reference pitch and pitch class.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"EeAoi27K1U"}],"key":"DQ7mNpLczM"},{"type":"image","url":"/sensingsoundandmusic/build/week6_image3-fb606222ac409a6c1a24a8e81bcdc20b.png","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"OH0okrnHfX","urlSource":"figures/week6_image3.png"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Image Source","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"Ugso4o8zLT"}],"key":"g9gTJqd2az"},{"type":"text","value":": Trainor, Laurel \u0026 Unrau, A.J.. (2012). Development of pitch and music perception. Springer Handbook of Auditory Research: Human Auditory Development. 223-254.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"XhthE3hkFI"}],"key":"ITKg3f1LmX"},{"type":"heading","depth":3,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Tonality and harmonic expectation","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"kYbT2AlrIk"}],"identifier":"tonality-and-harmonic-expectation","label":"Tonality and harmonic expectation","html_id":"tonality-and-harmonic-expectation","implicit":true,"key":"I8FxBGXRPD"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Tonality","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"a3EQTHPMqU"}],"key":"dFlOQN5tW1"},{"type":"text","value":" refers to the tendency for tones to resolve to a fundamental tonic note---a hierarchy of tones centered on a tonic (scale). It is pne of the main conceptual categories in Western music \u0026 musical thought, and corresponds to ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"p6ZfghIJo2"},{"type":"emphasis","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"key signatures","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"JSUtmsviO6"}],"key":"o2XeyCRgGI"},{"type":"text","value":" in musical notation (C major, C minor). Some tones feel stable in a melodic-harmonic sequence or scale, others seem to want to resolve. Knowledge of tonality does not require formal training in music theory---it is learned implicitly, by virtue of exposure to the music of one’s culture. This learning-by-exposure causes us to store knowledge about key as a cognitive schema. Tonality is also supported by acoustics, in the sense that the most crucial notes in a scale (3rd and 5th) tend to share partials with the tonic.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"Zl9641zUxw"}],"key":"xeUPmFBwBB"}],"key":"dc9d2O9dEP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Harmony","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eivVFhsASr"}],"identifier":"harmony","label":"Harmony","html_id":"harmony","implicit":true,"key":"iI2ktnPBzS"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Harmony","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Harmony","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UWIvKPF4oz"}],"urlSource":"https://en.wikipedia.org/wiki/Harmony","data":{"page":"Harmony","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"e608zA3qhg"},{"type":"text","value":" involves the combination of discernible tones to create intervals and chords. It is the simultaneous sounding of different pitches, which can evoke a wide range of emotional responses. Harmony plays a crucial role in the emotional tone and complexity of music. Harmony encompasses the interaction of pitches through intervals and chords, shaped by timbre and texture.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Yb4BGKiEVG"}],"key":"UOnCLEGX6a"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Intervals","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"MG3C1ewHK6"}],"identifier":"intervals","label":"Intervals","html_id":"intervals","implicit":true,"key":"Y3LyCj5E44"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"An ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"zGqODorXvP"},{"type":"emphasis","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"interval","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"UOakNLfYDu"}],"key":"uFrd5EoK2z"},{"type":"text","value":" is the distance between two pitches, measured in steps or frequency ratios. Intervals are the building blocks of harmony, as they define the relationships between notes played together or in succession. The human brain is sensitive to the relationships between pitches, perceiving certain combinations as consonant (pleasant or stable) and others as dissonant (tense or unstable). These perceptual responses are influenced by cultural exposure, musical training, and innate auditory processing mechanisms.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"mt6lLWiThW"}],"key":"L0pu4uVlcV"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Types of Intervals:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"yicxIKozva"}],"key":"ZNoDSNoTUw"},{"type":"text","value":" Intervals are named by counting the number of letter names from the lower to the higher note (e.g., C to E is a third). They can be major, minor, perfect, augmented, or diminished.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"KAjkA0L7ky"}],"key":"lNIGvUVV5Y"}],"key":"IAvZuUfX9h"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Consonance and Dissonance:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"xEEr7u4s5d"}],"key":"LzLeyKYxmX"},{"type":"text","value":" Some intervals, like octaves and perfect fifths, are perceived as consonant, while others, like minor seconds or tritones, are more dissonant.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Vij11GkWxF"}],"key":"ncRHh50GNS"}],"key":"g6wu3l8Vyh"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Role in Harmony:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"EzHRsS65sR"}],"key":"vB0R6rW20s"},{"type":"text","value":" Intervals form the basis for chords and harmonic progressions. The combination of intervals within a chord determines its character and function.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"sNwiq0fW0P"}],"key":"S58MFskl8I"}],"key":"VkMkAF4bKr"}],"key":"YBjNpyJJTZ"},{"type":"heading","depth":3,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Chords","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"vr3qa3VEBB"}],"identifier":"chords","label":"Chords","html_id":"chords","implicit":true,"key":"KdTeAdLfRb"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"A ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"dqfCLba66K"},{"type":"emphasis","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"chord","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"xjiF1Q6xHA"}],"key":"pnT4sxTCae"},{"type":"text","value":" is a group of three or more notes played simultaneously. Chords are the foundation of Western harmony and are used to create progressions that define the structure and mood of a piece.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"EOVzmmLx7x"}],"key":"jLG9TfcISn"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Triads:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"JT4fbBY42e"}],"key":"G6uvRfr0xL"},{"type":"text","value":" The most basic chords, consisting of three notes (root, third, fifth). Types include major, minor, diminished, and augmented triads.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"jHgizoLyXA"}],"key":"H35mcViiHp"}],"key":"LYyq3kdcSg"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Seventh Chords and Extensions:","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"QehVKlmcNm"}],"key":"rfaYYHDHrb"},{"type":"text","value":" Adding more notes (such as sevenths, ninths, elevenths, and thirteenths) creates richer harmonies.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"Adg5OrFjOQ"}],"key":"vuBZcjE826"}],"key":"DdSg8ukWpg"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Chord Progressions:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"Vk3NbMZd0p"}],"key":"qVb1TCEC2w"},{"type":"text","value":" Sequences of chords that create movement and tension-resolution patterns in music (e.g., I–IV–V–I in classical music, or ii–V–I in jazz).","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"doi1NnXWmQ"}],"key":"Fvv8vqvRmK"}],"key":"cjbH2r1w2w"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Functional Harmony:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"O4tRZAv3ht"}],"key":"qGcWQdFMMJ"},{"type":"text","value":" Chords have roles (tonic, dominant, subdominant) that guide the listener’s expectations.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"CzEpBOZ3Bu"}],"key":"HEJk0PoRp3"}],"key":"Bd84Cpd6qV"}],"key":"C7peiOzeVJ"},{"type":"heading","depth":3,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Timbre","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"hMgiUNgIdy"}],"identifier":"timbre","label":"Timbre","html_id":"timbre","implicit":true,"key":"EA6prZ491e"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Recall that ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"e9OIXGa46S"},{"type":"link","url":"https://en.wikipedia.org/wiki/Timbre","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"timbre","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"WRcTymwmSD"}],"urlSource":"https://en.wikipedia.org/wiki/Timbre","data":{"page":"Timbre","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"UCJgcE59NC"},{"type":"text","value":", often called “sound color,” is the quality of a sound that distinguishes different instruments or voices, even when they produce the same pitch and loudness.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"ZAkkU7zf6o"}],"key":"bYQHQ9Q7cR"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":27,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"“Sound Color”:","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"YUIPyax5F3"}],"key":"oXCOkRgsnl"},{"type":"text","value":" The unique quality that makes a violin sound different from a flute, even if both play the same note at the same loudness.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"oymc9yBG2W"}],"key":"cAbusC7n0i"}],"key":"H47nUrjceN"},{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Spectral Content:","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"J5wncGTwpT"}],"key":"J6wzWjoslq"},{"type":"text","value":" Timbre is shaped by the harmonic content (overtones) and the way energy is distributed across frequencies.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"rlSDHyjKHq"}],"key":"adzTG59wP8"}],"key":"DHLFthXCZq"},{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Temporal Alignment:","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"UWkvzbm4dY"}],"key":"pYaXrEdQQt"},{"type":"text","value":" The timing of sound waves, including attack, decay, sustain, and release, contributes to timbre perception.","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"lf8oj4WeyI"}],"key":"MBa8lEQxrw"}],"key":"DP2hvKmQak"},{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Just Noticeable Differences (JND):","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"xG0PZsMmOW"}],"key":"UaXA68eWp3"},{"type":"text","value":" The smallest change in a sound property (such as frequency, amplitude, or spectral content) that can be perceived.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"zYOXll0UTQ"}],"key":"BCSB9o9j37"}],"key":"pJ00rMselQ"}],"key":"m0kLWIoKMk"},{"type":"heading","depth":3,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Texture","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"jQOsLEYdwM"}],"identifier":"texture","label":"Texture","html_id":"texture","implicit":true,"key":"qZN8Um1wpE"},{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Texture_(music)","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Texture","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"txdc6rSPie"}],"urlSource":"https://en.wikipedia.org/wiki/Texture_(music)","data":{"page":"Texture_(music)","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"tjg9mPB2nV"},{"type":"text","value":" describes how multiple layers of sound interact in a musical composition. It ranges from monophonic (a single melody) to polyphonic (multiple independent melodies).","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"bApSlWqm2p"}],"key":"uhXlFiPjsM"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":37,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"Monophonic Texture:","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"jWj8BYKQDe"}],"key":"e9Ss5GMAOh"},{"type":"text","value":" A single melodic line without accompaniment (e.g., solo singing).","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"xL82Vm5Qtc"}],"key":"mymH6JSSE4"}],"key":"fv6iuWYe4m"},{"type":"listItem","spread":true,"position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Homophonic Texture:","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"X6aDsz4GJy"}],"key":"CusJk3546p"},{"type":"text","value":" A main melody supported by chords or accompaniment (e.g., singer with guitar).","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"VJqipSZ4VV"}],"key":"D9wG1v2HJh"}],"key":"XO3DgOsVHF"},{"type":"listItem","spread":true,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Polyphonic Texture:","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"JD9D9qsowL"}],"key":"ncWfDVtrSF"},{"type":"text","value":" Two or more independent melodies played simultaneously (e.g., fugues, counterpoint).","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"D8LVdzWWNv"}],"key":"RH9uhLApJ3"}],"key":"MKdg2uWWqV"},{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Heterophonic Texture:","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"Jfiq8aK6K6"}],"key":"ikeg1naDKK"},{"type":"text","value":" Multiple performers play variations of the same melody at the same time.","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"wSIgy6Wttq"}],"key":"i0qPwo3Cdf"}],"key":"idYAILA47M"},{"type":"listItem","spread":true,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"Combination of Timbres:","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"eP4yY3DoTK"}],"key":"u7HPBq7B6D"},{"type":"text","value":" The blending of different sound qualities to create a rich texture.","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"kmKLFq3oZ9"}],"key":"Qb7fcI4M6i"}],"key":"Wfl2cylv2v"}],"key":"QlHlf83EBO"}],"key":"ysWGWKwI7k"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Melody","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZJ0WGxoyTr"}],"identifier":"melody","label":"Melody","html_id":"melody","implicit":true,"key":"rdDs0PEoDc"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Melody","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Melody","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"rQALJiucGR"}],"urlSource":"https://en.wikipedia.org/wiki/Melody","data":{"page":"Melody","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"zhN6Bz8YGR"},{"type":"text","value":" is the sequence of musical notes that are perceived as a single entity. It is often the most recognizable and memorable aspect of a musical piece. Melody plays a crucial role in emotional engagement and memory recall. Tools like MIDI editors and pitch detection algorithms are used to analyze and manipulate melodies.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CwVQahlaHc"}],"key":"KIEqrwsGWa"}],"key":"q1QBFW3upT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import music21\nmusic21.environment.UserSettings()['musescoreDirectPNGPath'] = '/usr/bin/mscore3'\n\n# Create a simple melody: C D E F G F E D C\nmelody_notes = ['C4', 'D4', 'E4', 'F4', 'G4', 'F4', 'E4', 'D4', 'C4']\nmelody = music21.stream.Stream()\nfor n in melody_notes:\n    melody.append(music21.note.Note(n, quarterLength=0.5))\n\n# Show the musical score (this will render in Jupyter if MuseScore or similar is installed)\nmelody.show()","key":"m5jn3jS38a"},{"type":"output","id":"y6RWV3Fyscy7wEzhK0YmM","data":[{"output_type":"display_data","metadata":{"image/png":{"height":50,"width":408}},"data":{"image/png":{"content_type":"image/png","hash":"fcec387afe67a7ab7a8a4d0a0ac747ff","path":"/sensingsoundandmusic/build/fcec387afe67a7ab7a8a4d0a0ac747ff.png"},"text/plain":{"content":"\u003cIPython.core.display.Image object\u003e","content_type":"text/plain"}}}],"key":"oV7eGZKr5B"}],"key":"pf1T34b8AI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"From a psychological perspective, ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kXncgaZ5r9"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"melody","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"GubQ7MMGas"}],"key":"D1GzF0PDsj"},{"type":"text","value":" is the perception of a coherent sequence of tones that form a recognizable musical line. Melodies are central to musical memory and emotional response, as the brain tracks pitch contours, intervals, and rhythmic patterns to identify and recall tunes. Research in music cognition explores how listeners segment, remember, and anticipate melodic sequences.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"rzJcehK23u"}],"key":"BZyRBoCtcg"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Technological advances have enabled detailed analysis and manipulation of melody. Pitch tracking algorithms extract melodic lines from audio, while MIDI editors and music notation software allow for precise editing and visualization. In music generation and AI composition, models learn melodic patterns from large datasets to create new, stylistically consistent melodies. Melody extraction and similarity algorithms are also used in music search and recommendation systems.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"rvCf1wApe3"}],"key":"n5weXj6UVi"},{"type":"heading","depth":3,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Auditory stream segregation","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"l1eEVKAlQ0"}],"identifier":"auditory-stream-segregation","label":"Auditory stream segregation","html_id":"auditory-stream-segregation","implicit":true,"key":"ZRwywTqvg3"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Auditory stream segregation is the process by which the human auditory system organizes complex mixtures of sounds into perceptually meaningful elements, or “streams.” In music, this allows listeners to distinguish between different melodic lines, instruments, or voices, even when they are played simultaneously. This perceptual organization is influenced by factors such as pitch, timbre, spatial location, and timing. For example, melodies that move in different pitch ranges or have distinct timbres are more likely to be perceived as separate streams. Understanding auditory stream segregation is essential for analyzing polyphonic music, designing effective music information retrieval systems, and developing algorithms for source separation and automatic transcription. Advances in computational modeling and machine learning have enabled researchers to simulate and study how the brain separates and tracks multiple musical streams in real-world listening scenarios.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"zVk517Qgnd"}],"key":"vw3UjZX6oY"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"The foundational work of psychologist Albert S. Bregman, particularly his book ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"x3vvrVwmRx"},{"type":"emphasis","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Auditory Scene Analysis","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"kJRH2chisd"}],"key":"l0bsm3YabG"},{"type":"text","value":" (1990), established the theoretical framework for understanding how the auditory system parses complex acoustic environments. Bregman introduced the concept of “auditory scene analysis” (ASA), describing how the brain groups and segregates sounds based on cues such as frequency proximity, temporal continuity, common onset/offset, and timbral similarity. According to Bregman, these grouping principles allow us to perceive coherent musical lines and separate voices in polyphonic music, even when their acoustic signals overlap. His research has had a profound influence on music psychology, cognitive science, and the development of computational models for music and audio processing.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"iDwW9rvoks"}],"key":"q2lthvs7AX"},{"type":"heading","depth":3,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Gestalt theory","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"eWXJKuJ3o8"}],"identifier":"gestalt-theory","label":"Gestalt theory","html_id":"gestalt-theory","implicit":true,"key":"cFt8O11HGQ"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Gestalt theory, originating from early 20th-century psychology, describes how humans naturally organize sensory information into meaningful patterns and wholes. In music perception, Gestalt principles help explain how listeners group sequences of notes into coherent melodies, phrases, and motifs, even when the underlying acoustic signals are complex or ambiguous.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"v0ttUXNtnj"}],"key":"F54xnDqWEw"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Key Gestalt principles relevant to music include:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"cFn9TT787j"}],"key":"oPgnxewBLF"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Proximity:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"hKeMBj2Pdv"}],"key":"QI0Pt3DZ8K"},{"type":"text","value":" Notes that are close together in time or pitch are perceived as belonging to the same melodic group.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"U5ob0iU2l3"}],"key":"xsnOAnpfVh"}],"key":"HPF0smvxIK"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Similarity:","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"fwbpcZCs37"}],"key":"tssY6XDYga"},{"type":"text","value":" Notes with similar timbre, loudness, or articulation are grouped together.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"r9DOSrNfGq"}],"key":"nSOLmGhoeC"}],"key":"uAK3OlIr4C"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Continuity:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"dSrY5vOB45"}],"key":"Pi4vxwuYvW"},{"type":"text","value":" The brain tends to perceive smooth, continuous melodic lines rather than abrupt jumps.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"YHgjXFtyeZ"}],"key":"SPGANPdXjQ"}],"key":"JzrnLMEoJ6"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Closure:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"YZqi4rozFS"}],"key":"lnE6fWd5f9"},{"type":"text","value":" Listeners mentally “fill in” gaps to perceive complete musical phrases, even if some notes are missing.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"IkgOEmGe5l"}],"key":"VJFL13VzKe"}],"key":"UYLJnUcwc6"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Figure-Ground:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"jU71AgHG1B"}],"key":"Gfl9D2TNQD"},{"type":"text","value":" The ability to focus on a primary melody (figure) while treating accompaniment or background sounds as secondary (ground).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"U1xEfs5yC6"}],"key":"mQ9AQSIrVh"}],"key":"SUoXuyMxHH"}],"key":"zzZZHuCq79"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"These principles interact with auditory stream segregation, enabling us to follow individual voices in polyphonic music, recognize recurring themes, and make sense of complex musical textures. Gestalt theory has influenced both music psychology and computational models for music analysis, providing a framework for understanding how we perceive structure and form in music.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"U54JuiIoto"}],"key":"samKwNrjH7"},{"type":"heading","depth":3,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"Auditory illusions","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"YlpJP9VGFF"}],"identifier":"auditory-illusions","label":"Auditory illusions","html_id":"auditory-illusions","implicit":true,"key":"cfq78NKyoN"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Diana_Deutsch","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Diana Deutsch","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"HBIGsEhkt8"}],"urlSource":"https://en.wikipedia.org/wiki/Diana_Deutsch","data":{"page":"Diana_Deutsch","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"E7t5yMzHu0"},{"type":"text","value":" is a renowned psychologist and researcher who has made significant contributions to the study of auditory illusions and the psychology of music. Her experiments have uncovered a variety of perceptual phenomena that reveal how our brains organize and interpret complex sound patterns. Some of her most famous auditory illusions include:","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"nCu4WGB0CR"}],"key":"RFjA79hAqK"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":30,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"strong","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"The Tritone Paradox","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"HFnGRGgFBX"}],"key":"IugBE87Dms"},{"type":"text","value":": When two tones separated by a tritone (half an octave) are played in succession, some listeners perceive the sequence as ascending in pitch, while others hear it as descending. The direction of the perceived pitch change can vary depending on the listener’s linguistic background and even their geographical origin, suggesting that pitch perception is influenced by both biology and experience.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"mJCpvsKs47"}],"key":"r9wRaQlBCy"}],"key":"gNVdEYo0sN"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"strong","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"The Octave Illusion","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"BM7e8k5A4S"}],"key":"tvR3SnzABv"},{"type":"text","value":": When two tones an octave apart are alternately played to each ear (for example, high tone to the right ear, low tone to the left, then switching), listeners often perceive a single tone that alternates between ears and changes pitch, even though both tones are always present. This illusion demonstrates how the brain integrates and separates auditory information from both ears.","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"yOnXDSbVUf"}],"key":"Bkq1uPh00a"}],"key":"umB66uBJ9r"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"strong","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"The Scale Illusion","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"DX3doAXyTD"}],"key":"NP9mjH9D6P"},{"type":"text","value":": When ascending and descending musical scales are split between the two ears (with some notes sent to the left ear and others to the right), listeners tend to perceive coherent melodic lines that do not correspond to the actual physical input. The brain “reconstructs” the most plausible musical pattern, illustrating its tendency to organize sounds into familiar structures.","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"eOSpnlbKvo"}],"key":"FCsbYPMTuQ"}],"key":"RGix68PfZV"},{"type":"listItem","spread":true,"position":{"start":{"line":36,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"strong","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Phantom Words","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"K2tiMpczOt"}],"key":"ErxU7Mcou0"},{"type":"text","value":": In this illusion, repeating ambiguous speech sounds can cause listeners to “hear” words or phrases that are not actually present. The specific words perceived can vary between individuals, highlighting the role of expectation, language, and context in auditory perception.","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"fGFckiFZlp"}],"key":"FIzDk5ykF6"}],"key":"OiDctjoVuA"}],"key":"NghtkW56g9"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Deutsch’s work demonstrates that auditory perception is not a simple reflection of the physical properties of sound, but an active process shaped by cognitive, cultural, and neural factors. Her illusions are widely used in research, education, and demonstrations to illustrate the complexities of how we hear and interpret sound.","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"xomVe9E6zR"}],"key":"Rad0DlJOEz"},{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"For more examples and audio demonstrations, visit ","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"ko0UrN6vUJ"},{"type":"link","url":"https://deutsch.ucsd.edu/psychology/pages.php?i=201","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Diana Deutsch’s Auditory Illusions website","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"WxQqHlHOQO"}],"urlSource":"https://deutsch.ucsd.edu/psychology/pages.php?i=201","key":"KU6h5cIfE4"},{"type":"text","value":".","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"SAc5aVLedS"}],"key":"QVr6ObYfZQ"},{"type":"iframe","src":"https://www.youtube.com/embed/oF0g-UUqzgg?si=d5Id2ihd_KKVuGjt","width":"100%","key":"YaRBOWKymY"}],"key":"mGDx6yn9ZC"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Audio Visualizations","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vretix7msz"}],"identifier":"audio-visualizations","label":"Audio Visualizations","html_id":"audio-visualizations","implicit":true,"key":"PJfEzjVeJE"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We have previously looked at ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zPFbwOG8sp"},{"type":"link","url":"https://en.wikipedia.org/wiki/Waveform","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"waveform","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UfeJDNSStL"}],"urlSource":"https://en.wikipedia.org/wiki/Waveform","data":{"page":"Waveform","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"IRynb3WgqX"},{"type":"text","value":" displays and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FKgmTYALEJ"},{"type":"link","url":"https://en.wikipedia.org/wiki/Spectrogram","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"spectrograms","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d22S9BdKib"}],"urlSource":"https://en.wikipedia.org/wiki/Spectrogram","data":{"page":"Spectrogram","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"pX0zFa7UdR"},{"type":"text","value":". However, there are also several other visualization forms that try to better capture what humans hear.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"sorUMxy2gg"}],"key":"Uu2jQMEqNQ"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Visualizing audio is crucial for understanding both the physical properties of sound and how humans perceive it. Different representations highlight various aspects of the audio signal, making them useful for analysis, classification, and creative applications.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"H4Q3c3V4yz"}],"key":"grx3nouRYO"},{"type":"heading","depth":3,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Waveform","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"qEPWylvjTK"}],"identifier":"waveform","label":"Waveform","html_id":"waveform","implicit":true,"key":"QmHkWmg3VU"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"A waveform is a simple plot of amplitude versus time. It shows how the air pressure (or voltage, in digital audio) changes over time. While useful for seeing the overall shape and dynamics of a sound, it does not provide detailed information about frequency content.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"me6LCIXaJU"}],"key":"rIN4FgUzxH"},{"type":"heading","depth":3,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Spectrogram","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"hwZSwDrxW2"}],"identifier":"spectrogram","label":"Spectrogram","html_id":"spectrogram","implicit":true,"key":"LvyVQtuiJR"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"A spectrogram displays how the frequency content of a signal changes over time. It is created by applying the Short-Time Fourier Transform (STFT) to the audio, resulting in a 2D image where the x-axis is time, the y-axis is frequency, and the color represents amplitude (often in decibels). Spectrograms are widely used for audio analysis, speech recognition, and music research.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"f8l75cfxVC"}],"key":"uKSabjhLNC"},{"type":"heading","depth":3,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Log Mel Spectrogram","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"mKn0FJnXaa"}],"identifier":"log-mel-spectrogram","label":"Log Mel Spectrogram","html_id":"log-mel-spectrogram","implicit":true,"key":"bUgZepILiI"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"phm6zEuJi1"},{"type":"link","url":"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Log Mel Spectrogram","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"APUQwrTP1M"}],"urlSource":"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum","data":{"page":"Mel-frequency_cepstrum","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"x6ol9aTQzA"},{"type":"text","value":" mimics human hearing by applying the STFT, mapping the frequencies to the Mel scale (which is more perceptually relevant), and then applying a logarithmic transformation to represent the amplitude on a decibel scale. This representation compresses the frequency axis to better match how humans perceive pitch differences, making it especially useful in machine learning and audio classification tasks.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"F7eRVFI0yF"}],"key":"j8kZ4z957n"},{"type":"heading","depth":3,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"MFCCs (Mel-Frequency Cepstral Coefficients)","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"HIVFPsW8iy"}],"identifier":"mfccs-mel-frequency-cepstral-coefficients","label":"MFCCs (Mel-Frequency Cepstral Coefficients)","html_id":"mfccs-mel-frequency-cepstral-coefficients","implicit":true,"key":"cN7Hc9Qb39"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"MFCCs","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"XToazzRUR8"}],"urlSource":"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum","data":{"page":"Mel-frequency_cepstrum","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"UBCezJ66rX"},{"type":"text","value":" are a compact representation of the spectral envelope of a sound. They are computed by taking the Log Mel Spectrogram and applying the Discrete Cosine Transform (DCT), which decorrelates the features and compresses the information. MFCCs are widely used in speech recognition, music classification, and audio similarity tasks because they capture timbral characteristics that are important for distinguishing different sounds.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"cpvnUOdl7k"}],"key":"wqCBTAFWsR"},{"type":"heading","depth":3,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"CQT (Constant-Q Transform)","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"YH1mmM8vSf"}],"identifier":"cqt-constant-q-transform","label":"CQT (Constant-Q Transform)","html_id":"cqt-constant-q-transform","implicit":true,"key":"bPZOiIgAzk"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"SmOe5qpIAQ"},{"type":"link","url":"https://en.wikipedia.org/wiki/Constant-Q_transform","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Constant-Q Transform","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"JS13Zldu83"}],"urlSource":"https://en.wikipedia.org/wiki/Constant-Q_transform","data":{"page":"Constant-Q_transform","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"aF6vu5ywDP"},{"type":"text","value":" uses a logarithmic frequency scale, with exponentially spaced center frequencies and varying filter bandwidths. This makes it ideal for musical applications, as it aligns with the way musical notes are spaced (e.g., each octave is divided into equal steps). The CQT is particularly useful for tasks like pitch tracking, chord recognition, and music transcription, as it provides a more musically meaningful frequency representation than the linear STFT.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"BYiA5zX1Kn"}],"key":"P8URwHL0rU"},{"type":"heading","depth":3,"position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Summary Table","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"Q0xgH8lyoT"}],"identifier":"summary-table","label":"Summary Table","html_id":"summary-table","implicit":true,"key":"HoHbK2Qfy7"},{"type":"table","position":{"start":{"line":30,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Visualization","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"iItF5nsCnR"}],"key":"bzKescwKJs"},{"type":"tableCell","header":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"What it shows","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"VgpJEgvt5c"}],"key":"XwykQjNYFf"},{"type":"tableCell","header":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Typical Use Cases","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"KpsuaM4JkI"}],"key":"QGJLlmKapd"}],"key":"BsfbKRHPUZ"},{"type":"tableRow","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Waveform","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"Iksia3jl4p"}],"key":"bnJscl0T4d"},{"type":"tableCell","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Amplitude vs. time","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"K7RDMERVp8"}],"key":"zSMdGDujjD"},{"type":"tableCell","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Editing, dynamics, onset detection","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"yUqSTC73J3"}],"key":"bwjxbpWKk0"}],"key":"HzBvPLpBw5"},{"type":"tableRow","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Spectrogram","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"Hl9nYmlLCh"}],"key":"d0XoEXZwrW"},{"type":"tableCell","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Frequency vs. time (linear)","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"ArDNaE3nwF"}],"key":"UQaSGKLYDT"},{"type":"tableCell","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Audio analysis, speech/music research","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"nJlaNkzesx"}],"key":"CtpTCWXk16"}],"key":"fL1mH9y4Vj"},{"type":"tableRow","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Log Mel Spectrogram","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"sFn7pOe3O9"}],"key":"XygaQXqeGW"},{"type":"tableCell","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Perceptual frequency vs. time","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"M5LD64u6UW"}],"key":"W10fsJB0wW"},{"type":"tableCell","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Machine learning, audio classification","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"yJZqcw8CPL"}],"key":"ptexLP7LMX"}],"key":"CdVdjNm2fR"},{"type":"tableRow","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"MFCCs","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"QGWul2UfTe"}],"key":"RlWWlxHqOV"},{"type":"tableCell","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Compressed spectral envelope","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"nG8GGWiRAi"}],"key":"QL6A2ZrlSR"},{"type":"tableCell","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Speech/music recognition, feature extraction","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"YjEDwWBMKh"}],"key":"ddW2FrqKsp"}],"key":"U8NNqGkq8E"},{"type":"tableRow","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"CQT","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"UweJCZnngJ"}],"key":"XMC2WZxbxT"},{"type":"tableCell","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Musical pitch vs. time","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"pVfG0tNB7d"}],"key":"NwvaDU1fCj"},{"type":"tableCell","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Pitch tracking, chord recognition, MIR","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"Hz8oWczwGW"}],"key":"cRYNd2P2y5"}],"key":"v5cYRDK6nK"}],"key":"y5M56MPa1J"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"These visualizations are implemented in Python using libraries such as ","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"fyGL6xW23d"},{"type":"inlineCode","value":"librosa","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"ebImt9g1oL"},{"type":"text","value":" and ","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"fn8X7wNAF2"},{"type":"inlineCode","value":"matplotlib","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"W6ZJjvV8cr"},{"type":"text","value":", as shown in the code below. Each representation provides unique insights into the structure and content of audio signals, supporting both scientific analysis and creative exploration.","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"VpNa37U1VD"}],"key":"ClUQVSo7Vo"}],"key":"HT25G7sDPd"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport librosa\n\nimport matplotlib.pyplot as plt\nimport librosa.display\n\n# Generate a test audio signal (sine wave + harmonics)\nsr = 22050  # sample rate\nduration = 2.0  # seconds\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\nf_start = 0\nf_end = 20000\naudio = 0.5 * np.sin(2 * np.pi * ((f_start + (f_end - f_start) * t / duration) * t))\n\nfig, axs = plt.subplots(3, 2, figsize=(14, 12))\nfig.suptitle('Audio Representations', fontsize=16)\n\n# Waveform\nlibrosa.display.waveshow(audio, sr=sr, ax=axs[0, 0])\naxs[0, 0].set_title('Waveform')\naxs[0, 0].set_xlabel('')\naxs[0, 0].set_ylabel('Amplitude')\n\n# Spectrogram\nS = np.abs(librosa.stft(audio, n_fft=1024, hop_length=256))\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='hz', ax=axs[0, 1])\naxs[0, 1].set_title('Spectrogram (dB)')\n\n# Mel Spectrogram\nS_mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=256, n_mels=64)\nlibrosa.display.specshow(librosa.power_to_db(S_mel, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='mel', ax=axs[1, 0])\naxs[1, 0].set_title('Mel Spectrogram (dB)')\n\n# MFCC\nmfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=1024, hop_length=256)\nlibrosa.display.specshow(mfccs, x_axis='time', ax=axs[1, 1])\naxs[1, 1].set_title('MFCC')\n\n# CQT\nC = np.abs(librosa.cqt(audio, sr=sr, hop_length=256, n_bins=60))\nlibrosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='cqt_note', ax=axs[2, 0])\naxs[2, 0].set_title('CQT (dB)')\n\n# Hide the last empty subplot\naxs[2, 1].axis('off')\n\nplt.tight_layout(rect=[0, 0, 1, 0.97])\nplt.show()\n","visibility":"hide","key":"zca2PIOPRW"},{"type":"output","id":"ixtVQGhxOgQ_hc2q01KiS","data":[{"output_type":"display_data","metadata":{"image/png":{"height":1181,"width":1389}},"data":{"image/png":{"content_type":"image/png","hash":"49b5229f35e1e416b5299b7568d49a2d","path":"/sensingsoundandmusic/build/49b5229f35e1e416b5299b7568d49a2d.png"},"text/plain":{"content":"\u003cFigure size 1400x1200 with 6 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"N2LUvhr8mj"}],"visibility":"show","key":"Zz8pBeKRF8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Symbolic Representations","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o0bADLV3TB"}],"identifier":"symbolic-representations","label":"Symbolic Representations","html_id":"symbolic-representations","implicit":true,"key":"IgLCtGyLBa"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Symbolic representations are structured, human- and machine-readable formats that encode musical information such as pitch, rhythm, dynamics, and articulation. These representations are essential for music analysis, composition, generation, and interoperability between software tools. Below are some of the most widely used symbolic formats:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"K2tjsOzm7o"}],"key":"lLJtsmnvFJ"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"MIDI","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"xm0ygjmOCR"}],"identifier":"midi","label":"MIDI","html_id":"midi","implicit":true,"key":"zUhheGLpTO"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/MIDI","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"MIDI","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"MX4WCpztQE"}],"urlSource":"https://en.wikipedia.org/wiki/MIDI","data":{"page":"MIDI","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"QIuWIfHxkd"},{"type":"text","value":" (Musical Instrument Digital Interface) is a standard protocol for communicating musical performance data between electronic instruments and computers. It encodes information such as note pitch, velocity (how hard a note is played), duration, instrument type, and control changes (e.g., modulation, sustain pedal). MIDI files do not contain actual audio but rather instructions for how music should be played, making them compact and widely compatible. MIDI is the backbone of most digital music production environments and is used for sequencing, editing, and playback.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"cr8b2LMQcq"}],"key":"ix7ViOPSLd"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"SFx9UYcZiH"}],"key":"RgrN2Np2Vv"}],"key":"t8QZVd3arO"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Encodes note events (on/off), pitch, velocity, and timing.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"NDlg8VE06p"}],"key":"I1XRD91BGo"}],"key":"rdgbgPb5Et"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Supports multiple channels (instruments) and tracks.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"XfEKO1isS8"}],"key":"fAUCPawfao"}],"key":"Aa7GU7F0Wm"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Widely supported by DAWs, synthesizers, and notation software.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"wuA8982EBB"}],"key":"t3cGh60iYy"}],"key":"bvscqGIyPj"}],"key":"r2T8aKfAf1"},{"type":"heading","depth":3,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"ABC Notation","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"lOSnbgEtPD"}],"identifier":"abc-notation","label":"ABC Notation","html_id":"abc-notation","implicit":true,"key":"qw9NCPhHmF"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/ABC_notation","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"ABC Notation","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"jBGhmyHEJL"}],"urlSource":"https://en.wikipedia.org/wiki/ABC_notation","data":{"page":"ABC_notation","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"y6opEp7x8i"},{"type":"text","value":" is a text-based music notation system that uses ASCII characters to represent musical scores. It is especially popular for folk and traditional music due to its simplicity and ease of sharing via plain text. ABC notation can encode melody, rhythm, lyrics, and basic chords.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"k5WSIn1oao"}],"key":"w6I52k3LbS"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"strong","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"NHYrs2sVI5"}],"key":"orfZUdYUYR"}],"key":"kYq4F1UDX6"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Human-readable and easy to edit in any text editor.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"YKfMzpsLyD"}],"key":"dvtWhGkQTv"}],"key":"bNnmOMU3fb"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Supports simple melodies, chords, and lyrics.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"VcoSnRsSMt"}],"key":"JUWTaJNiW2"}],"key":"kNKFLlXQ5G"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Many online tools exist for converting ABC to sheet music or MIDI.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"PcZyv6GP9y"}],"key":"raOuYR8bvi"}],"key":"uEICLGj1gX"}],"key":"yTBh4i8vVG"},{"type":"heading","depth":3,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"REMI","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"xS1ywUlTZH"}],"identifier":"remi","label":"REMI","html_id":"remi","implicit":true,"key":"nfJ1Detdaf"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"link","url":"https://arxiv.org/abs/2002.00212","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"REMI","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"YkkkNqFtig"}],"urlSource":"https://arxiv.org/abs/2002.00212","key":"ugI7Yt7Uyf"},{"type":"text","value":" (REvamped MIDI-derived events) is an enhanced representation of MIDI data designed for deep learning-based music generation. REMI introduces additional event types such as Note Duration, Bar, Position, and Tempo, allowing for a more structured and musically meaningful encoding of rhythm and meter.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"ywpBmwC8pK"}],"key":"WsH5y1DDVJ"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"UzZPCzUtsG"}],"key":"LqW4RTPZsX"}],"key":"N7lTMhNSji"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Designed for symbolic music generation with neural networks.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"i293d5Z4bt"}],"key":"O2lP9HxLqv"}],"key":"OmCQARPirj"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Encodes timing, structure, and expressive elements.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"r5JELjqd5N"}],"key":"QydebFGYXg"}],"key":"ZrERYqOjEI"},{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Facilitates learning of musical form and rhythm.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"oDtLcqpd7a"}],"key":"VxygDrnK7D"}],"key":"TxMg6oXbef"}],"key":"oEn4aFRxiJ"},{"type":"heading","depth":3,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"MusicXML","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"tdrC5PpXMj"}],"identifier":"musicxml","label":"MusicXML","html_id":"musicxml","implicit":true,"key":"OijlvpJS00"},{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/MusicXML","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"MusicXML","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"CQ6fbHIZDv"}],"urlSource":"https://en.wikipedia.org/wiki/MusicXML","data":{"page":"MusicXML","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"gRCaalGuo3"},{"type":"text","value":" is an XML-based format for representing Western music notation. It encodes detailed musical elements such as notes, rests, articulations, dynamics, lyrics, and layout information. MusicXML is ideal for sharing, analyzing, and archiving sheet music, and is supported by most notation software.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"MExyiXj9ha"}],"key":"npF94fJzNb"},{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"strong","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"kqzC9NOX3N"}],"key":"JXYPdO48rP"}],"key":"UQ6WK9blH4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":33,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Encodes full sheet music, including layout and expressive markings.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"oOaLewp9Ew"}],"key":"KxXClk248G"}],"key":"MsvOJ3yD41"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Supports complex scores with multiple staves, voices, and instruments.","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"HrmWbzTZxZ"}],"key":"APjuMCqbvY"}],"key":"s9yVKoFaTp"},{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Enables interoperability between notation programs (e.g., Finale, Sibelius, MuseScore).","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"SexLiRp6g8"}],"key":"n1aj9qZqQ4"}],"key":"IEXFQMKHuL"}],"key":"qdEPd5GZdq"},{"type":"heading","depth":3,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"Piano Roll","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"DPMybduduo"}],"identifier":"piano-roll","label":"Piano Roll","html_id":"piano-roll","implicit":true,"key":"ObrsTIh9Ci"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"BviiiVDeSV"},{"type":"link","url":"https://en.wikipedia.org/wiki/Piano_roll","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Piano Roll","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"arxoGG8q6y"}],"urlSource":"https://en.wikipedia.org/wiki/Piano_roll","data":{"page":"Piano_roll","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"b6lOnCXyuv"},{"type":"text","value":" is a visual representation of music, where time is displayed on the horizontal axis and pitch on the vertical axis. Notes are shown as rectangles, with their position indicating onset, their length indicating duration, and their vertical placement indicating pitch. Piano rolls are commonly used in DAWs for editing MIDI data and visualizing performances.","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"OERjbHYMKr"}],"key":"WUvpfFHvVV"},{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"strong","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"ScY5GW6T1i"}],"key":"ECwDSJ6gUK"}],"key":"M4Ji9r2O0b"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":41,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Intuitive, graphical interface for editing MIDI notes.","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"KQuxbPUQhe"}],"key":"F0BZeHjCVp"}],"key":"BGvWOpBV2e"},{"type":"listItem","spread":true,"position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Useful for sequencing, quantization, and visualization.","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"IPfKlnRcAM"}],"key":"gKOSDEeLIL"}],"key":"M9VcARz2jW"},{"type":"listItem","spread":true,"position":{"start":{"line":43,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Does not encode expressive markings or complex notation.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"IwhyBUYPqZ"}],"key":"S3ImOs1iHf"}],"key":"lutnCcAwjC"}],"key":"YXrdMtkq7G"},{"type":"heading","depth":3,"position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"Note Graph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"vaV4JLlVby"}],"identifier":"note-graph","label":"Note Graph","html_id":"note-graph","implicit":true,"key":"wMHcKQcmSb"},{"type":"paragraph","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"A ","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"XDbjLov8om"},{"type":"link","url":"https://arxiv.org/abs/2006.05417","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"Note Graph","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"gHsmksIJLL"}],"urlSource":"https://arxiv.org/abs/2006.05417","key":"I6dwxa7XKX"},{"type":"text","value":" is a graph-based representation of musical scores, where nodes represent notes and edges capture relationships such as sequence, onset, and sustain. This approach provides a structured way to analyze and model complex musical relationships, such as polyphony, voice leading, and harmonic context.","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"WDJecHddHq"}],"key":"GbNtKGy3iN"},{"type":"paragraph","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Key features:","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"nqM6Ap9X6d"}],"key":"inqU10ZpvK"}],"key":"MoNmtwa7El"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":49,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Captures relationships between notes beyond simple sequences.","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"qAWatFfywm"}],"key":"cxntiPkbty"}],"key":"QuKjy807B2"},{"type":"listItem","spread":true,"position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Useful for music analysis, generation, and machine learning.","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"jSNl4J6cZH"}],"key":"U9OOUTYTzw"}],"key":"qwWaBw1cLY"},{"type":"listItem","spread":true,"position":{"start":{"line":51,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Enables modeling of complex structures like counterpoint and harmony.","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"ObFIGPWoFs"}],"key":"rwSYzfSVjh"}],"key":"OF23DSwkDv"}],"key":"t73rM8GJ2n"},{"type":"thematicBreak","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"ZCsP6nL7Q1"},{"type":"paragraph","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"strong","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"Summary Table","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"UKf7Lc6aXl"}],"key":"E3ZgwDNgun"}],"key":"g0VdrttPY0"},{"type":"table","position":{"start":{"line":57,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"tableRow","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"tableCell","header":true,"position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Representation","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"cXSrjIi9rU"}],"key":"VRMZCLtk7z"},{"type":"tableCell","header":true,"position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Type","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"Gdb7v2upX7"}],"key":"SMrwLMN0Ta"},{"type":"tableCell","header":true,"position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Typical Use Cases","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"J4fjNOvnMC"}],"key":"nECMGwmQKf"},{"type":"tableCell","header":true,"position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Strengths","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"D2dgHKL4JT"}],"key":"nm50cQ9FcK"}],"key":"h15bFJTjDd"},{"type":"tableRow","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"MIDI","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"Z9W3UMtlnX"}],"key":"OJNmKsOtIw"},{"type":"tableCell","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"Event-based","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"IBartaczcs"}],"key":"qmHaemk1EF"},{"type":"tableCell","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"Sequencing, playback, editing","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"U9CS7HsYAq"}],"key":"vimxrc7tlp"},{"type":"tableCell","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"Compact, widely supported","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"OSmqTd0v1w"}],"key":"baJ2Bpd7Ac"}],"key":"KGKgCaFHnY"},{"type":"tableRow","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"ABC Notation","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"LjXPKy6VNZ"}],"key":"HZ1cHUFBYd"},{"type":"tableCell","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"Text-based","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"BnGnAO4HfW"}],"key":"UstglpnRT3"},{"type":"tableCell","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"Folk/traditional music, sharing","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"OxsIffDLjA"}],"key":"luNJ70VDFi"},{"type":"tableCell","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"Simple, human-readable","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"p2GS3MAvAQ"}],"key":"pnyIKmLu86"}],"key":"C5lr0mfbDR"},{"type":"tableRow","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"REMI","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"cMxB9rmx85"}],"key":"UJ83gH7ibB"},{"type":"tableCell","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"Event-based","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"SV67adn4wr"}],"key":"t3651uMVOn"},{"type":"tableCell","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"AI music generation","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"NzIM2myfKM"}],"key":"v4BkK2ZMUZ"},{"type":"tableCell","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"Structured, rhythm-aware","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"gi8ddEyQht"}],"key":"G6aGHOR8Ni"}],"key":"thjVHteFIW"},{"type":"tableRow","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"text","value":"MusicXML","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"FD37N0sHO1"}],"key":"SlLdyV4eFY"},{"type":"tableCell","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"text","value":"XML-based","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"iDdVT3tlIO"}],"key":"S9LuZARR1Y"},{"type":"tableCell","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"text","value":"Sheet music, notation, analysis","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"mdwgmhULkm"}],"key":"GqWUB0XCXk"},{"type":"tableCell","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"text","value":"Detailed, expressive, interoperable","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"miHnuJBrQE"}],"key":"aomXni528c"}],"key":"kc4rI0JKCs"},{"type":"tableRow","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Piano Roll","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"efWoNTPhxY"}],"key":"eyC3aPoDyV"},{"type":"tableCell","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Visual","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"gBPYEhSTu1"}],"key":"bNGSi3tchf"},{"type":"tableCell","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"MIDI editing, sequencing","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"jR5cCvEfpC"}],"key":"YLiw9iRidS"},{"type":"tableCell","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Intuitive, easy to manipulate","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"nSdjuysgc3"}],"key":"TdHQaTvkpU"}],"key":"st3uQPDlA6"},{"type":"tableRow","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"tableCell","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Note Graph","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"Mr4q3y7h6L"}],"key":"NYe0d5ZPzK"},{"type":"tableCell","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Graph-based","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"mHc1DjgOBB"}],"key":"BZeBCtpDcu"},{"type":"tableCell","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Analysis, AI, complex relationships","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"DOZcnt2871"}],"key":"pmrDQj9CjS"},{"type":"tableCell","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Captures structure and context","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"VPi9vs32PH"}],"key":"rtuFK9Li6s"}],"key":"vph3LvSvAc"}],"key":"IW6t8l6Vtw"}],"key":"riyk16F1m1"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Questions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WEUGqTkEg0"}],"identifier":"questions","label":"Questions","html_id":"questions","implicit":true,"key":"qxO6F7Dudj"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What is the difference between a tone and a note in music psychology?","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ePGlXGytvD"}],"key":"nzaURSySDk"}],"key":"QI4IdQJlkd"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"How does a spectrogram differ from a waveform in audio visualization?","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"PcOgqjfNUK"}],"key":"FgAzCeIaR9"}],"key":"JFETxI1ZKd"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What is the purpose of the Mel Spectrogram and why is it perceptually relevant?","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"bFYmkLa4al"}],"key":"pDTm6ZutkQ"}],"key":"ybmdxzVklo"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Describe the concept of harmony and how technology can be used to analyze it.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"GuhESZZOre"}],"key":"N90Lvupjxy"}],"key":"Hb3CmkVhbI"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What are the main differences between symbolic representations such as MIDI, ABC Notation, and MusicXML?","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ebrSRiqMrb"}],"key":"lx8MKNuUA7"}],"key":"tIhggEUXcK"}],"key":"qjoZRul8Qt"}],"key":"knVtjAZT9g"}],"key":"HWkJQ9blWf"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Week 5: Time and Rhythm","url":"/week5","group":"Sensing Sound and Music"},"next":{"title":"Week 7: Body Motion","url":"/week7","group":"Sensing Sound and Music"}}},"domain":"http://localhost:3000"},"project":{"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true}},"exports":[],"title":"Sensing Sound and Music","description":"An interdisciplinary exploration of how humans perceive, experience, and create sound and music through psychological and technological perspectives.","authors":[{"id":"MUS2640","name":"MUS2640"},{"id":"University of Oslo","name":"University of Oslo"}],"github":"https://github.com/fourMs/sensingsoundandmusic","keywords":["sensing","sound","music","psychology","technology"],"id":"11cc8b5a-b0a2-4516-8397-a7dbad782f82","toc":[{"file":"intro.md"},{"file":"week1.md"},{"file":"week2.md"},{"file":"week3.ipynb"},{"file":"week4.ipynb"},{"file":"week5.md"},{"file":"week6.ipynb"},{"file":"week7.md"},{"file":"week8.md"},{"file":"week9.md"},{"file":"week10.md"},{"file":"week11.md"}],"bibliography":[],"index":"index","pages":[{"slug":"week1","title":"Week 1: Tuning in","description":"This page introduces the foundational concepts of music psychology and technology, exploring how humans perceive, experience, and create sound and music through both psychological and technological perspectives.","date":"","thumbnail":"/sensingsoundandmusic/build/c20d5f224f701120b83307814eab6564.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week2","title":"Week 2: Listening","description":"This chapter explores the art and science of listening, focusing on how sounds and soundscapes are described, understood, and analyzed across disciplines. It introduces influential theories and thinkers, practical listening exercises, and tools for engaging with the sonic environment.","date":"","thumbnail":"/sensingsoundandmusic/build/impulsive-sustained--c0c48158496bcc681fba65df66fa6f2f.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week3","title":"Week 3: Acoustics","description":"This document provides an introduction to the fundamentals of acoustics, covering the physics of sound, the behavior of sound in rooms and instruments, and the basics of digital audio. It includes explanations, visualizations, and practical exercises to help you understand how sound is produced, transmitted, and perceived in various contexts.","date":"","thumbnail":"/sensingsoundandmusic/build/695b2b8faecb94212205e4e80f715504.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week4","title":"Week 4: Psychoacoustics","description":"This notebook introduces the fundamentals of psychoacoustics—the science of how humans perceive and interpret sound. It covers the anatomy of the ear, principles of loudness and pitch perception, auditory illusions, masking effects, and the application of psychoacoustic concepts in audio technology and music analysis.","date":"","thumbnail":"/sensingsoundandmusic/build/c03cf3c207d0b46d253830651ea7ba28.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week5","title":"Week 5: Time and Rhythm","description":"This chapter explores the foundations of musical time and rhythm, covering concepts such as onset timing, perceptual centers, meter, microrhythm, groove, and entrainment. It examines how rhythm is structured, perceived, and performed, highlighting the roles of technology and analysis tools in understanding the nuances of timing and groove in various musical styles.","date":"","thumbnail":"/sensingsoundandmusic/build/week5_image6-42ef9fdd71a421c1baf682d65217062b.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week6","title":"Week 6: Harmony and melody","description":"This notebook explores the concepts of harmony, melody, and musical structure through both theoretical explanations and practical visualizations. It covers fundamental audio representations, symbolic music formats, and provides code examples for analyzing and visualizing musical elements using Python.","date":"","thumbnail":"/sensingsoundandmusic/build/week6_image2-4fa03b8bde3b7bda89ece8a2ac6da510.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week7","title":"Week 7: Body Motion","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week8","title":"Week 8: The Brain","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/0698af3bcaf829b93eb28d09596d0541.jpeg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week9","title":"Week 9: Vision","description":"","date":"","thumbnail":"/sensingsoundandmusic/build/undefined","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week10","title":"Week 10: Physiology","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"week11","title":"Week 11: Machine Listening","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/sensingsoundandmusic/build/manifest-B2B2E6A5.js";
import * as route0 from "/sensingsoundandmusic/build/root-IB5726YR.js";
import * as route1 from "/sensingsoundandmusic/build/routes/$-LXLHKVOR.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/sensingsoundandmusic/build/entry.client-UNPC4GT3.js");</script></body></html>