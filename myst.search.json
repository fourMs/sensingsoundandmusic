{"version":"1","records":[{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Introduction"},"type":"lvl2","url":"/#introduction","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Introduction"},"content":"\n\nThis textbook is written for the course \n\nMUS2640 Sensing Sound and Music at the University of Oslo, a foundation course for later studies in music psychology and music technology. You will be introduced to fundamental principles of acoustics, psychoacoustics, and perception. This includes knowledge about how sound is produced in instruments, reflected in space, and perceived by humans. This is the basis for how we experience pitch, timbre, harmony, and rhythm in music. You will also learn about computer-based representations of sound and music, and get an overview of digital audio, sound synthesis, and analysis. The course provides theoretical knowledge and practical skills for further studies of music psychology and technology.\n\nLearning Outcome\n\nHaving completed the course, you will:\n\nbe familiar with principles of how musical sound is produced and perceived.\n\nunderstand relationships between sound-theoretical and music-theoretical concepts.\n\nbe able to process, synthesize, and analyze sound.","type":"content","url":"/#introduction","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pedagogical Strategy"},"type":"lvl2","url":"/#pedagogical-strategy","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pedagogical Strategy"},"content":"Students in the course typically have mixed backgrounds. Everyone usually has some kind of musical background, ranging from classical performance on acoustic instruments to electronic studio production. Some come from musicology, others from psychology, informatics, media studies, philosophy, linguistics, medicine, and more. Everyone is welcome! In-class activities will be adjusted to cater to the collective knowledge and experience of the student group.","type":"content","url":"/#pedagogical-strategy","position":5},{"hierarchy":{"lvl1":"Introduction","lvl3":"Active learning and “flipped classroom”","lvl2":"Pedagogical Strategy"},"type":"lvl3","url":"/#active-learning-and-flipped-classroom","position":6},{"hierarchy":{"lvl1":"Introduction","lvl3":"Active learning and “flipped classroom”","lvl2":"Pedagogical Strategy"},"content":"This course builds on the idea of active learning, an approach that emphasizes student engagement and participation in the learning process. Rather than passively receiving information through lectures, students are encouraged to interact with the material, ask questions, solve problems, and collaborate with peers. Activities may include group discussions, hands-on experiments, peer reviewing, and real-world projects. This method helps deepen understanding, improve retention, and develop critical thinking skills.\n\nWe also rely on a flipped classroom* approach, which is a teaching model where traditional lecture content is delivered outside of class, typically through readings, videos, or interactive resources. This frees up classroom time to apply concepts through discussion, exercises, and collaborative work. This structure allows students to learn foundational material at their own pace and use class sessions for deeper exploration, clarification, and practical application. The flipped classroom model fosters a more personalized and interactive learning environment, supporting diverse learning styles.","type":"content","url":"/#active-learning-and-flipped-classroom","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"A research-based and research-led course","lvl2":"Pedagogical Strategy"},"type":"lvl3","url":"/#a-research-based-and-research-led-course","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"A research-based and research-led course","lvl2":"Pedagogical Strategy"},"content":"This is a research-based course, which means that the content builds on new research results. All the teachers are active researchers and will bring in perspectives from ongoing projects. Much of this is based on scientific methods, but given the nature of the subject, we also include research that is design-centered or based in artistic practice. We will dwell on these differences at times, since understanding the different epistemological foundations for our knowledge production is important.\n\nThe course is also research-led, meaning that students will take part in ongoing research. This makes it possible for students to see how “real” research is conducted in practice. It is also valuable for the ongoing projects in the department. Although it is not required, you are encouraged to participate in research activities, contributing to ongoing projects or initiating new ones. This hands-on involvement helps bridge the gap between learning and research, fostering a culture of inquiry and innovation.","type":"content","url":"/#a-research-based-and-research-led-course","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Open Education","lvl2":"Pedagogical Strategy"},"type":"lvl3","url":"/#open-education","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Open Education","lvl2":"Pedagogical Strategy"},"content":"The course material is developed from the perspective of Open Education, meaning that all material is freely and openly available. This approach ensures that students have unrestricted access to resources, enabling them to revisit and explore the material beyond the course duration. Open Education also promotes collaboration and sharing of knowledge within and outside the academic community. This is important for societal innovation and the legitimization of ongoing research.\n\nOpen Education aligns closely with the principles of Open Research, which emphasize transparency, accessibility, and reproducibility. In this course, we aim to integrate these principles by providing access to:\n\nOpen Publications: Most of the required reading materials will be openly available, either entirely free (“free as in speech”) or through institutional agreements (“free as in beer”). This means that there should be no economic barrier to attaining relevant knowledge.\n\nOpen Data: Wherever possible, datasets used in the course will be openly shared. This allows students to analyze, visualize, and interpret data independently, fostering a deeper understanding of the material and encouraging reproducibility in their work.\n\nOpen Source Code: Tools, scripts, and examples provided during the course will be shared as open-source code. This enables students to study, modify, and build upon the code, promoting a hands-on approach to learning and encouraging contributions to the broader community.\n\nBy adopting these practices, the course not only supports students in their academic journey but also contributes to the global movement toward open and equitable access to knowledge. This approach empowers students to become active participants in the creation and dissemination of knowledge, preparing them for future roles as researchers, educators, and innovators.","type":"content","url":"/#open-education","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"Embracing AI","lvl2":"Pedagogical Strategy"},"type":"lvl3","url":"/#embracing-ai","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"Embracing AI","lvl2":"Pedagogical Strategy"},"content":"In this course, we will actively explore the use of artificial intelligence both for exploring the content of the course but also as a pedagogical tool. There are currently many available AI tools, yet they are underexploited. We will try different tools and evaluate their effectiveness.\n\nThis textbook is an example of AI-based co-creation. Sections of the text are partly written by large language models. However, the text has been going through a human “peer review” to ensure that everything makes sense. Throughout the course, we will explore when AI can safely be used for text generation and when it fails.\n\nIn this class, you are encouraged to use AI actively. AI-powered platforms can adapt content and feedback to individual learning styles and paces, helping you master concepts more effectively. However, AI-based tools should be used wisely; they are there to help learning, not to replace it. After all, the exam will be performed without any tools. Then you will need to think and write on your own!","type":"content","url":"/#embracing-ai","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"Tools"},"type":"lvl2","url":"/#tools","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"Tools"},"content":"We will explore various tools throughout the semester. You will not learn any of these in detail, but you will see how they work and understand their applications. These tools are designed to provide a broad overview of the possibilities in music technology and sound analysis.\n\nPC Software\n\nWe will use the following software, most of which are free and/or open source. They are also all cross-platform, that is, they work on both Windows, OSX, and Linux-based systems.\n\nSonic Visualiser: A powerful application for viewing and analyzing the contents of audio files. It allows you to visualize waveforms, spectrograms, and other audio features, making it a valuable tool for music analysis and research.\n\nAudacity: A free, open-source, cross-platform audio editor and recorder. Audacity is widely used for recording, editing, and processing audio files, making it a versatile tool for both beginners and advanced users in music and sound analysis.\n\nPython - Jupyter Notebook: An open-source web application that enables you to create and share documents containing live code, equations, visualizations, and narrative text. It is widely used for data analysis, including audio and music data.\n\nPure Data (Pd): An open-source visual programming language for audio and multimedia. It is widely used for sound synthesis, audio processing, and interactive installations. It resembles the commercial package \n\nMax.\n\nAudiostellar: A unique tool for exploring and organizing sound samples using a visual interface. It helps you discover relationships between sounds and create new compositions.\n\nFreesound: A collaborative online database of sound samples. It provides access to a wide variety of sounds that can be used for music production, sound design, and research.\n\nmoises.ai: An AI-powered platform for music source separation and audio processing. It allows you to isolate vocals, instruments, and other elements from audio tracks, making it useful for practice, remixing, and analysis.\n\nMaître Gnome: An experimental web-based tool for exploring and manipulating sound and music. It offers creative ways to interact with audio, suitable for both educational and artistic purposes.\n\nPhone appsWe will use these free and cross-platform apps, working on both Android and iOS devices.\n\nNoise Capture: A mobile app for recording and analyzing environmental noise. It is useful for studying soundscapes and understanding the impact of noise in different environments.\n\nSensorLogger: A mobile app for recording and visualizing sensor data from your smartphone, such as accelerometer, gyroscope, magnetometer, and more. SensorLogger enables you to collect, export, and analyze movement and environmental data, making it useful for experiments in sound, music, and motion research.\n\nHardwareWe will explore these devices in class. You are not expected to purchase these yourself; we have them available to borrow.\n\nLittleBits: A platform of modular electronic components that snap together to create interactive projects. It is a fun and creative way to explore sound synthesis and music-making.\n\nAmbisonics - Zoom H3-VR: A portable recorder designed for capturing 360-degree spatial audio. It is ideal for creating immersive soundscapes and exploring 3D audio reproduction.\n\nOptiTrack Motion Capture: A high-precision motion capture system used for tracking movement in 3D space. It is widely used in research, gaming, and performance arts to analyze and visualize motion, including applications in music and sound interaction.\n\nEquivital Life Monitors: Wearable devices designed to monitor physiological data such as heart rate, respiration, and body temperature. These devices are useful for studying the relationship between physiological responses and musical experiences.","type":"content","url":"/#tools","position":15},{"hierarchy":{"lvl1":"Introduction","lvl2":"Curriculum"},"type":"lvl2","url":"/#curriculum","position":16},{"hierarchy":{"lvl1":"Introduction","lvl2":"Curriculum"},"content":"The current textbook comprises the core curriculum for this course. Interested readers can find a lot of more information in the texts listed below as well as in the references section for each chapter.\n\nSome short introduction books\n\nMusic Psychology: A Very Short Introduction: Elizabeth Hellmuth Margulis (2018), Oxford: Oxford University Press. A concise introduction to the field of music psychology, exploring how music affects the mind and behavior.\n\nMusic and Technology: A Very Short Introduction: Mark Katz (2014), Oxford: Oxford University Press. An accessible overview of the relationship between music and technology, examining its impact on creation, performance, and listening.\n\nWays of Listening: An Ecological Approach to the Perception of Musical Meaning: Eric F. Clarke (2005), Oxford: Oxford University Press. A unique perspective on how listeners perceive and interpret musical meaning in ecological contexts.\n\nSound Actions: Conceptualizing Musical Instruments: Alexander Refsum Jensenius (2022), Cambridge: The MIT Press. An insightful examination of musical instruments as tools for interaction, creativity, and expression.\n\nSome large reference worksSome books serve more as large-scale references of their respective fields:\n\nHandbook of Systematic Musicology: Edited by multiple authors (2020), Springer. A detailed reference for interdisciplinary approaches to the study of musicology.\n\nThe Oxford Handbook of Music Psychology: Edited by Susan Hallam, Michael Thaut, Ian Cross (2018), Oxford: Oxford University Press. A comprehensive overview of research and theories in music psychology.\n\nThe Computer Music Tutorial, Second Edition: Curtis Roads (2023), Cambridge: The MIT Press. An extensively updated and expanded reference covering the latest developments in computer music, including new technologies, techniques, and applications.\n\nRelevant Norwegian-language booksThere are not many relevant books in Norwegian, but here are some:\n\nMusikkvitenskap: Introduksjon og Perspektiver: Even Ruud (2005), Oslo: Universitetsforlaget. An introductory Norwegian textbook covering key perspectives and approaches in musicology.\n\nMusikk og Bevegelse: Alexander Refsum Jensenius (2009), Oslo: Unipub. A Norwegian book exploring the relationship between music and movement.\n\nLydlandskap: Om Bruk og Misbruk av Musikk: Even Ruud (2005), Bergen: Fagbokforlaget. A Norwegian book discussing the use and misuse of music in various contexts.","type":"content","url":"/#curriculum","position":17},{"hierarchy":{"lvl1":"Introduction","lvl2":"Overview"},"type":"lvl2","url":"/#overview","position":18},{"hierarchy":{"lvl1":"Introduction","lvl2":"Overview"},"content":"This textbook is divided into chapters corresponding to the lectures in the course.\n\nSensing Sound and Music\n\nWeek 1: Tuning in\n\nWeek 2: Listening\n\nWeek 3: Acoustics\n\nWeek 4: Psychoacoustics\n\nWeek 5: Time and Rhythm\n\nWeek 6: Harmony and melody\n\nWeek 7: Body Motion\n\nWeek 8: The Brain\n\nWeek 9: Vision\n\nWeek 10: Physiology\n\nWeek 11: Machine Listening","type":"content","url":"/#overview","position":19},{"hierarchy":{"lvl1":"Introduction","lvl2":"Learn more"},"type":"lvl2","url":"/#learn-more","position":20},{"hierarchy":{"lvl1":"Introduction","lvl2":"Learn more"},"content":"MUS2133 Music Psychology\n\nThis course explores the psychological aspects of music perception and cognition, including auditory perception, emotional responses to music, and the cognitive processes involved in musical activities such as listening, performing, and composing.\n\nMUS2830 Computer Music\n\nThis course delves into the intersection of music and technology, covering topics such as sound synthesis, digital audio processing, and algorithmic composition. Gain hands-on experience with tools and techniques used in creating and analyzing music through computational methods.","type":"content","url":"/#learn-more","position":21},{"hierarchy":{"lvl1":"Week 1: Tuning in"},"type":"lvl1","url":"/week1","position":0},{"hierarchy":{"lvl1":"Week 1: Tuning in"},"content":"","type":"content","url":"/week1","position":1},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Sensing Sound and music"},"type":"lvl2","url":"/week1#sensing-sound-and-music","position":2},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Sensing Sound and music"},"content":"This course is called “Sensing Sound and Music”. But what does that mean? And what do those three words mean on their own?\n\nQuestion\n\nCan you define the three terms “sensing”, “sound”, and “music”?\n\nSound and music are fundamental to human experience, serving as both a medium of communication and a form of artistic expression. Sound is a physical phenomenon characterized by vibrations that travel through a medium, while music is an organized arrangement of sounds that evoke emotional, cognitive, and cultural responses. Humans sense sound through our auditory system, but as we will discuss throughout the course, also through our bodies. Exactly how we “sense” music is still one of the core questions of both musicology at large and music psychology more specifically. It is also of interest in music technology.\n\nResearch in music psychology highlights how humans perceive and process sound, exploring topics such as pitch, rhythm, timbre, and harmony. Technological advancements, such as digital audio workstations and sound synthesis, have expanded the boundaries of music creation and analysis, enabling new forms of artistic exploration.","type":"content","url":"/week1#sensing-sound-and-music","position":3},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"An interdisciplinary approach"},"type":"lvl2","url":"/week1#an-interdisciplinary-approach","position":4},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"An interdisciplinary approach"},"content":"This course aims to be interdisciplinary, meaning that it draws on methods, theories, and perspectives from multiple academic fields to address complex questions about sound and music. Interdisciplinarity goes beyond simply combining knowledge from different areas; in this case, between musicology, psychology, and technology. The idea is that by approaching sound and music from these varied viewpoints, we gain a richer understanding of how humans perceive, experience, and create music, as well as how technology can enhance and transform these processes.","type":"content","url":"/week1#an-interdisciplinary-approach","position":5},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Some etymology","lvl2":"An interdisciplinary approach"},"type":"lvl3","url":"/week1#some-etymology","position":6},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Some etymology","lvl2":"An interdisciplinary approach"},"content":"Etymology is the study of the origin and history of words and how their meanings and forms have changed over time. The word “etymology” is itself derived from the Ancient Greek words ἔτυμον (étymon), meaning ‘true sense or sense of a truth’, and the suffix -logia, denoting ‘the study or logic of’. Similarly, we can investigate the three disciplines in question:\n\nMusicology: The term “musicology” comes from the Greek words mousikē (music) and -logia (study or science). It is the scholarly study of music, covering its history, theory, and cultural context. Musicologists analyze how music is composed, performed, and understood across different societies and eras.\n\nPsychology: “Psychology” is derived from the Greek psyche (soul, mind) and -logia (study). It is the scientific study of the mind and behavior. In the context of music, psychology explores how people perceive, process, and respond to music emotionally and cognitively.\n\nTechnology: The word “technology” originates from the Greek techne (art, craft, skill) and -logia (study). Technology refers to the tools, techniques, and systems humans create to solve problems or enhance capabilities. In music, technology includes instruments, recording devices, software, and other innovations that help create, analyze, or experience music.\n\nQuestion\n\nCan you think of other words with a “logy” at the end? What do they mean?\n\nBy the end of the course, you will have learned some of the basic terminology, theories, and methods used in all these directions.","type":"content","url":"/week1#some-etymology","position":7},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Differences between disciplinarities","lvl2":"An interdisciplinary approach"},"type":"lvl3","url":"/week1#differences-between-disciplinarities","position":8},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Differences between disciplinarities","lvl2":"An interdisciplinary approach"},"content":"When approaching different disciplines, it is important to understand how they interact. Many people call all sorts of collaboration between disciplines “interdisciplinarity”. However, there are some differences, highlighted in this figure:\n\n\n\nIntradisciplinary work stays within a single discipline, while crossdisciplinary approaches view one discipline from the perspective of another. Multidisciplinary collaboration involves people from different disciplines working together, each drawing on their own expertise. Interdisciplinary work integrates knowledge and methods from multiple disciplines, synthesizing approaches for a deeper understanding. Transdisciplinary approaches go further, creating unified intellectual frameworks that transcend individual disciplinary boundaries.\n\nOne approach is not better than another, and many researchers combine their approach depending on the project type. Still, it is helpful to consider these differences as we approach each discipline and look at how they combine theories and methods.\n\nQuestion\n\nHow do you define your own background in terms of disciplinarity?","type":"content","url":"/week1#differences-between-disciplinarities","position":9},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Concepts, theories, methods","lvl2":"An interdisciplinary approach"},"type":"lvl3","url":"/week1#concepts-theories-methods","position":10},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Concepts, theories, methods","lvl2":"An interdisciplinary approach"},"content":"An academic discipline is generally characterized by a distinct set of concepts, theories, and methods that guide inquiry and knowledge production within a particular field. Disciplines have established traditions, specialized terminology, and recognized standards for evaluating research and scholarship. They often possess dedicated journals, conferences, and professional organizations that foster communication and development among practitioners. The boundaries of a discipline are shaped by its historical evolution, core questions, and the types of problems it seeks to address, providing a framework for systematic study and advancement of understanding.\n\nIn our case, each of the three involved disciplines (musicology, psychology, and technology) brings its own theories and methods. Generally speaking, musicology uses analysis, historiography, and ethnography, psychology employs experiments and surveys, and technology relies on engineering and computational modeling. Understanding these differences helps clarify how interdisciplinary research can integrate diverse perspectives to enrich our knowledge of sound and music.","type":"content","url":"/week1#concepts-theories-methods","position":11},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Fundamentals of Music Psychology"},"type":"lvl2","url":"/week1#fundamentals-of-music-psychology","position":12},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Fundamentals of Music Psychology"},"content":"Psychology in music focuses on understanding how humans perceive, process, and respond to sound and music. This includes exploring topics such as:\n\nPerception: How the brain interprets sound waves as musical elements like pitch, rhythm, and timbre.\n\nCognition: The mental processes involved in understanding and remembering music, including pattern recognition and emotional responses.\n\nBehavior: How music influences actions, such as movement, performance, and social interaction.\n\nEmotion: The study of how music evokes feelings and moods, and its role in emotional regulation and expression.\n\nThese principles help us understand the universal and individual ways in which music impacts human experience, providing insights into both the psychological and cultural dimensions of music. In this course, we will primarily investigate perception, but also touch briefly upon cognition and behavior.\n\nMusic psychology is a thriving field internationally. There are numerous international communities, conferences, and journals for music psychology. These conferences provide excellent opportunities for networking, collaboration, and staying updated on the latest research.\n\nConferences and Communities in Music Psychology\n\nNeuromusic: A conference dedicated to the intersection of neuroscience and music, exploring topics such as music perception, cognition, and therapy.\n\nESCOM (European Society for the Cognitive Sciences of Music): A society that organizes conferences and promotes research in the cognitive sciences of music.\n\nICMPC (International Conference on Music Perception and Cognition): A biennial conference that brings together researchers from around the world to discuss music perception and cognition.\n\nSMPC (Society for Music Perception and Cognition): A society that hosts conferences and fosters research on the psychological and cognitive aspects of music.\n\nICCMR (International Conference on Cognitive Musicology and Research): A conference focusing on the intersection of cognitive science and musicology, exploring how humans understand and interact with music.\n\nMusic and Neurosciences: A conference series exploring the relationship between music and brain function, organized by the Mariani Foundation.\n\nJournals in Music Psychology\n\nMusic Perception: A leading journal that publishes research on the perception and cognition of music, including studies on auditory processing, musical memory, and emotional responses.\n\nJournal of New Music Research: Explores the intersection of music psychology, technology, and theory, with an emphasis on computational and experimental approaches.\n\nEmpirical Musicology Review: Publishes empirical studies on music perception, cognition, and performance, as well as reviews of current research.\n\nPsychology of Music: Covers a wide range of topics in music psychology, including music education, therapy, and cultural studies.\n\nFrontiers in Psychology: Auditory Cognitive Neuroscience: A section of the Frontiers in Psychology journal that focuses on auditory perception, music cognition, and related neuroscience.\n\nMusic & Science: An interdisciplinary journal that publishes research on the scientific study of music, including its psychological, cultural, and technological dimensions.\n\nMusicae Scientiae: The journal of the European Society for the Cognitive Sciences of Music, publishing research on music psychology, cognition, perception, and interdisciplinary studies.\n\nGiven its interdisciplinary nature, researchers are typically working in either musicology or psychology departments, which also often “skews” the research in one or the other direction. Researchers on the musicology side tend to be more focused on real-world musical experiences, what is often termed “ecological validity,” and using (more) qualitative methods. Researchers in psychology typically work more on controlled experiments and using quantitative methods.\n\nWhile many researchers in music psychology may often feel “alone” in their respective departments, there are a few larger and specialized departments or research centers that focus on music psychology specifically. In these institutions, one can often see the width of the field, covering many different theoretical and methodological perspectives.","type":"content","url":"/week1#fundamentals-of-music-psychology","position":13},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Fundamentals of Music Technology"},"type":"lvl2","url":"/week1#fundamentals-of-music-technology","position":14},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Fundamentals of Music Technology"},"content":"Technology in music involves the use of tools and systems to create, analyze, and manipulate sound. Key areas include:\n\nSound Synthesis: Generating sound electronically using techniques like additive, subtractive, and granular synthesis.\n\nDigital Audio Processing: Editing and transforming sound using software tools for tasks such as filtering, equalization, and effects.\n\nMusic Information Retrieval (MIR): Extracting meaningful information from audio data, such as tempo, key, and genre classification.\n\nInteractive Systems: Designing systems that respond to user input, enabling real-time music creation and performance.\n\nThese technologies expand the possibilities for music creation and analysis, offering new ways to explore and innovate within the field. There are also many international journals, communities, and annual conferences in music technology.\n\nConferences and Communities in Music Technology\n\nICMC (International Computer Music Conference): Focuses on computer music research, composition, and performance, bringing together artists, scientists, and technologists.\n\nSMC (Sound and Music Computing Conference): Covers topics in sound and music computing, including audio analysis, synthesis, and interactive systems.\n\nNIME (New Interfaces for Musical Expression): Explores new musical instruments and interfaces, emphasizing innovation in music performance and interaction.\n\nCMMR (Computer Music Multidisciplinary Research): Promotes multidisciplinary research in computer music, including psychology, acoustics, and engineering.\n\nICAD (International Conference on Auditory Display): Focuses on the use of sound to convey information, covering topics such as sonification and auditory interfaces.\n\nDAFx (Digital Audio Effects Conference): Dedicated to research on digital audio effects, signal processing, and music technology.\n\nISMIR (International Society for Music Information Retrieval): Advances research in music information retrieval, including audio analysis, machine learning, and music data processing.\n\nAES (Audio Engineering Society): An international organization for audio engineers, hosting conferences on sound recording, processing, and reproduction.\n\nJournals in Music Technology\n\nComputer Music Journal: Focuses on digital audio, sound synthesis, and computer-assisted composition, providing insights into the intersection of music and technology.\n\nJournal of the Audio Engineering Society (JAES): Covers a wide range of topics in audio engineering, including sound recording, processing, and reproduction.\n\nOrganised Sound: Explores the theory and practice of electroacoustic music and sound art, with an emphasis on innovative approaches.\n\nJournal of New Music Research: Publishes research on music technology, computational musicology, and the development of new musical tools and systems.\n\nLeonardo Music Journal: Focuses on the creative use of technology in music and sound art, highlighting experimental and interdisciplinary work.\n\nFrontiers in Digital Humanities: Digital Musicology: Explores the application of digital tools and methods to music analysis, composition, and performance.\n\nTransactions of the International Society for Music Information Retrieval (TISMIR): Publishes open-access research on music information retrieval, covering topics such as audio analysis, machine learning, and music data processing.\n\nWhen comparing them, there are some important differences between music technology and music psychology as disciplines. Music psychology is primarily a scientific field of study, focused on understanding how humans perceive, process, and respond to music through empirical research and theoretical frameworks. Its methods are rooted in experimental design, data analysis, and psychological theory, aiming to uncover universal principles and individual variations in musical experience.\n\nMusic technology, on the other hand, spans several domains: science, art, design, and engineering. It encompasses scientific research into sound and audio processing, artistic exploration through composition and performance, design of musical instruments and interfaces, and engineering of hardware and software systems. Music technologists may work on developing new tools for music creation, analyzing audio signals, designing interactive installations, or exploring creative possibilities in digital media. This diversity means that music technology is not limited to scientific inquiry but also includes creative practice, technical innovation, and user-centered design.\n\nGiven its many directions, music technologists typically work in many different types of departments: computer science, electrical or mechanical engineering, design and art schools, as well as within musicology departments and music conservatories. As for psychologists, this institutional basis often also influences the directions taken.","type":"content","url":"/week1#fundamentals-of-music-technology","position":15},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Listening to the world"},"type":"lvl2","url":"/week1#listening-to-the-world","position":16},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Listening to the world"},"content":"We will spend next week’s class only on listening, but we will start with a little warm-up here. After all, listening is an essential human capacity and central to music. However, most people do not think much about listening in daily life, except for when it is annoying. Too low sound is difficult to hear what is said, and too low sound is unpleasant and even dangerous. But why is that, and how do we talk about it in precise terms? That is what we will explore both here and later in class.\n\nStand up. Close your eyes. Listen. What do you hear?","type":"content","url":"/week1#listening-to-the-world","position":17},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Hearing vs listening","lvl2":"Listening to the world"},"type":"lvl3","url":"/week1#hearing-vs-listening","position":18},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Hearing vs listening","lvl2":"Listening to the world"},"content":"Let us start by considering the difference between hearing and listening. Hearing is the passive physiological process of detecting sound waves through the auditory system, while listening is an active cognitive process that involves interpreting and making meaning of those sounds.\n\nIn the psychology literature, you will probably find references to several different types of hearing and listening:\n\nPassive hearing: This occurs when sounds are registered by the ears and processed by the brain without conscious attention. For example, background noise in a café or the hum of an air conditioner.\n\nSelective hearing: The ability to focus on specific sounds or voices while ignoring others, such as following a conversation in a noisy environment.\n\nActive listening: Engaging attention and intention to understand, analyze, or respond to sounds or speech. This is essential in music appreciation, communication, and learning.\n\nCritical listening: Evaluating and analyzing sound quality, musical structure, or meaning, often used in music production, performance, or academic study.\n\nEmpathetic listening: Focusing on the emotional content and intent behind sounds or speech, important in social interactions and therapeutic contexts.\n\nIt is important to note that the mechanisms underlying these different approaches to hearing and listening are still actively researched. So there are many—sometimes diverging—opinions about them, their differences, and how they overlap or connect. In any case, understanding these distinctions helps clarify how we interact with sound and music, moving from automatic sensory processing to deeper engagement and interpretation.","type":"content","url":"/week1#hearing-vs-listening","position":19},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Embodied music cognition","lvl2":"Listening to the world"},"type":"lvl3","url":"/week1#embodied-music-cognition","position":20},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Embodied music cognition","lvl2":"Listening to the world"},"content":"The approach to human psychology presented in this book is not neutral (not that any other book is that, either, even though they pretend to be). It is heavily based on a tradition called \n\nembodied music cognition, emphasizing the role of the body in musical experience. Although he did not invent the term, it became popularized after  wrote a book with the same name.\n\nEmbodied music cognition was seen as radical at first but has become more mainstream over the years. Several researchers at the University of Oslo—with \n\nProfessor emeritus Rolf Inge Godøy in front—have been active in developing this branch of research over the last decades. The approach has been foundational to the studies of music-related body motion in the \n\nfourMs Lab and is also at the core of many activities at \n\nRITMO Centre for Interdisciplinary Studies of Rhythm, Time and Motion.\n\nThe core idea of embodied music cognition is that both the production and perception of music are based on integrating sensory, motor, and emotional dimensions. To paraphrase the father of \n\n“ecological psychology”, James Gibson, you explore the world with eyes in a head on a body that moves around. He was mainly interested in visual perception, but his ideas have inspired a similar thinking about other senses.\n\nA different entry point to a similar idea is the concept of \n\nmusicking. This term popularized by musicologist \n\nChristopher Small, who argued that music is best understood as a verb—to music—not a noun . Music is not an object or a finished product (like a song or a score), but rather an activity—something people do. When a band plays a concert, the musicians, the dancers, the audience, the sound engineers, and even the people selling tickets and cleaning the hall are all engaged in musicking. The music exists not just in the notes played, but in the shared experience and interaction.\n\nResearchers within embodied music cognition are eager to not only study “the music” but instead explore the complexity of musical interactions, from the role of gestures in musical performance to how perceivers synchronize their bodies to a musical beat. By engaging the body, embodied music cognition provides a holistic framework for studying how music is experienced and understood.","type":"content","url":"/week1#embodied-music-cognition","position":21},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Multimodality","lvl2":"Listening to the world"},"type":"lvl3","url":"/week1#multimodality","position":22},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Multimodality","lvl2":"Listening to the world"},"content":"Multimodality is a core feature of embodied music cognition and refers to the integration and interaction of multiple sensory modalities in the perception and understanding of the world. This table summarizes the main human senses, their associated sensory modalities, and the organs involved in each.\n\nSense\n\nModality\n\nHuman Organ(s)\n\nHearing\n\nAuditory\n\nEars\n\nSight\n\nVisual\n\nEyes\n\nTouch\n\nTactile\n\nSkin, Hands\n\nTaste\n\nGustatory\n\nTongue, Mouth\n\nSmell\n\nOlfactory\n\nNose\n\nBalance\n\nVestibular\n\nInner Ear\n\nProprioception\n\nKinesthetic\n\nMuscles, Joints\n\nFrom a psychological perspective, multimodality emphasizes how the brain combines information from these different sensory channels to create a cohesive and enriched experience. In the context of music cognition, this could involve the interplay between auditory and motor systems, such as how visual cues from a performer influence the perception of sound or how physical gestures enhance musical expression and understanding. However, the other senses are also involved. Touch provides tactile feedback for instrumentalists and singers; proprioception and balance help musicians coordinate movement and posture, especially in dance or performance; and even smell or taste can contribute to the emotional context and memory of musical events. These multisensory interactions enrich both performance and perception, deepening engagement for musicians and audiences alike.\n\nMost people agree that sound is a core component of music, yet there are notable examples of musicians and composers who have created and performed music despite significant hearing loss. \n\nLudwig van Beethoven, one of the most influential composers in Western music history, continued to compose groundbreaking works even after becoming profoundly deaf. In more recent times, percussionist \n\nEvelyn Glennie has demonstrated that musical experience and expression are not limited by the ability to hear sound in a conventional way. Glennie, who lost most of her hearing by the age of twelve, developed unique techniques to sense vibrations through her body, allowing her to perform and interpret music at the highest level. These examples challenge traditional notions of music as purely an auditory phenomenon and highlight the importance of multimodal and embodied approaches to musical experience.","type":"content","url":"/week1#multimodality","position":23},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Action and Perception","lvl2":"Listening to the world"},"type":"lvl3","url":"/week1#action-and-perception","position":24},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl3":"Action and Perception","lvl2":"Listening to the world"},"content":"The last concept we will introduce this week is the action–perception loop. The idea of embodied cognition in general is that we sense through action, and act through sensing. In music, the action-perception loop is evident in activities such as playing an instrument, where sensory feedback informs motor actions, and vice versa. For example, a pianist adjusts their touch based on the sound produced, creating a continuous cycle of interaction.\n\nEric Clarke has used the action–perception loop as core to his thinking about ecological listening. He bridges psychology, musicology, and acoustic ecology, offering insights into how humans engage with sound as part of their lived experience. In his influential book Ways of Listening, he argues for three different listening modes:\n\nDirect perception: Listeners perceive sounds in relation to their environment without needing extensive cognitive processing.\n\nAffordances: Sounds provide cues about actions or interactions possible within a given environment.\n\nContextual listening: The meaning of sounds is shaped by their environmental and situational context.\n\nAs we will see next week, there are many similar ways of thinking about how we listen.","type":"content","url":"/week1#action-and-perception","position":25},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Questions"},"type":"lvl2","url":"/week1#questions","position":26},{"hierarchy":{"lvl1":"Week 1: Tuning in","lvl2":"Questions"},"content":"What are the main differences between musicology, psychology, and technology as academic disciplines?\n\nHow can interdisciplinarity enhance our understanding of sound and music compared to single-discipline approaches?\n\nWhat is the distinction between hearing and listening, and why does it matter when studying music?\n\nHow does embodied music cognition explain relationships between physical motion and musical experiences?\n\nWhat is the action-perception loop, and how does it manifest in musical activities?","type":"content","url":"/week1#questions","position":27},{"hierarchy":{"lvl1":"Week 10: Physiology"},"type":"lvl1","url":"/week10","position":0},{"hierarchy":{"lvl1":"Week 10: Physiology"},"content":"","type":"content","url":"/week10","position":1},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Physiological Reactions to Music"},"type":"lvl2","url":"/week10#physiological-reactions-to-music","position":2},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Physiological Reactions to Music"},"content":"Music has a profound impact on the human body, eliciting various physiological responses. These reactions can include changes in heart rate, blood pressure, respiration, and even skin conductance.","type":"content","url":"/week10#physiological-reactions-to-music","position":3},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Skin Conductance"},"type":"lvl2","url":"/week10#skin-conductance","position":4},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Skin Conductance"},"content":"Skin conductance, a measure of emotional arousal, can change in response to music. Exciting or emotionally charged music may increase skin conductance, indicating heightened emotional engagement, while calming music may reduce it.","type":"content","url":"/week10#skin-conductance","position":5},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Cardiac responses"},"type":"lvl2","url":"/week10#cardiac-responses","position":6},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Cardiac responses"},"content":"","type":"content","url":"/week10#cardiac-responses","position":7},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Heart Rate","lvl2":"Cardiac responses"},"type":"lvl3","url":"/week10#heart-rate","position":8},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Heart Rate","lvl2":"Cardiac responses"},"content":"Music can influence heart rate by either calming or exciting the listener. Slow, soothing music often reduces heart rate, promoting relaxation, while fast-paced or intense music can increase heart rate, reflecting heightened arousal or excitement.","type":"content","url":"/week10#heart-rate","position":9},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Shared Absorption and Cardiac Interrelations in String Quartets","lvl2":"Cardiac responses"},"type":"lvl3","url":"/week10#shared-absorption-and-cardiac-interrelations-in-string-quartets","position":10},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Shared Absorption and Cardiac Interrelations in String Quartets","lvl2":"Cardiac responses"},"content":"Høffding et al. (2023) investigated the phenomenon of shared absorption and its impact on cardiac interrelations within expert and student string quartets. The study examined how group dynamics and collective focus influence physiological synchronization, particularly heart rate patterns, during ensemble performances. Findings suggest that shared absorption fosters a unique form of interpersonal connection, enhancing both musical cohesion and emotional engagement among performers.\n\nFor further details, refer to the original article: \n\nInto the Hive-Mind: Shared Absorption and Cardiac Interrelations in Expert and Student String Quartets.","type":"content","url":"/week10#shared-absorption-and-cardiac-interrelations-in-string-quartets","position":11},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"type":"lvl3","url":"/week10#cardiac-oscillations-in-large-ensembles","position":12},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"content":"Cardiac synchronization, the phenomenon where heart rate patterns align between individuals, has been observed in various musical contexts. Research highlights its occurrence both among performers and between performers and audiences, shedding light on the physiological and emotional connections fostered by music.","type":"content","url":"/week10#cardiac-oscillations-in-large-ensembles","position":13},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Synchronization Among Choristers","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"type":"lvl4","url":"/week10#synchronization-among-choristers","position":14},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Synchronization Among Choristers","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"content":"Müller and Lindenberger (2011) demonstrated that cardiac and respiratory patterns synchronize between individuals during choir singing. This synchronization reflects the deep interpersonal connection and shared focus required in ensemble performances, emphasizing the role of collective musical engagement in fostering physiological coherence.\n\nFor further details, refer to the original article: \n\nCardiac and Respiratory Patterns Synchronize Between Persons During Choir Singing.","type":"content","url":"/week10#synchronization-among-choristers","position":15},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Audience Synchronization During Performances","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"type":"lvl4","url":"/week10#audience-synchronization-during-performances","position":16},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Audience Synchronization During Performances","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"content":"Czepiel et al. (2024) explored how piano performances can induce cardiac synchronization among audience members. Their findings suggest that audio-visual concert experiences create a shared physiological response, enhancing the collective emotional experience of the audience.\n\nFor further details, refer to the original article: \n\nAudio-Visual Concert Performances Synchronize an Audience’s Heart Rates.","type":"content","url":"/week10#audience-synchronization-during-performances","position":17},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Open Questions","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"type":"lvl4","url":"/week10#open-questions","position":18},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl4":"Open Questions","lvl3":"Cardiac Oscillations in Large Ensembles","lvl2":"Cardiac responses"},"content":"An intriguing question remains: how does cardiac coherence between musicians and audiences relate and evolve during a performance? Understanding this dynamic could provide deeper insights into the interplay of physiological and emotional connections in live musical settings.","type":"content","url":"/week10#open-questions","position":19},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Heart Rate Variability","lvl2":"Cardiac responses"},"type":"lvl3","url":"/week10#heart-rate-variability","position":20},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Heart Rate Variability","lvl2":"Cardiac responses"},"content":"Heart rate variability (HRV) refers to the variation in time intervals between consecutive heartbeats. It is an important indicator of autonomic nervous system activity and overall cardiovascular health. Music has been shown to influence HRV, with different types of music eliciting varying effects.\n\nRelaxing music, such as classical or ambient genres, can increase HRV, indicating a shift towards parasympathetic nervous system dominance, which is associated with relaxation and stress reduction. On the other hand, stimulating or high-tempo music may decrease HRV, reflecting sympathetic nervous system activation, which is linked to heightened arousal or stress.\n\nThe impact of music on HRV is being explored in therapeutic contexts, such as stress management, anxiety reduction, and even cardiac rehabilitation, highlighting its potential as a non-invasive tool for improving well-being.","type":"content","url":"/week10#heart-rate-variability","position":21},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Blood Pressure","lvl2":"Cardiac responses"},"type":"lvl3","url":"/week10#blood-pressure","position":22},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Blood Pressure","lvl2":"Cardiac responses"},"content":"Listening to music can also affect blood pressure. Relaxing music has been shown to lower blood pressure, which can be beneficial for stress relief. Conversely, loud or stimulating music may temporarily raise blood pressure.","type":"content","url":"/week10#blood-pressure","position":23},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Respiration"},"type":"lvl2","url":"/week10#respiration","position":24},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Respiration"},"content":"Music can synchronize with breathing patterns. For example, slow tempos may encourage deeper, slower breaths, while faster tempos can lead to quicker, shallower breathing. This connection is often used in therapeutic settings to regulate respiration.","type":"content","url":"/week10#respiration","position":25},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Effects of Lung Volume on the Electroglottographic Waveform","lvl2":"Respiration"},"type":"lvl3","url":"/week10#effects-of-lung-volume-on-the-electroglottographic-waveform","position":26},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Effects of Lung Volume on the Electroglottographic Waveform","lvl2":"Respiration"},"content":"Ternström, D’Amario, and Selamtzis (2018) investigated how lung volume influences the electroglottographic (EGG) waveform in trained female singers. Their findings revealed that lung volume significantly affects vocal fold behavior during phonation. Specifically, higher lung volumes were associated with increased subglottal pressure, which altered the EGG waveform characteristics. These changes suggest that lung volume plays a critical role in vocal control and may impact vocal performance and technique. The study highlights the importance of understanding respiratory mechanics in vocal training and pedagogy.\n\nFor further details, refer to the original article: \n\nEffects of the Lung Volume on the Electroglottographic Waveform in Trained Female Singers.","type":"content","url":"/week10#effects-of-lung-volume-on-the-electroglottographic-waveform","position":27},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Interperformer Coordination in Piano-Singing Duo Performances","lvl2":"Respiration"},"type":"lvl3","url":"/week10#interperformer-coordination-in-piano-singing-duo-performances","position":28},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Interperformer Coordination in Piano-Singing Duo Performances","lvl2":"Respiration"},"content":"D’Amario et al. (2023) explored the dynamics of interperformer coordination in piano-singing duo performances, focusing on how phrase structure and empathy influence synchronization. The study revealed that performers’ ability to anticipate and adapt to each other’s timing is crucial for achieving cohesive musical expression. Empathy between performers was found to enhance coordination, particularly during complex or expressive passages. These findings underscore the importance of interpersonal connection and shared musical understanding in collaborative performances.\n\nFor further details, refer to the original article: \n\nInterperformer Coordination in Piano-Singing Duo Performances: Phrase Structure and Empathy Impact.","type":"content","url":"/week10#interperformer-coordination-in-piano-singing-duo-performances","position":29},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Measuring physiological data"},"type":"lvl2","url":"/week10#measuring-physiological-data","position":30},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Measuring physiological data"},"content":"","type":"content","url":"/week10#measuring-physiological-data","position":31},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Equivital LifeMonitor","lvl2":"Measuring physiological data"},"type":"lvl3","url":"/week10#equivital-lifemonitor","position":32},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl3":"Equivital LifeMonitor","lvl2":"Measuring physiological data"},"content":"The Equivital LifeMonitor is a wearable physiological monitoring device designed to capture a range of biometric data, including respiration and heart rate. It is commonly used in research, healthcare, and performance monitoring due to its accuracy and portability.\n\nThe device uses respiratory inductance plethysmography (RIP) technology, which involves elastic bands embedded with sensors that measure the expansion and contraction of the chest and abdomen during breathing. By analyzing these movements, the LifeMonitor can determine respiratory rate and patterns.\n\nThe LifeMonitor employs electrocardiography (ECG) sensors to measure the electrical activity of the heart. These sensors are integrated into the chest strap, allowing the device to detect heartbeats and calculate heart rate. The ECG data can also provide insights into heart rate variability (HRV), which is an important indicator of autonomic nervous system activity.\n\nThe Equivital LifeMonitor is valued for its ability to provide continuous, real-time physiological data, making it a versatile tool for applications such as stress analysis, physical performance assessment, and medical research.\n\nNote\n\nEquivital LifeMonitor","type":"content","url":"/week10#equivital-lifemonitor","position":33},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Citations"},"type":"lvl2","url":"/week10#citations","position":34},{"hierarchy":{"lvl1":"Week 10: Physiology","lvl2":"Citations"},"content":"the following syntax: {cite}`holdgraf_evidence_2014`\n\nHere is the bibliography","type":"content","url":"/week10#citations","position":35},{"hierarchy":{"lvl1":"Week 11: Machine Listening"},"type":"lvl1","url":"/week11","position":0},{"hierarchy":{"lvl1":"Week 11: Machine Listening"},"content":"","type":"content","url":"/week11","position":1},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl2":"Machine Listening"},"type":"lvl2","url":"/week11#machine-listening","position":2},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl2":"Machine Listening"},"content":"Machine listening is a field of study that focuses on enabling machines to interpret and analyze audio signals. It has applications in various domains, including music, speech, and environmental sound analysis.","type":"content","url":"/week11#machine-listening","position":3},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Information Retrieval","lvl2":"Machine Listening"},"type":"lvl3","url":"/week11#information-retrieval","position":4},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Information Retrieval","lvl2":"Machine Listening"},"content":"Information retrieval in machine listening involves extracting meaningful information from audio data. Key areas include:\n\nClassification: This involves categorizing audio into predefined classes, such as:\n\nGenre\n\nInstruments\n\nMood\n\nRecommendation: Systems that suggest music or audio content based on user preferences. Learn more about \n\nrecommender systems.\n\nSource Separation: The process of isolating individual sound sources from a mixture. See \n\nsource separation.\n\nTranscription: Converting audio into symbolic representations like sheet music. Learn about \n\nmusic transcription.\n\nQuestion-Answering: Systems that answer questions based on audio content.\n\nSegmentation: Dividing audio into meaningful segments, such as verses or choruses in music.\n\nFeature Extraction: Extracting characteristics like pitch, tempo, or timbre from audio. See \n\naudio feature extraction.","type":"content","url":"/week11#information-retrieval","position":5},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Data","lvl2":"Machine Listening"},"type":"lvl3","url":"/week11#data","position":6},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Data","lvl2":"Machine Listening"},"content":"Machine listening relies on various types of data:\n\nSymbolic Data: Representations like \n\nMIDI and \n\nMusicXML.\n\nSubsymbolic Data: Includes raw audio, video, and sensor data.\n\nMetadata: Information about the audio, such as artist or album details. Learn about \n\nmetadata.\n\nParadata: Data about the process of data collection or analysis.\n\nUser Data: Information about user interactions and preferences.","type":"content","url":"/week11#data","position":7},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Artificial Intelligence (AI)","lvl2":"Machine Listening"},"type":"lvl3","url":"/week11#artificial-intelligence-ai","position":8},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Artificial Intelligence (AI)","lvl2":"Machine Listening"},"content":"AI techniques in machine listening can be categorized as:\n\nRule-Based Systems: Systems that rely on predefined rules and \n\nstatistics.\n\nLearning-Based Systems: Systems that use machine learning to adapt and improve. Learn about \n\nmachine learning.","type":"content","url":"/week11#artificial-intelligence-ai","position":9},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Time","lvl2":"Machine Listening"},"type":"lvl3","url":"/week11#time","position":10},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl3":"Time","lvl2":"Machine Listening"},"content":"Machine listening systems can operate in different time modes:\n\nRealtime: Systems that process audio as it is received, such as \n\nonline systems.\n\nNon-Realtime: Systems that process audio after it has been recorded, such as \n\noffline systems.\n\nNote\n\nSonicVisualiser","type":"content","url":"/week11#time","position":11},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl2":"Citations"},"type":"lvl2","url":"/week11#citations","position":12},{"hierarchy":{"lvl1":"Week 11: Machine Listening","lvl2":"Citations"},"content":"the following syntax: {cite}`holdgraf_evidence_2014`\n\nHere is the bibliography","type":"content","url":"/week11#citations","position":13},{"hierarchy":{"lvl1":"Week 2: Listening"},"type":"lvl1","url":"/week2","position":0},{"hierarchy":{"lvl1":"Week 2: Listening"},"content":"","type":"content","url":"/week2","position":1},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Describing sounds"},"type":"lvl2","url":"/week2#describing-sounds","position":2},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Describing sounds"},"content":"Recall last week’s discussion about the differences between hearing and listening. We will get back to the mechanisms for both hearing in the coming weeks—focusing on both the acoustic and psychoacoustic parts—but this week will focus on how we listen, what we listen to, and how we can describe the sound and reflect on its meaning.\n\nListen with closed and open eyes\n\nFind a suitable location.\n\nStand still for two minutes with your eyes closed.\n\nListen attentively to the environment.\n\nOpen your eyes and stand still for another two minutes.\n\nListen attentively to the environment.\n\nReflect on what you heard. Did it change after opening the eyes?","type":"content","url":"/week2#describing-sounds","position":3},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Many approaches to describing sounds","lvl2":"Describing sounds"},"type":"lvl3","url":"/week2#many-approaches-to-describing-sounds","position":4},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Many approaches to describing sounds","lvl2":"Describing sounds"},"content":"Academic approaches to describing sound vary across the three main directions we are considering in this course (musicology, psychology, and technology). They each tap into various subfields (or paradigms), each offering distinct frameworks and terminologies:\n\nAcoustics: Focuses on the physical properties of sound waves, such as frequency, amplitude, duration, and propagation in different media.Example: An acoustician might describe a clap in a concert hall as “a broadband impulse with a peak amplitude of 85 dB SPL, followed by a reverberation decay time of 1.8 seconds,” using measurements and graphs to illustrate how sound behaves in the space.\n\nPsychoacoustics: Investigates how humans perceive sound, using quantitative measures (frequency, loudness, spatial location) and perceptual attributes (brightness, roughness).Example: A psychoacoustic study might report that “a 1000 Hz tone at 60 dB SPL is perceived as moderately loud and bright,” and compare listener responses to tones with varying roughness or spatial placement.\n\nMusic theory: Focuses on musical parameters (pitch, rhythm, timbre, dynamics) and cultural context.Example: A musicologist might describe a violin tone as “a slightly sharp C, with a bright timbre, starting from pianissimo and with a gradual crescendo,” using musical notation to visualize it on paper.\n\nSpectromorphology: This specialized form of music theory analyzes the spectral (frequency) and morphological (shape and evolution) characteristics of sounds.Example: A spectromorphological analysis could describe a cymbal crash as “an impulsive onset followed by a complex, evolving spectrum that decays over several seconds,” visualized with a spectrogram showing frequency content over time.\n\nAll of these descriptions focus specifically on describing the sound “itself” or how we hear it. There are also several (slightly overlapping) approaches to studying the effects of sound on people or environments:\n\nSound Studies: An interdisciplinary field that examines sound as a cultural, social, and material phenomenon. Sound studies draw from media studies, anthropology, history, and philosophy to explore how sound shapes and is shaped by society, technology, and everyday life.Example: A sound studies scholar might analyze how urban noise regulations reflect social attitudes toward public space, or investigate the role of sound in shaping collective memory and identity.\n\nAcoustic Ecology: Emphasizes environmental context, categorizing sounds as keynote, signal, or soundmark. Soundscapes are described in terms of their ecological function and impact.Example: An acoustic ecologist could document a city park by noting “birdsong as a keynote, a distant siren as a signal, and the hourly chime of a church bell as a soundmark,” analyzing how each sound shapes the experience of the space.\n\nEthnography: This subbranch of \n\nanthropology uses qualitative methods such as interviews, field notes, and participatory observation to describe how communities interact with and interpret their sonic environments.Example: An ethnographer might record that “local residents describe the evening call to prayer as calming and unifying,” supplementing this with field notes and interviews about its social significance.\n\nLinguistics and \n\nSemiotics: Examines the meaning and communicative function of sounds, including onomatopoeia, prosody, and sound symbolism.Example: A linguist could analyze the word “buzz” as onomatopoeic, noting how its sound mimics the noise of a bee, or study how rising intonation in speech signals a question.\n\nThere is no right or wrong when it comes to studying sound. All of these approaches (and more) aim to uncover various aspects of both the physical properties of sounds but also their meaning, context, and impact on listeners. Several of them are also used in various combinations. We won’t have time to cover all of these in detail in this course, but we will look more closely at some of the closest ones to the fields of music psychology and technology. This week, we will focus on two main concepts that have been influential in the development of our understanding of listening: soundscapes and sound objects.","type":"content","url":"/week2#many-approaches-to-describing-sounds","position":5},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Soundscapes"},"type":"lvl2","url":"/week2#soundscapes","position":6},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Soundscapes"},"content":"A soundscape refers to the acoustic environment as perceived or experienced by people, encompassing all the sounds that arise from both natural and human-made sources. Describing soundscapes involves several dimensions:\n\nPhysical Properties: Documenting the types of sounds present (e.g., birdsong, traffic, water), their frequency ranges, loudness, and temporal patterns.\n\nEcological Function: Identifying the roles sounds play in the environment, such as signaling, masking, or providing information about ecological health.\n\nSpatial Characteristics: Noting how sounds are distributed in space: directionality, distance, and reverberation within the environment.\n\nPerceptual Attributes: Describing how listeners experience the soundscape: pleasantness, annoyance, tranquility, or stimulation.\n\nA comprehensive description of a soundscape often combines “objective” measurements, such as sound level measurements and field recordings, that can be used to make spectrograms with annotated sound maps and written descriptions to capture and analyze soundscapes. Together, these objective measurements, subjective impressions, and contextual information provide a holistic understanding of the sonic environment.\n\nNote\n\nThe distinction between “objectivity” and “subjectivity” in research is complex and debated. It is important to note that the so-called objective approaches (relying on recording and measurements) are always based on context and subjective choices. Similarly, subjective note-taking and interpretation can be systematized to reduce bias. In contemporary research, we try to use multi-method approaches that combine perspectives from the “qualitative” and “quantitative” directions. Integrating both perspectives provides a richer, more nuanced understanding of soundscapes, acknowledging that scientific inquiry into sound must balance measurement with meaning.","type":"content","url":"/week2#soundscapes","position":7},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"R. Murray Schafer and Acoustic Ecology","lvl2":"Soundscapes"},"type":"lvl3","url":"/week2#r-murray-schafer-and-acoustic-ecology","position":8},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"R. Murray Schafer and Acoustic Ecology","lvl2":"Soundscapes"},"content":"R. Murray Schafer (1933–2021) was one of the pioneers of soundscape studies. He was to propose and define soundscape as the acoustic environment as perceived by humans. Starting from the 1960s, he led the \n\nWorld Soundscape Project at Simon Fraser University in Canada, a groundbreaking research initiative focused on studying, documenting, and analyzing the sonic environments of various locations. Through this work, Schafer and his team developed new methods for field recording, sound mapping, and acoustic analysis, aiming to understand how sounds shape our experience of place and community.\n\nSchafer’s most famous book is The Tuning of the World , which introduced key concepts for analyzing soundscapes:\n\nKeynote sounds: These are background sounds that are fundamental to a particular environment, often heard unconsciously. Examples include the hum of traffic in a city or the rustling of leaves in a forest. Keynotes set the acoustic context but are not usually the focus of attention.\n\nSignals: Signals are foreground sounds that are listened to consciously because they carry specific information or meaning. Examples include a ringing phone, a siren, or a bird call. Signals stand out from the background and often prompt a response or action.\n\nSoundmarks: Soundmarks are unique or characteristic sounds that are especially valued by a community or location, similar to landmarks in the visual environment. Examples might be the chimes of a local church bell, a distinctive factory whistle, or a waterfall. Soundmarks help define the identity of a place and are often preserved or celebrated.\n\nSchafer’s work laid the foundation for the field of \n\nacoustic ecology and inspired similar projects worldwide, encouraging interdisciplinary collaboration between musicians, scientists, urban planners, and environmentalists. The project advocated for the preservation of valuable soundscapes and raised awareness about the impact of noise pollution and urbanization on our acoustic environment. It also inspired the \n\nWorld Forum for Acoustic Ecology (WFAE) Conference, an international gathering bringing together researchers, artists, educators, and practitioners to explore the relationship between humans and their sonic environments.\n\nSchafer also introduced the term \n\nschizophonia to describe the separation of a sound from its source, often through recording technology. For example, when you hear a bird song played from a speaker, the sound is no longer coming from the bird itself, but from a device. This separation can change how we experience and relate to sounds, sometimes making them feel less “authentic” or connected to their natural context.","type":"content","url":"/week2#r-murray-schafer-and-acoustic-ecology","position":9},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Hildegard Westerkamp and Soundwalking","lvl2":"Soundscapes"},"type":"lvl3","url":"/week2#hildegard-westerkamp-and-soundwalking","position":10},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Hildegard Westerkamp and Soundwalking","lvl2":"Soundscapes"},"content":"Hildegard Westerkamp (1946–) worked with Schafer in the World Soundscape Project and is famous for developing the concept of \n\nsoundwalking as a reflective practice of walking and listening to the environment. This is not only a practice of listening but also a method of engaging with the environment in a mindful and reflective way. It encourages participants to become aware of the acoustic ecology of their surroundings, fostering a deeper connection to place and community. Soundwalking can be used as a tool for artistic inspiration, environmental awareness, and even therapeutic purposes.\n\nKey aspects of soundwalking include:\n\nActive Listening: Paying close attention to the layers of sound in the environment, from the most prominent to the subtle.\n\nContextual Awareness: Understanding how sounds interact with the physical and social context of a space.\n\nDocumentation: Participants may choose to record sounds, take notes, or create maps to capture their auditory experience.\n\nHildegard Westerkamp’s soundwalking practice bridges both scientific research and artistic exploration. In her scientific work, Westerkamp used soundwalks as a method for gathering data about urban and natural soundscapes. Participants documented their listening experiences, helping researchers analyze acoustic environments, identify sources of noise pollution, and understand how people perceive and interact with their sonic surroundings. These soundwalks contributed to studies in acoustic ecology, urban planning, and environmental psychology.\n\nArtistically, Westerkamp transformed soundwalking into a creative process. She composed works based on field recordings and reflections gathered during soundwalks, such as her acclaimed piece Kits Beach Soundwalk. Her compositions often blend environmental sounds with narration, inviting listeners to experience places through attentive listening. Westerkamp’s approach encourages audiences to engage with everyday sounds as musical material, blurring the boundaries between scientific observation and artistic expression.\n\nThrough soundwalking, Westerkamp demonstrated how attentive listening can deepen our understanding of environments, foster community awareness, and inspire new forms of sonic art.\n\nSound Walking\n\nWalk slowly in a group, without talking\n\nStop whenever you hear something interesting\n\nWrite a note on why you stopped and what was interesting","type":"content","url":"/week2#hildegard-westerkamp-and-soundwalking","position":11},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Steven Feld and Acoustemology","lvl2":"Soundscapes"},"type":"lvl3","url":"/week2#steven-feld-and-acoustemology","position":12},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Steven Feld and Acoustemology","lvl2":"Soundscapes"},"content":"Steven Feld (1949–) is an American anthropologist and ethnomusicologist known for his pioneering work on the relationship between sound, culture, and perception. Feld introduced the concept of acoustemology as a blend of “acoustics” and “epistemology” to describe how knowledge and experience are shaped through sound and listening.\n\nAcoustemology emphasizes that listening is not just a sensory act but a way of knowing and engaging with the world. Feld’s research, particularly with the Kaluli people of Papua New Guinea, demonstrates how sound is deeply embedded in social life, memory, and identity. He explored how environmental sounds, music, and language are interconnected, and how communities use sound to make sense of their surroundings.\n\nKey aspects of acoustemology include:\n\nSound as Knowledge: Understanding that sound is a primary medium for learning, communicating, and relating to place.\n\nCultural Listening: Recognizing that listening practices are shaped by cultural context, history, and environment.\n\nSonic Identity: Investigating how communities define themselves and their spaces through distinctive soundscapes and musical traditions.\n\nFeld’s work encourages researchers and listeners to consider how sound shapes experience and meaning, and how listening can be a method for understanding both local and global cultures.","type":"content","url":"/week2#steven-feld-and-acoustemology","position":13},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Sound objects"},"type":"lvl2","url":"/week2#sound-objects","position":14},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Sound objects"},"content":"After having considered soundscapes more broadly, let us “zoom in” and investigate specific \n\nsound objects in more detail. The development of this concept came before that of soundscapes, emerging from a French “school” of composers and theorists.","type":"content","url":"/week2#sound-objects","position":15},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Pierre Schaeffer and the Sound Object","lvl2":"Sound objects"},"type":"lvl3","url":"/week2#pierre-schaeffer-and-the-sound-object","position":16},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Pierre Schaeffer and the Sound Object","lvl2":"Sound objects"},"content":"The concept of sound object was proposed by the French composer and musicologist \n\nPierre Schaeffer (1910–1995). He is renowned for pioneering \n\nmusique concrète, a form of electroacoustic music that uses recorded sounds as raw material. Throughout his work, Schaeffer laid the foundation for modern sound design, electronic music, and auditory research, influencing generations of composers and sound theorists.\n\nThe sound object—l’objet sonore in French—is at the core of Schaeffer’s theoretical work. His argument was that when listening to sound, we do not hear the continuous sound, but that our perception is grouped into a series of separate sound objects with specific properties. When we perceive speech, we hear words, not single phonemes. When we hear music, we hear tones and short phrases that fuse into “chunks” of sound, typically in the range of 0.5 to 5 seconds.\n\nSchaeffer also introduced the idea of reduced listening as a way to focus on the intrinsic qualities of sound itself rather than its source (sound-producing objects and actions) or semantic meaning. This includes a sound’s texture, tone, and dynamics.\n\nThrough reduced listening, Schaeffer developed a large spectromorphology that can be used to describe any type of sound. We will not go into the whole system, but instead look at the general characterization of sound objects. He suggested three core types:\n\nImpulsive: Short, percussive sounds (e.g., a click or a drum hit).\n\nSustained: Continuous sounds with steady qualities (e.g., a drone or a held note).\n\nIterative: Rapidly repeating sounds (e.g., a tremolo or a rattling noise).\n\nThe categories are coarse, but this model can still be helpful for describing the general shape of sound objects.\n\nSchaeffer’s thoughts have inspired numerous theorists and composers to date. He is often considered the “father” of \n\nelectroacoustic music, music that incorporates electronic technology for the production, manipulation, and reproduction of sound. It often involves recorded sounds, synthesis, and digital processing, allowing composers to explore new sonic possibilities beyond traditional instruments.\n\nSchaeffer’s focus on reduced listening has also inspired a specific type of electroacoustic music called \n\nacousmatic music, where sound is heard without seeing its originating cause. Acousmatic music, often presented through “loudspeaker orchestras” in dark concert settings, emphasizes the experience of sound itself rather than its source.","type":"content","url":"/week2#pierre-schaeffer-and-the-sound-object","position":17},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Expanding Schaeffer’s thinking","lvl2":"Sound objects"},"type":"lvl3","url":"/week2#expanding-schaeffers-thinking","position":18},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Expanding Schaeffer’s thinking","lvl2":"Sound objects"},"content":"Several theorists have built upon Pierre Schaeffer’s foundational ideas about sound objects and listening:\n\nDennis Smalley (1946–, University of London) formalized the concept of spectromorphology, providing a comprehensive vocabulary for describing the spectral and morphological evolution of sounds. Smalley’s approach is widely used in electroacoustic music analysis and has helped clarify how listeners perceive the shape and transformation of sound objects over time.\n\nLasse Thoresen (1949–, Norwegian Academy of Music) has expanded spectromorphological analysis, creating practical frameworks for describing and notating sound objects in both electroacoustic and acoustic music. Thoresen’s work bridges theory and practice, making Schaeffer’s taxonomy accessible for composers and analysts.\n\nRolf Inge Godøy (1955–, University of Oslo) has advanced Schaeffer’s concepts by developing detailed models for how listeners perceive and mentally represent sound objects. Godøy’s work emphasizes gestural-sonorous objects, linking sound perception to physical gestures and movement, and has contributed to the field of embodied music cognition and morphological analysis.\n\nMichel Chion (1947–) has introduced the concept of synchresis to describe the perceptual phenomenon where a sound and a visual event are perceived as occurring simultaneously, even if they are artificially synchronized. This concept is central to audiovisual theory, as it highlights the human tendency to create a cohesive relationship between what is seen and heard. Synchresis plays a crucial role in film sound design, where it is used to enhance the emotional and narrative impact of scenes by aligning specific sounds with visual actions, regardless of their actual source or origin.","type":"content","url":"/week2#expanding-schaeffers-thinking","position":19},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Artistic explorations"},"type":"lvl2","url":"/week2#artistic-explorations","position":20},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Artistic explorations"},"content":"As the overview above has shown, the developments of soundscape and sound object theory have been developed by people that identify both as artistic and scientific researchers, producing both artistic and scientific results This may be uncommon in some fields, but within sound and music, theoretical development can be seen as coming out of artistic practice, and the artistic practice has been inspired by the theoretical development. Here, we will look at some influential works that have been part of the same development.","type":"content","url":"/week2#artistic-explorations","position":21},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"John Cage and 4’33’’","lvl2":"Artistic explorations"},"type":"lvl3","url":"/week2#john-cage-and-433","position":22},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"John Cage and 4’33’’","lvl2":"Artistic explorations"},"content":"John Cage (1912–1992) was an American composer and music theorist whose work challenged traditional notions of sound and music. One of his most influential and controversial pieces is 4’33’', composed in 1952. The piece consists of three movements, during which performers are instructed not to play their instruments. Instead, the focus shifts to the ambient sounds of the environment, making the audience’s listening experience the central element of the composition. Cage’s work emphasizes the idea that silence is never truly silent. The piece invites listeners to engage deeply with the sounds around them, blurring the line between music and environmental noise. 4’33’’ is a seminal work in experimental music, influencing fields such as sound art, acoustic ecology, and contemporary composition.","type":"content","url":"/week2#john-cage-and-433","position":23},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Pauline Oliveros and Deep Listening","lvl2":"Artistic explorations"},"type":"lvl3","url":"/week2#pauline-oliveros-and-deep-listening","position":24},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Pauline Oliveros and Deep Listening","lvl2":"Artistic explorations"},"content":"Pauline Oliveros (1932–2016) created the \n\nDeep Listening practice, emphasizing sonic awareness as heightened attention to sound and its context. Her Sonic Meditations is a collection of text-based instructions (1971) guiding groups in listening and sound-making exercises, fostering communal awareness and creativity. Bye Bye Butterfly (1965) was an early electronic composition blending live feedback and tape delay, reflecting on the transformation of sound and memory.\n\nThroughout her long career, Oliveros made numerous performances exploring acoustic space and group interaction. Oliveros’s work encourages active, inclusive listening and has influenced contemporary sound art, music therapy, and community music practices.","type":"content","url":"/week2#pauline-oliveros-and-deep-listening","position":25},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Yoko Ono and Experimental Listening","lvl2":"Artistic explorations"},"type":"lvl3","url":"/week2#yoko-ono-and-experimental-listening","position":26},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Yoko Ono and Experimental Listening","lvl2":"Artistic explorations"},"content":"Yoko Ono (1933–) is a pioneering artist and composer whose work often challenges conventional boundaries between sound, performance, and audience participation. Her influential Instruction Pieces, such as those in Grapefruit (1964), invite listeners and performers to engage with sound and silence in imaginative, conceptual ways. Ono’s approach emphasizes the creative act of listening, encouraging audiences to perceive everyday sounds as art and to reflect on the relationship between sound, environment, and intention. Her work has inspired generations of artists to explore listening as an active, transformative practice.","type":"content","url":"/week2#yoko-ono-and-experimental-listening","position":27},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Alvin Lucier and “I am sitting in a room”","lvl2":"Artistic explorations"},"type":"lvl3","url":"/week2#alvin-lucier-and-i-am-sitting-in-a-room","position":28},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Alvin Lucier and “I am sitting in a room”","lvl2":"Artistic explorations"},"content":"Alvin Lucier (1931–2021) was an American composer known for his experimental works exploring acoustic phenomena and the perception of sound. His iconic piece, I am sitting in a room (1969), is a landmark in sound art and acoustic ecology. In this work, Lucier records himself reading a text describing the process: he is sitting in a room, recording his voice, and repeatedly plays back and rerecords the tape. With each iteration, the resonant frequencies of the room reinforce themselves, gradually transforming the speech into pure tones shaped by the space’s acoustics. Eventually, the words become unintelligible, replaced by the unique sonic fingerprint of the room.\n\nI am sitting in a room demonstrates how environments shape sound and listening. It invites reflection on the relationship between technology, space, and perception, and is widely cited in discussions of sound art, acoustic ecology, and experimental music.","type":"content","url":"/week2#alvin-lucier-and-i-am-sitting-in-a-room","position":29},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Norwegian artists and practitioners","lvl2":"Artistic explorations"},"type":"lvl3","url":"/week2#norwegian-artists-and-practitioners","position":30},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Norwegian artists and practitioners","lvl2":"Artistic explorations"},"content":"Norway has a vibrant community of artists and researchers working with soundscapes, listening practices, acoustic ecology, and electroacoustic composition:\n\nBritt Pernille Frøholm (1974–): Composer and performer exploring acoustic ecology and soundscape composition in Norwegian landscapes.\n\nEspen Sommer Eide (1972–): Artist and musician working with field recordings, sound installations, and listening walks.\n\nJana Winderen (1965–): Sound artist whose field recordings and installations focus on underwater and natural sound environments.\n\nMaja S. K. Ratkje (1973–): Composer and performer integrating environmental sounds and experimental listening in her works.\n\nNatasha Barrett (1972–): Composer and sound artist specializing in spatial audio, immersive sound installations, and electroacoustic composition, with a focus on environmental listening.\n\nThese practitioners have contributed to both artistic and academic developments in the field, often collaborating across disciplines to deepen our understanding of sound and listening in Norwegian contexts.","type":"content","url":"/week2#norwegian-artists-and-practitioners","position":31},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Capturing sound"},"type":"lvl2","url":"/week2#capturing-sound","position":32},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Capturing sound"},"content":"Let us conclude this week by summarizing how we can capture and represent sound in different ways. This can include written descriptions, visual representations, and audio recordings. Each approach offers unique insights into the qualities and context of sounds.","type":"content","url":"/week2#capturing-sound","position":33},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Writing about sound","lvl2":"Capturing sound"},"type":"lvl3","url":"/week2#writing-about-sound","position":34},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Writing about sound","lvl2":"Capturing sound"},"content":"Writing about sound is a way to translate auditory experiences into language. This can involve:\n\nDescriptive writing: Use adjectives and metaphors to convey the character of sounds (e.g., “a shimmering, metallic clang” or “a gentle, rhythmic hum”).\n\nContextual notes: Record the time, location, and circumstances in which the sound occurred.\n\nReflective journaling: Capture your emotional or sensory response to the sound, noting how it affected your mood or perception.\n\nAnalytical writing: Break down the sound into its components (e.g., pitch, rhythm, timbre, dynamics) and discuss its structure or function.\n\nWriting helps clarify your listening experience and can be used for research, creative projects, or personal reflection.","type":"content","url":"/week2#writing-about-sound","position":35},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Drawing sounds","lvl2":"Capturing sound"},"type":"lvl3","url":"/week2#drawing-sounds","position":36},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Drawing sounds","lvl2":"Capturing sound"},"content":"Drawing sounds is a creative way to visualize sonic phenomena. Techniques include:\n\nWaveform sketches: Represent the amplitude and shape of a sound over time.\n\nSpectrogram drawings: Illustrate the frequency content and evolution of a sound.\n\nSymbolic notation: Use shapes, lines, and colors to depict qualities like loudness, pitch, or texture.\n\nSound maps: Create diagrams showing the spatial distribution of sounds in an environment.\n\nVisual representations can reveal patterns and relationships that are difficult to express in words and are useful in both artistic and scientific contexts.","type":"content","url":"/week2#drawing-sounds","position":37},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Recording sounds","lvl2":"Capturing sound"},"type":"lvl3","url":"/week2#recording-sounds","position":38},{"hierarchy":{"lvl1":"Week 2: Listening","lvl3":"Recording sounds","lvl2":"Capturing sound"},"content":"Recording sounds is the most direct way to capture and preserve sonic events. Key points include:\n\nEquipment: Use microphones and recording devices suited to your environment and purpose (e.g., handheld recorders for fieldwork, studio microphones for controlled settings).\n\nTechnique: Pay attention to microphone placement, levels, and background noise to ensure clear recordings.\n\nDocumentation: Keep notes about the recording context, including date, time, location, and any relevant observations.\n\nArchiving and sharing: Organize recordings for future use and consider sharing them on platforms like \n\nFreesound.org for collaborative projects.\n\nAudio recordings allow for detailed analysis, creative manipulation, and sharing of soundscapes and sonic objects with others.\n\nRecord soundscapes\n\nTurn on a sound recording device (e.g., mobile phone)\n\nWalk slowly while listening attentively\n\nListen to your recording and compare it to what you heard while walking\n\nShare your sound recording with reflections on \n\nFreesound.org","type":"content","url":"/week2#recording-sounds","position":39},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Questions"},"type":"lvl2","url":"/week2#questions","position":40},{"hierarchy":{"lvl1":"Week 2: Listening","lvl2":"Questions"},"content":"What are the main differences between describing sounds using acoustics, psychoacoustics, and music theory?\n\nExplain the concepts of keynote sounds, signals, and soundmarks as defined by R. Murray Schafer.\n\nWhat is the practice of soundwalking, and how did Hildegard Westerkamp contribute to its development?\n\nDescribe Pierre Schaeffer’s concept of the sound object and the three core types he identified.\n\nHow do artistic practices such as John Cage’s 4’33’’ and Pauline Oliveros’s Deep Listening challenge traditional approaches to listening and sound?","type":"content","url":"/week2#questions","position":41},{"hierarchy":{"lvl1":"Week 3: Acoustics"},"type":"lvl1","url":"/week3","position":0},{"hierarchy":{"lvl1":"Week 3: Acoustics"},"content":"This week will explore the world of acoustics, which is a term that everyone knows, yet few people can define properly. We will go over the physics of sound, and look more closely at instrument acoustics and room acoustics before ending with an introduction to digital sound.","type":"content","url":"/week3","position":1},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Introduction"},"type":"lvl2","url":"/week3#introduction","position":2},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Introduction"},"content":"","type":"content","url":"/week3#introduction","position":3},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Defining acoustics","lvl2":"Introduction"},"type":"lvl3","url":"/week3#defining-acoustics","position":4},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Defining acoustics","lvl2":"Introduction"},"content":"A short definition can be that acoustics is the study of sound and its properties. The term originates from the Greek word ἀκουστικός (akoustikos), meaning “of or for hearing, ready to hear.” According to the \n\nANSI/ASA S1.1-2013 standard, acoustics typically have two meanings:\n\nThe science of sound, encompassing its production, transmission, and effects, both biological and psychological (\n\nacoustics)\n\nThe qualities of a room that determine its auditory characteristics (which is the sub-discipline of \n\nroom acoustics).\n\nWe will begin by considering acoustics broadly and then some of its subdisciplines, room acoustics, instrument acoustics, and electroacoustics. However, the latter three are just some of the many subdisciplines of acoustics, as seen in this overview.\n\nFigure: Lindsay’s Wheel of Acoustics, illustrating the interdisciplinary nature of acoustics (\n\nsource).","type":"content","url":"/week3#defining-acoustics","position":5},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Why is acoustics important for musicology, music psychology, and music technology?","lvl2":"Introduction"},"type":"lvl3","url":"/week3#why-is-acoustics-important-for-musicology-music-psychology-and-music-technology","position":6},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Why is acoustics important for musicology, music psychology, and music technology?","lvl2":"Introduction"},"content":"Understanding acoustics is essential for musicology, music psychology, and music technology because it provides the scientific foundation for how sound is produced, transmitted, and perceived. In musicology, acoustics helps analyze the physical properties of musical instruments and performance spaces, informing historical and cultural studies of music. In music psychology, acoustics underpins research into how humans perceive pitch, timbre, loudness, and spatial attributes of sound, which are crucial for understanding musical cognition and emotion. In music technology, acoustics guides the design of audio equipment, recording techniques, and digital sound processing, enabling innovations in music production, reproduction, and analysis. Thus, acoustics bridges the gap between the physical world of sound and its artistic, perceptual, and technological dimensions.\n\n","type":"content","url":"/week3#why-is-acoustics-important-for-musicology-music-psychology-and-music-technology","position":7},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Cause and effect","lvl2":"Introduction"},"type":"lvl3","url":"/week3#cause-and-effect","position":8},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Cause and effect","lvl2":"Introduction"},"content":"At its core, acoustics is about describing the cause and effect of sound. If we consider this systemically, we can think of the following chain from cause to effect:graph LR\n    A[Cause] --> B[Generating mechanism]\n    B --> C[Propagation]\n    C --> D[Reception]\n    D --> E[Effect]\n\nThe generating and receiving mechanisms in acoustics are typically achieved through transduction, the process of converting energy from one form to another (e.g., mechanical to electrical, or vice versa). Sound first propagates through a medium (such as air, water, or solids), and is then transduced again at the point of reception, enabling further processing or perception. This chain of transduction and propagation is fundamental to how sound is produced, transmitted, and experienced.\n\nFor example, when a guitarist plucks a string (cause), the string vibrates and generates sound waves (generating mechanism). These sound waves travel through the air (propagation) and reach the human ear, where it is transduced and processed further. We will get back to the ear next week.\n\nThe chain can be more complex. For example, the guitar sound can be picked up by a microphone, which converts the sound waves into electrical signals (reception/transduction). The electrical signals are then sent to a speaker, which converts them back into sound waves (effect/transduction), allowing the audience to hear the music.\n\n","type":"content","url":"/week3#cause-and-effect","position":9},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Nature of sound waves"},"type":"lvl2","url":"/week3#nature-of-sound-waves","position":10},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Nature of sound waves"},"content":"\n\n","type":"content","url":"/week3#nature-of-sound-waves","position":11},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Vibrations","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#vibrations","position":12},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Vibrations","lvl2":"Nature of sound waves"},"content":"Vibrations are oscillatory motions of particles within a medium, which generate sound waves. These vibrations can be periodic (regular and repeating, as in musical notes) or aperiodic (irregular, as in noise). The nature of these vibrations determines the characteristics of the resulting sound, such as pitch and timbre. Vibrations are fundamental to the production and transmission of sound in acoustics. Let us begin by investigating some of the properties of sound waves.\n\nNote\n\nIn the following, all the plots will be generated by Python code running inside the Jupyter Notebook that this text is written in. You can at any point in time check the source of the code, and even try yourself locally or in \n\nColab. You don’t need to understand the details of the code, but as you progress in your learning, it may help to be able to create such plots yourself.\n\n# To make the rest of the code work, we need to load some Python libraries: \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq\n\n# Define x\nx = np.linspace(0, 2 * np.pi, 1000)\n\n# Generate a periodic wave (sine wave)\nperiodic_wave = np.sin(2 * np.pi * x)\n\n# Generate an aperiodic wave (random noise)\naperiodic_wave = np.random.normal(0, 0.5, len(x))\n\n# Plot the periodic and aperiodic waves\nplt.figure(figsize=(12, 6))\n\n# Plot periodic wave\nplt.subplot(2, 1, 1)\nplt.plot(x, periodic_wave, label='Periodic Wave (Sine)', color='blue')\nplt.title('Periodic Wave')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\n# Plot aperiodic wave\nplt.subplot(2, 1, 2)\nplt.plot(x, aperiodic_wave, label='Aperiodic Wave (Noise)', color='red')\nplt.title('Aperiodic Wave')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/week3#vibrations","position":13},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Longitudinal and Transverse Waves","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#longitudinal-and-transverse-waves","position":14},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Longitudinal and Transverse Waves","lvl2":"Nature of sound waves"},"content":"Mechanical waves travel through a medium (such as air, water, or solids) by causing particles to vibrate. These waves are classified based on the direction of particle motion relative to the direction of wave propagation:\n\nLongitudinal waves: Particles oscillate parallel to the direction the wave travels. Sound waves in air are a common example.\n\nTransverse waves: Particles oscillate perpendicular to the direction of wave travel. Examples include waves on a string or surface water waves.\n\nUnderstanding these wave types is fundamental to acoustics, as they determine how energy is transmitted through different materials.\n\n# Define parameters for the waves\nx = np.linspace(0, 2 * np.pi, 100)\namplitude = 0.5\nfrequency = 1\n\n# Generate waveforms\nwave = np.sin(2 * np.pi * frequency * x)\n\n# Create particle positions for longitudinal and transverse waves\nlongitudinal_x = x + amplitude * np.sin(2 * np.pi * frequency * x)\nlongitudinal_y = np.zeros_like(x)  # No vertical displacement\n\ntransverse_x = x  # No horizontal displacement\ntransverse_y = amplitude * np.sin(2 * np.pi * frequency * x)\n\n# Create the figure\nplt.figure(figsize=(12, 6))\n\n# Plot longitudinal wave\nplt.subplot(2, 1, 1)\nplt.plot(x, np.zeros_like(x), '--', color='gray', label='Wave Direction')\nplt.scatter(longitudinal_x, longitudinal_y, color='blue', label='Particles')\nplt.title('Longitudinal Wave (Particles Oscillate Parallel to Wave Direction)')\nplt.xlabel('Wave Direction')\nplt.ylabel('Particle Displacement')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\n# Plot transverse wave\nplt.subplot(2, 1, 2)\nplt.plot(x, np.zeros_like(x), '--', color='gray', label='Wave Direction')\nplt.scatter(transverse_x, transverse_y, color='red', label='Particles')\nplt.title('Transverse Wave (Particles Oscillate Perpendicular to Wave Direction)')\nplt.xlabel('Wave Direction')\nplt.ylabel('Particle Displacement')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/week3#longitudinal-and-transverse-waves","position":15},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Frequency","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#frequency","position":16},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Frequency","lvl2":"Nature of sound waves"},"content":"Frequency refers to the number of complete oscillations or cycles a sound wave undergoes per second, measured in Hertz (Hz). It determines the audible pitch of a sound: higher frequencies produce higher-pitched sounds, while lower frequencies result in lower-pitched sounds. Frequency is a fundamental property in acoustics, influencing how we perceive and analyze sound.\n\n# Define parameters for the sine waves\ntime = np.linspace(0, 1, 1000)  # Time in seconds (0 to 1 second)\nfreq1, freq2, freq3 = 1, 2, 3  # Frequencies of the sine waves in Hz\nwave1 = np.sin(2 * np.pi * freq1 * time)  # First sine wave\nwave2 = np.sin(2 * np.pi * freq2 * time)  # Second sine wave\nwave3 = np.sin(2 * np.pi * freq3 * time)  # Third sine wave\n\n# Plot the sine waves\nplt.figure(figsize=(12, 4))\nplt.plot(time, wave1, label=f'A: Frequency={freq1} Hz')\nplt.plot(time, wave2, label=f'B: Frequency={freq2} Hz')\nplt.plot(time, wave3, label=f'C: Frequency={freq3} Hz')\nplt.title('Sine Waves with Different Frequencies')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\nThe frequency of a wave is closely related to its period, the time it takes for one complete cycle of a wave to occur. The period is measured in seconds (s) and frequency is the number of cycles that occur per second, measured in Hertz (Hz).\n\n[\nf = \\frac{1}{T}\n]\n[\nT = \\frac{1}{f}\n]\n\nFor example, if a wave has a period of 0.01 seconds, its frequency is ( f = 1 / 0.01 = 100 ) Hz. If the frequency is 50 Hz, the period is ( T = 1 / 50 = 0.02 ) seconds.\n\n","type":"content","url":"/week3#frequency","position":17},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Amplitude","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#amplitude","position":18},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Amplitude","lvl2":"Nature of sound waves"},"content":"A sound wave’s amplitude defines how “loud” it is. More precisely, amplitude is the maximum displacement of particles in the medium from their rest position as the wave passes through. In a graphical representation, such as a sine wave, amplitude corresponds to the peak value above and below the center line (zero).\n\nHigher amplitude means greater energy in the wave, resulting in a louder sound. Lower amplitude produces a quieter sound. Amplitude is typically measured in units such as meters (for displacement), pascals (for pressure), or volts (for electrical signals).\n\nFor example, in the plot below, each sine wave has a specific amplitude. The peaks and troughs of the wave show the maximum and minimum values, illustrating how amplitude determines the loudness of the sound.\n\n# Define amplitudes for the sine tones\namplitude1 = 0.5\namplitude2 = 1.0\namplitude3 = 1.5\n\n# Generate the sine tones\nsine1 = amplitude1 * np.sin(x)\nsine2 = amplitude2 * np.sin(x)\nsine3 = amplitude3 * np.sin(x)\n\n# Plot the sine tones\nplt.figure(figsize=(12, 4))\nplt.plot(x, sine1, label='Amplitude = 0.5', alpha=0.8)\nplt.plot(x, sine2, label='Amplitude = 1.0', alpha=0.8)\nplt.plot(x, sine3, label='Amplitude = 1.5', alpha=0.8)\nplt.title('Sine Tones with Different Amplitudes')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\n","type":"content","url":"/week3#amplitude","position":19},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sound Pressure Level (SPL)","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#sound-pressure-level-spl","position":20},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sound Pressure Level (SPL)","lvl2":"Nature of sound waves"},"content":"Sound Pressure Level (SPL) is a measure of the pressure variation caused by a sound wave. It is used in various fields, such as acoustics, audio engineering, and environmental noise monitoring, to assess sound levels and ensure compliance with safety standards. Understanding SPL is crucial for analyzing how sound behaves in different environments and its impact on human hearing.\n\nSPL is expressed in \n\ndecibels (dB), which is a logarithmic unit used to express the ratio of two values. In a logarithmic scale, each step on the scale represents a multiplication rather than a simple addition. For example, an increase of 10 dB corresponds to a tenfold increase in sound intensity, while an increase of 20 dB means a hundredfold increase. This allows us to represent very large ranges of sound pressure in a compact and manageable way, since human hearing also perceives loudness logarithmically rather than linearly.\n\nThe formula for SPL in decibels is:\n\nSPL = 20 log10(p / p0)\n\nwhere p is the measured sound pressure and p 0 is the reference sound pressure (typically 20 μPa in air).\n\n# Plot illustrating regular (linear) vs logarithmic scale\n\nx = np.linspace(1, 1000, 1000)\ny = x  # Linear relationship\n\nplt.figure(figsize=(12, 5))\n\n# Linear scale plot\nplt.subplot(1, 2, 1)\nplt.plot(x, y, color='blue')\nplt.title('Linear Scale')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\n\n# Logarithmic scale plot\nplt.subplot(1, 2, 2)\nplt.plot(x, y, color='red')\nplt.yscale('log')\nplt.title('Logarithmic Scale (y-axis)')\nplt.xlabel('x')\nplt.ylabel('y (log scale)')\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\nWhat is important to know is that a 10 dB increase in SPL is generally perceived as about twice as loud by the human ear. This is because human hearing responds logarithmically: a 10 dB rise means the sound pressure increases by a factor of about 3.16, but our perception of loudness roughly doubles.\n\nThe 3 dB rule states that if you add a second identical sound source, the SPL increases by about 3 dB, doubling the energy but only slightly increasing perceived loudness. For example, if a single speaker produces 70 dB SPL at a certain point, adding a second identical speaker playing the same signal at the same location will increase the SPL to approximately 73 dB. This is because the sound pressure doubles, resulting in a 3 dB increase, but the perceived loudness is only slightly greater, not doubled. Similarly, two 60 dB sound sources will have a combined SPL of 63 dB.\n\nIn addition, you need to know about the Inverse-Square Law, which states that when the distance from a sound source doubles, the sound pressure drops to one-fourth, resulting in a 6 dB reduction in SPL. This explains why sounds become much quieter as you move further away from the source.\n\nFigure: Illustration of the Inverse-Square Law, showing how sound pressure decreases with distance (\n\nWikipedia).\n\n","type":"content","url":"/week3#sound-pressure-level-spl","position":21},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Phase","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#phase","position":22},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Phase","lvl2":"Nature of sound waves"},"content":"Describes the position of a point within a wave cycle, measured in degrees or radians. Phase differences between waves can lead to constructive or destructive interference.\n\n# Define the amplitude and frequency for the sine tones\nx = np.linspace(0, 2 * np.pi, 100)\namplitude = 1\nfrequency = 1  # Frequency in Hz\n\n# Define the phases for the three sine tones\nphase1 = 0  # 0 radians\nphase2 = np.pi / 4  # 45 degrees in radians\nphase3 = np.pi / 2  # 90 degrees in radians\n\n# Convert x to time in seconds\ntime_in_seconds = x / (2 * np.pi * frequency)\n\n# Generate the sine tones\nsine1 = amplitude * np.sin(2 * np.pi * frequency * time_in_seconds + phase1)\nsine2 = amplitude * np.sin(2 * np.pi * frequency * time_in_seconds + phase2)\nsine3 = amplitude * np.sin(2 * np.pi * frequency * time_in_seconds + phase3)\n\n# Plot the sine tones\nplt.figure(figsize=(12, 4))\nplt.plot(time_in_seconds, sine1, label='Phase = 0 rad', alpha=0.8)\nplt.plot(time_in_seconds, sine2, label='Phase = π/4 rad', alpha=0.8)\nplt.plot(time_in_seconds, sine3, label='Phase = π/2 rad', alpha=0.8)\nplt.title('Sine Tones with Different Phases')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Define amplitude and frequency for the sine tones\namplitude = 1\nfrequency = 1  # Frequency in Hz\n\n# Use x from previous cells\n# Define two sine tones with opposite phases (0 and pi)\nsine1 = amplitude * np.sin(x)\nsine2 = amplitude * np.sin(x + np.pi)  # 180 degrees out of phase\n\n# Sum of the two sine tones\nsum_wave = sine1 + sine2\n\n# Plot the two sine tones and their sum\nplt.figure(figsize=(12, 4))\nplt.plot(x, sine1, label='Sine Tone 1 (Phase = 0)', alpha=0.8)\nplt.plot(x, sine2, label='Sine Tone 2 (Phase = π)', alpha=0.8)\nplt.plot(x, sum_wave, label='Sum (Cancellation)', color='black', linewidth=2)\nplt.title('Two Sine Tones Out of Phase (Cancellation)')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\nHave you tried swapping the phase in a music track? Try \n\nthis phase cancellation web experiment.\n\n","type":"content","url":"/week3#phase","position":23},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Beating waves","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#beating-waves","position":24},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Beating waves","lvl2":"Nature of sound waves"},"content":"When two sound waves of slightly different frequencies are played together, they interfere with each other. This interference causes the amplitude of the combined wave to fluctuate up and down in a regular pattern, called a beat. The beat frequency is equal to the difference between the two original frequencies. For example, two tones with frequencies 440 Hz and 441 Hz will cause a beat frequency of 1 Hz. You hear this as the sound getting louder and softer at this beat frequency.\n\nMathematically, if you add two sine waves with close frequencies, the result is a wave whose amplitude varies slowly, creating the “beating” effect. This is commonly heard when tuning musical instruments or when two notes are almost, but not quite, in tune.\n\n# Define parameters for the two sine waves\nfrequency1 = 5  # Frequency of the first wave in Hz\nfrequency2 = 5.5  # Frequency of the second wave in Hz\namplitude = 1  # Amplitude of the waves\ntime = np.linspace(0, 5, 1000)  # Time array from 0 to 5 seconds\n\n# Generate the two sine waves\nwave1 = amplitude * np.sin(2 * np.pi * frequency1 * time)\nwave2 = amplitude * np.sin(2 * np.pi * frequency2 * time)\n\n# Generate the resulting wave (superposition)\nbeating_wave = wave1 + wave2\n\n# Plot the individual waves and the resulting wave\nplt.figure(figsize=(12, 4))\nplt.plot(time, wave1, label='Wave 1', alpha=0.7)\nplt.plot(time, wave2, label='Wave 2', alpha=0.7)\nplt.plot(time, beating_wave, label='Beating Wave', color='black', linewidth=2)\nplt.title('Beating Waves: Interference of Two Sine Waves')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\nTry out making sine tones in \n\nGlicol. Change the amplitude and frequency to hear how they are affecting what you hear. Try adding an extra sine tone with a different frequency create a beat frequency.\n\n","type":"content","url":"/week3#beating-waves","position":25},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Complex waves","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#complex-waves","position":26},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Complex waves","lvl2":"Nature of sound waves"},"content":"A complex wave is a sound wave that consists of multiple frequencies combined together, rather than a single pure tone. Most sounds we hear in everyday life, such as musical notes, speech, or environmental noises, are complex waves.\n\n# Define parameters for the sine waves\nx = np.linspace(0, 2 * np.pi, 1000)\nwave1 = np.sin(x)  # First sine wave\nwave2 = 0.5 * np.sin(2 * x)  # Second sine wave with half amplitude and double frequency\nwave3 = 0.25 * np.sin(3 * x)  # Third sine wave with quarter amplitude and triple frequency\n\n# Sum of the sine waves (complex wave)\ncomplex_wave = wave1 + wave2 + wave3\n\n# Plot the individual sine waves and the complex wave\nplt.figure(figsize=(12, 4))\nplt.plot(x, wave1, label='Sine Wave 1: sin(x)', alpha=0.7)\nplt.plot(x, wave2, label='Sine Wave 2: 0.5*sin(2x)', alpha=0.7)\nplt.plot(x, wave3, label='Sine Wave 3: 0.25*sin(3x)', alpha=0.7)\nplt.plot(x, complex_wave, label='Complex Wave: sum of sine waves', color='black', linewidth=2)\nplt.title('Composition of a Complex Wave from Sine Waves')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\n","type":"content","url":"/week3#complex-waves","position":27},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Time vs Frequency Domain","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#time-vs-frequency-domain","position":28},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Time vs Frequency Domain","lvl2":"Nature of sound waves"},"content":"Sound can be represented visually in two fundamentally different ways:\n\nTime domain: A “waveform display” shows how the amplitude of a signal (such as a sound wave) changes over time This view helps us understand the shape, duration, and dynamics of the signal.\n\nFrequency domain: A spectrum plot or spectrogram shows how much of the signal lies within each frequency band. The frequency domain representation is typically obtained using the Fourier transform. This view is useful for analyzing pitch, timbre, and spectral content.","type":"content","url":"/week3#time-vs-frequency-domain","position":29},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"The Fourier Transform","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#the-fourier-transform","position":30},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"The Fourier Transform","lvl2":"Nature of sound waves"},"content":"Any complex wave can be represented as the sum of simpler sinusoidal waves with different frequencies, amplitudes, and phases, a process called Fourier analysis (\n\nWikipedia). This technique was discovered by the French mathematician \n\nJoseph Fourier (1768–1830), laying the foundation for modern signal processing and harmonic analysis.\n\nBelwThis decomposition is fundamental in acoustics, music technology, and audio engineering, as it allows us to analyze, synthesize, and manipulate sounds in both the time and frequency domains.\n\n# Define parameters for the complex wave\nfrequency1 = 5  # Frequency of the first wave in Hz\nfrequency2 = 10  # Frequency of the second wave in Hz\nfrequency3 = 15  # Frequency of the third wave in Hz\namplitude1 = 1\namplitude2 = 0.5\namplitude3 = 0.25\n\n# Generate the complex wave as a sum of three sine waves\nwave1 = amplitude1 * np.sin(2 * np.pi * frequency1 * x)\nwave2 = amplitude2 * np.sin(2 * np.pi * frequency2 * x)\nwave3 = amplitude3 * np.sin(2 * np.pi * frequency3 * x)\ncomplex_wave = wave1 + wave2 + wave3\n\n# Compute the Fourier Transform of the complex wave\nN = len(complex_wave)\nT = (x[1] - x[0]) / (2 * np.pi)\nfrequencies = fftfreq(N, T)[:N // 2]\nfft_values = fft(complex_wave)[:N // 2]\n\n# Plot time domain, frequency domain, and spectrogram\nplt.figure(figsize=(12, 9))\n\n# Time domain plot\nplt.subplot(3, 1, 1)\nplt.plot(x, complex_wave, label='Complex Wave (Time Domain)', color='blue')\nplt.title('Waveform display (Time Domain)')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\n# Frequency domain plot\nplt.subplot(3, 1, 2)\nplt.plot(frequencies, np.abs(fft_values), label='Frequency Domain', color='red')\nplt.title('Spectrum plot (Frequency Domain)')\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Magnitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\n\n# Spectrogram plot\nplt.subplot(3, 1, 3)\nplt.specgram(complex_wave, NFFT=256, Fs=1/(x[1]-x[0]), noverlap=128, cmap='magma')\nplt.title('Spectrogram (Frequency Domain)')\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency (Hz)')\n\nplt.tight_layout()\nplt.show()\n\nNote how the spectrum plot displays frequency on the X axis and magnitude on the Y axis, while the spectrogram shows time on the X axis and frequency on the Y axis. The spectrum provides an overview of the frequency content averaged over the entire signal. The spectrogram reveals how the frequency content changes over time, showing the temporal evolution.\n\nTo better visualize their relationship, you can plot the spectrum rotated 90 degrees, aligning its frequency axis with the spectrogram’s frequency axis. This highlights the difference: the spectrum summarizes the whole signal, while the spectrogram shows its development over time.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Left: Spectrum turned 90 degrees\naxes[0].plot(np.abs(fft_values), frequencies, color='green', linewidth=2)\naxes[0].set_title('Spectrum')\naxes[0].set_ylabel('Frequency (Hz)')\naxes[0].set_xlabel('Magnitude')\naxes[0].grid()\n\n# Right: Spectrogram\naxes[1].specgram(complex_wave, NFFT=256, Fs=1/(x[1]-x[0]), noverlap=128, cmap='magma')\naxes[1].set_title('Spectrogram')\naxes[1].set_xlabel('Time (s)')\naxes[1].set_ylabel('Frequency (Hz)')\n\nplt.tight_layout()\nplt.show()\n\nNote\n\nYou may come across the term sonogram in the literature. It is essentially a spectrogram of sound signals. For sound and music, they are used interchangeably. However, spectrograms are more general, and can be used to decompose also other types of complex time-based signals.\n\n","type":"content","url":"/week3#the-fourier-transform","position":31},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Noise","lvl2":"Nature of sound waves"},"type":"lvl3","url":"/week3#noise","position":32},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Noise","lvl2":"Nature of sound waves"},"content":"Noise refers to random or unpredictable fluctuations in sound, often lacking a clear pitch or musical quality. In acoustics and audio engineering, different types of noise are characterized by their frequency content and how energy is distributed across the spectrum. Common types of noise include:\n\nWhite noise: Contains all frequencies at equal intensity, resulting in a “hissing” sound similar to static from a radio or TV. It is often used for sound masking and audio testing because of its uniform frequency distribution.\n\nPink noise: Has equal energy per octave, meaning its power decreases as frequency increases. This gives it a deeper sound, similar to rainfall or wind, and makes it useful for audio calibration and sleep aids.\n\nBrownian noise (Red noise): Emphasizes even lower frequencies than pink noise, producing a deep rumble or distant thunder-like sound. It is generated by random walk processes and is sometimes called “red noise.”\n\nBlue noise: Contains more energy at higher frequencies, resulting in a brighter, sharper sound. Blue noise is rare in nature but is used in dithering applications in digital audio and image processing.\n\nGrey noise: Adjusted so that all frequencies are perceived as equally loud to the human ear, based on psychoacoustic principles. It is used in research and testing to account for human hearing sensitivity.\n\nThe above noise types are “constant” in the sense that they have the same characteristics. One can also talk about different types of “impulse noise”, based on sudden, short bursts of sound, such as clicks, pops, or bangs. Impulse noise is common in environments with machinery, gunshots, or electrical discharges.\n\nThese noise types are used in audio testing, sound masking, electronic music, and various scientific applications. Understanding the characteristics of each type helps in designing systems for noise reduction, audio analysis, and environmental sound studies.\n\n# Define parameters\nsampling_rate = 1000  # Hz\nduration = 2  # seconds\nN = sampling_rate * duration\nt = np.linspace(0, duration, N, endpoint=False)\n\n# Generate noises\nwhite_noise = np.random.normal(0, 1, N)\n\n# Pink noise (approximate using Voss-McCartney algorithm)\ndef pink_noise(N):\n    n_rows = 16\n    n_cols = N\n    array = np.random.randn(n_rows, n_cols)\n    array = np.cumsum(array, axis=1)\n    array = array / np.arange(1, n_cols + 1)\n    return np.sum(array, axis=0)\n\npink_noise = pink_noise(N)\n\n# Brownian noise (integrated white noise)\nbrownian_noise = np.cumsum(np.random.normal(0, 1, N))\nbrownian_noise /= np.max(np.abs(brownian_noise))\n\n# Blue noise (differentiated white noise)\nblue_noise = np.diff(white_noise, prepend=0)\nblue_noise /= np.max(np.abs(blue_noise))\n\n# Grey noise (white noise shaped by equal loudness curve, here just normalized white noise)\ngrey_noise = white_noise / np.max(np.abs(white_noise))\n\n# Impulse noise (random sparse spikes)\nimpulse_noise = np.zeros(N)\nimpulse_indices = np.random.choice(N, size=int(N * 0.01), replace=False)\nimpulse_noise[impulse_indices] = np.random.choice([-1, 1], size=len(impulse_indices))\n\n# List of noises\nnoises = [\n    (\"White\", white_noise),\n    (\"Pink\", pink_noise),\n    (\"Brownian\", brownian_noise),\n    (\"Blue\", blue_noise),\n    (\"Grey\", grey_noise),\n    (\"Impulse\", impulse_noise)\n]\n\n# Plot spectrograms\nplt.figure(figsize=(12, 12))\nfor i, (name, noise) in enumerate(noises, 1):\n    plt.subplot(6, 1, i)\n    plt.specgram(noise, Fs=sampling_rate, NFFT=1024, noverlap=512, cmap='magma')\n    plt.title(f'{name} Noise Spectrogram')\n    plt.ylabel('Frequency (Hz)')\n    plt.xticks([])\n    plt.yticks([])\nplt.xlabel('Time (s)')\nplt.tight_layout()\nplt.show()\n\nDownload and install \n\nSonic Visualiser, a free tool for visualizing and analyzing audio files. Open a sound file (such as a recording or sample) and experiment with creating spectrograms:\n\nLoad your audio file into Sonic Visualiser.\n\nAdd a new spectrogram layer (choose “Layer” → “Add Spectrogram”).\n\nAdjust the spectrogram settings (window size, color map, etc.) to explore how they affect the visualization.\n\nTry zooming in and out to examine different time and frequency regions.\n\nHow does the spectrogram help you understand the frequency content and changes over time in your audio? Compare the visual patterns for different sounds (speech, music, noise).\n\n","type":"content","url":"/week3#noise","position":33},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Sound propagation"},"type":"lvl2","url":"/week3#sound-propagation","position":34},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Sound propagation"},"content":"Sound propagation refers to how sound waves travel through different environments and interact with materials.","type":"content","url":"/week3#sound-propagation","position":35},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Medium of Propagation","lvl2":"Sound propagation"},"type":"lvl3","url":"/week3#medium-of-propagation","position":36},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Medium of Propagation","lvl2":"Sound propagation"},"content":"Sound needs a medium, such as air, water, or solids, to travel. The particles in these media vibrate and transmit energy from one place to another. The speed and efficiency of sound propagation depend on the medium’s properties, especially density and elasticity. Generally, sound travels faster in materials that are more elastic and have closely packed particles.\n\nHere are some examples of the \n\nspeed of sound in different media:\n\nMedium\n\nSpeed (m/s)\n\nAir\n\n343\n\nHelium\n\n965\n\nWater\n\n1481\n\nGlass\n\n4540\n\nIron\n\n5120\n\nDiamond\n\n12000\n\nSound moves fastest in solids (like iron or diamond) because their particles are tightly packed and transmit vibrations efficiently. For example, you can hear a distant train by placing your ear on the rail, as sound travels much faster through metal than air.\n\nTemperature, humidity, and altitude also affect the speed of sound. Warmer air or higher humidity generally increases the speed, while higher altitude (lower air density) decreases it.\n\n","type":"content","url":"/week3#medium-of-propagation","position":37},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Reflection, Refraction, Diffraction, and Absorption","lvl2":"Sound propagation"},"type":"lvl3","url":"/week3#reflection-refraction-diffraction-and-absorption","position":38},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Reflection, Refraction, Diffraction, and Absorption","lvl2":"Sound propagation"},"content":"There are four physics concepts that are important for understanding the behavior of sound in general, but also in both rooms and instruments:\n\nReflection: Sound waves bounce off surfaces, creating echoes and affecting acoustics. In a concert hall, hard walls and ceilings reflect sound, producing reverberation and echoes. Inside a guitar, sound waves reflect off the wooden body, reinforcing certain frequencies and contributing to the instrument’s tone.\n\nRefraction: When sound moves between media (e.g., air to water), its speed changes, causing the wave to bend Temperature gradients in a room can cause sound waves to bend, affecting how sound travels from the stage to the audience. In wind instruments, sound waves refract as they move through air of varying temperature or humidity inside the instrument, subtly changing pitch and timbre.\n\nDiffraction: Sound waves bend around obstacles and spread out after passing through openings. Sound diffracts around furniture or pillars, allowing you to hear someone speaking even if they are not in direct line of sight. The sound from a violin’s f-holes diffracts, helping project the instrument’s sound in all directions.\n\nAbsorption: Materials absorb sound energy, reducing its intensity. Carpets, curtains, and acoustic panels absorb sound, reducing echoes and making rooms quieter. The type of wood or material used in a drum absorbs some sound energy, affecting the instrument’s resonance and sustain.\n\nThese principles shape how we experience sound in different environments, from open fields to concert halls, and influence the design and performance of musical instruments.\n\n","type":"content","url":"/week3#reflection-refraction-diffraction-and-absorption","position":39},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Room Acoustics"},"type":"lvl2","url":"/week3#room-acoustics","position":40},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Room Acoustics"},"content":"Room acoustics explores how the physical characteristics of a space influence sound quality. The shape, size, and materials of a room affect how sound waves behave, impacting clarity, warmth, and reverberation.","type":"content","url":"/week3#room-acoustics","position":41},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Room Size and Shape","lvl2":"Room Acoustics"},"type":"lvl3","url":"/week3#room-size-and-shape","position":42},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Room Size and Shape","lvl2":"Room Acoustics"},"content":"The dimensions and geometry of a room determine how sound waves reflect, interact, and form standing waves. Irregular shapes and non-parallel surfaces help minimize unwanted echoes and resonances, while rectangular rooms with parallel walls are prone to standing waves, where certain frequencies are reinforced due to repeated reflections.\n\nStanding waves form when sound waves reflect between parallel surfaces and interfere with themselves, creating regions of constructive and destructive interference. This results in certain frequencies being amplified (peaks) and others being diminished (nulls) at specific locations in the room. Standing waves are most pronounced at low frequencies and can cause uneven bass response, making some notes sound much louder or softer depending on where you stand. Treating room modes with bass traps and careful placement of speakers/listeners helps minimize these effects.\n\nRoom modes are the specific frequencies at which standing waves occur, determined by the room’s dimensions (length, width, height). Each mode corresponds to a resonance frequency where sound energy is reinforced. Modes are categorized as axial (between two parallel surfaces), tangential (between four surfaces), and oblique (between six surfaces). Calculating room modes helps identify problematic frequencies and guides acoustic treatment to achieve a balanced sound environment.\n\nTry a \n\nRoom Mode Calculator to estimate the acoustic properties of a room.\n\nSome you may have experienced a particular acoustic phenomenon called flutter echo. This is the rapid, repetitive echoes that occur between hard, parallel surfaces (such as bare walls or ceilings). When a sound wave bounces back and forth between these surfaces, it creates a series of closely spaced echoes that can sound like a “ping-pong” effect or a metallic ringing. Flutter echo is especially noticeable in empty rooms or corridors and can degrade speech intelligibility and musical clarity. Acoustic panels or diffusers are often used to break up parallel surfaces and eliminate flutter echo.\n\nIn Oslo, we have an acoustic installation at one of the entrances of the National Theatre train station based on a spectacular flutter echo.\n\n","type":"content","url":"/week3#room-size-and-shape","position":43},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Materials","lvl2":"Room Acoustics"},"type":"lvl3","url":"/week3#materials","position":44},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Materials","lvl2":"Room Acoustics"},"content":"Different construction materials play a crucial role in shaping the acoustic properties of a room:\n\nHard surfaces (glass, concrete, tile): Reflect sound waves efficiently, leading to increased reverberation and potential echoes. These surfaces can make a space sound “live” or “bright,” but may also cause unwanted reflections and clarity issues.\n\nSoft materials (carpet, curtains, upholstered furniture, acoustic panels): Absorb sound energy, especially at mid and high frequencies, reducing reverberation and minimizing echoes. These materials help create a “dry” or “warm” acoustic environment, improving speech intelligibility and musical detail.\n\nPorous materials (foam, mineral wool, fiberglass): Highly effective at absorbing sound, particularly at higher frequencies. Used in acoustic panels and bass traps to control reflections and room modes.\n\nDense materials (brick, stone, thick wood): Reflect low-frequency sound waves and can help contain sound within a space, but may also contribute to standing waves and bass buildup.\n\nDiffusive surfaces (bookshelves, irregular walls, specialized diffusers): Scatter sound waves in multiple directions, breaking up strong reflections and preventing flutter echoes. Diffusion improves clarity and creates a more balanced listening environment.\n\nWindows and doors: Can transmit sound between rooms, affecting isolation and privacy. Double glazing and solid-core doors help reduce sound transmission.\n\nThe combination of these materials determine the overall acoustic character of a room.","type":"content","url":"/week3#materials","position":45},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Acoustic Treatment","lvl2":"Room Acoustics"},"type":"lvl3","url":"/week3#acoustic-treatment","position":46},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Acoustic Treatment","lvl2":"Room Acoustics"},"content":"The shape, size, and construction of a room are difficult to change after it has\n\nThe above Effective acoustic treatment involves a combination of absorption, diffusion, and strategic placement of materials to optimize sound quality in a room:\n\nAbsorptive Panels: Typically made from foam, fiberglass, or mineral wool, these panels are mounted on walls or ceilings to absorb mid and high frequencies. They help reduce reflections, reverberation, and flutter echoes, making speech and music clearer.\n\nBass Traps: Specialized absorbers placed in corners or along walls to target low-frequency energy. Bass traps help control room modes and prevent bass buildup, which can cause uneven sound and “boomy” bass.\n\nDiffusers: Unlike absorbers, diffusers scatter sound waves in multiple directions. They are often made from wood or plastic and have irregular surfaces or patterns. Diffusers break up strong reflections and standing waves, preserving a sense of spaciousness and natural ambience.\n\nCeiling Clouds: Suspended panels above listening or performance areas absorb sound from above, reducing ceiling reflections and improving overall clarity.\n\nFurniture Arrangement: Placing bookshelves, couches, and other furnishings strategically can help break up sound reflections and add both absorption and diffusion. Soft furniture absorbs sound, while irregular surfaces diffuse it.\n\nDoor and Window Seals: Adding seals or heavy curtains to doors and windows improves sound isolation, preventing unwanted noise from entering or leaving the room.\n\nAcoustic Curtains and Rugs: Thick curtains and rugs add absorption, especially in spaces with many hard surfaces, helping to tame excessive reverberation.\n\nA balanced approach using both absorption and diffusion creates a room that is neither too “dead” nor too “live,” supporting accurate sound reproduction and comfortable listening.\n\nThink about the acoustics of the room you are in. What are the key defining properties of the room? How can you improve it?\n\n","type":"content","url":"/week3#acoustic-treatment","position":47},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Reverberation","lvl2":"Room Acoustics"},"type":"lvl3","url":"/week3#reverberation","position":48},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Reverberation","lvl2":"Room Acoustics"},"content":"Reverberation is one of the most defining acoustic properties of a room. It can be defined as the persistence of sound in a space after the original sound source has stopped, caused by reflections from surfaces such as walls, ceilings, and floors.\n\nAcousticians often calculate reverberation time based on the concept T60, which is defined as the time it takes for sound to decay by 60 dB after the source stops. The optimal reverberation time depends on the room’s purpose and the type of music or activity. Spaces designed for speech, such as lecture halls, benefit from shorter reverberation times, while concert halls for symphonic music often require longer reverberation for a fuller sound.\n\nShort T60 (0.5–1 s): Ideal for speech and clarity, minimizing echoes and enhancing intelligibility.\n\nModerate T60 (1.5–2 s): Suitable for chamber music, balancing clarity and warmth.\n\nLong T60 (2–3 s): Preferred for orchestral and choral music, creating a rich and immersive sound.\n\nUnderstanding and controlling reverberation is essential for creating spaces with desirable acoustic properties, whether for recording studios, concert halls, or home listening rooms. Proper acoustic design ensures clarity, warmth, and an enjoyable listening experience.\n\n# Simulate a simple exponential decay to illustrate T60 reverberation time\ninitial_amplitude = 1.0\nt = np.linspace(0, 3, 1000)  # 3 seconds duration\n\n# T60 is the time for amplitude to decay by 60 dB (factor of 1/1000)\nT60 = 2.0  # seconds (example value)\ndecay_curve = initial_amplitude * np.exp(-t * np.log(1000) / T60)\n\nplt.figure(figsize=(12, 4))\nplt.plot(t, decay_curve, label='Reverberation Decay')\nplt.axhline(initial_amplitude / 1000, color='red', linestyle='--', label='-60 dB Level')\nplt.title('Simulated Reverberation Decay (T60)')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.legend()\nplt.grid()\nplt.show()\n\n","type":"content","url":"/week3#reverberation","position":49},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Instrument acoustics"},"type":"lvl2","url":"/week3#instrument-acoustics","position":50},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Instrument acoustics"},"content":"Room acoustics is a big field of study and there are numerous jobs for people working with both constructing and modifying rooms in buildings. There are much fewer people focusing on instrument acoustics. While the scale is smaller, most of the same principles apply to instruments, investigating how musical instruments generate, shape, and radiate sound.","type":"content","url":"/week3#instrument-acoustics","position":51},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Principles of instrument acoustics","lvl2":"Instrument acoustics"},"type":"lvl3","url":"/week3#principles-of-instrument-acoustics","position":52},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Principles of instrument acoustics","lvl2":"Instrument acoustics"},"content":"The main physical principles of instrument acoustics include the vibration source and the instrument’s resonance The initial sound is produced by vibrating elements such as strings (guitar, violin), air columns (flute, trumpet), membranes (drum), or solid bodies (xylophone). Most instruments have resonating bodies (soundboards, tubes, shells) that amplify and color the sound. The shape, size, and material of these bodies determine the instrument’s timbre and loudness.\n\nThe vibration source heavily influences the frequency range of the instrument. Each instrument has a characteristic range of frequencies it can produce, determined by its physical dimensions and construction.\n\nFigure: Fundamentals of Instrument Acoustics (\n\nCredit: Sebastian Merchel).\n\nThe resonance in the instrument is important for its timbre based on its partials and overtones. Instruments rarely produce pure tones; instead, they generate complex waves with multiple harmonics. The relative strength of these harmonics defines the instrument’s unique sound.\n\nAn instrument’s radiation pattern is based on the way sound is projected into the surrounding space depends on the instrument’s geometry and playing technique.\n\nString instruments\n\nWhen a string is plucked or bowed, it vibrates at its fundamental frequency and produces harmonics. The body of the instrument (such as a guitar or violin) amplifies these vibrations and shapes the sound. The material and construction of the body affect the instrument’s tone and projection.\n\nWind instruments\n\nWind instruments produce sound by vibrating air columns. The length, shape, and material of the tube determine the pitch and timbre. Opening and closing holes changes the effective length of the air column, allowing different notes to be played.\n\nPercussion instruments\n\nPercussion instruments generate sound through striking, shaking, or scraping. The vibration of membranes (drums) or solid bodies (bells, xylophones) creates complex waveforms. The size, tension, and material of the vibrating surface influence the pitch and timbre.","type":"content","url":"/week3#principles-of-instrument-acoustics","position":53},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Organology","lvl2":"Instrument acoustics"},"type":"lvl3","url":"/week3#organology","position":54},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Organology","lvl2":"Instrument acoustics"},"content":"In addition to acousticians, there is an academic field focused on the study of musical instruments: organology. This was developed by (ethno)musicologists and researchers working in instrument museums in the late 19th and early 20th century, based on the need to organize large collections of instruments.\n\nThe most famous organological system is the \n\nHornbostel–Sachs System, which classifies instruments based on how they produce sound. The main categories are:\n\nIdiophones: Instruments that produce sound primarily by the vibration of their own material, without strings, membranes, or external air columns. Examples include xylophones, cymbals, and bells.\n\nMembranophones: Instruments that produce sound by vibrating a stretched membrane. Drums are the most common example, where the membrane is struck, rubbed, or otherwise excited.\n\nChordophones: Instruments that produce sound by vibrating strings stretched between fixed points. This group includes violins, guitars, harps, and pianos.\n\nAerophones: Instruments that produce sound by vibrating columns of air. Examples are flutes, trumpets, saxophones, and pipe organs.\n\nElectrophones: Instruments that produce sound primarily through electrical means. This includes synthesizers, electric guitars (when amplified), and theremins.\n\nEach category can be further subdivided based on how the sound is initiated (struck, plucked, bowed, blown, etc.), the construction of the instrument, and its acoustic properties. Although not entirely similar,  Disney made an interesting animation film called Toot Whistle Plunk and Boom in 1953 that shows differences between instruments:\n\n","type":"content","url":"/week3#organology","position":55},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Electro-Acoustics"},"type":"lvl2","url":"/week3#electro-acoustics","position":56},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Electro-Acoustics"},"content":"Electro-Acoustics\" refers to the field of study and technology that deals with the conversion between electrical signals and sound waves. It encompasses the design, analysis, and application of devices that perform this conversion, such as microphones, loudspeakers, headphones, and hearing aids. This field combines principles from acoustics (the science of sound) and electronics to create systems that enable sound recording, reproduction, and transmission.\n\n","type":"content","url":"/week3#electro-acoustics","position":57},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Microphones","lvl2":"Electro-Acoustics"},"type":"lvl3","url":"/week3#microphones","position":58},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Microphones","lvl2":"Electro-Acoustics"},"content":"Microphones are devices that convert sound waves into electrical signals. They are categorized by their design and working principles. The three main types are:\n\nDynamic microphones use a diaphragm attached to a coil of wire, which is placed within a magnetic field. When sound waves hit the diaphragm, it moves the coil, generating an electrical signal through electromagnetic induction. These microphones are known for their durability and ability to handle high sound pressure levels, making them ideal for live performances and situations where robustness is required.\n\nCondenser microphones operate using a diaphragm positioned close to a charged backplate, together forming a capacitor. Sound waves cause the diaphragm to move, changing the distance between the diaphragm and backplate, which alters the capacitance and produces an electrical signal. This design makes condenser microphones highly sensitive and capable of capturing subtle details, making them well-suited for studio recordings and applications requiring high fidelity.\n\nContact microphones detect vibrations directly from solid surfaces rather than from the air. They often use piezoelectric materials to convert these vibrations into electrical signals. This type of microphone is commonly used for amplifying acoustic instruments such as violins or guitars, as it can pick up the vibrations from the instrument’s body, providing a unique perspective on the sound.\n\nIn her exploreation of microphones as instruments, Cathy van Eck calls microphones for “softhearers” as a parallel to loudspeakers.\n\n","type":"content","url":"/week3#microphones","position":59},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Loudspeakers","lvl2":"Electro-Acoustics"},"type":"lvl3","url":"/week3#loudspeakers","position":60},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Loudspeakers","lvl2":"Electro-Acoustics"},"content":"Loudspeakers (or perhaps only speakers) are devices that convert electrical signals into sound waves. They come in various types based on their application and design:\n\nStandard Speakers are used in audio systems for general sound reproduction. They employ a diaphragm (cone) driven by an electromagnet to produce sound and are designed to cover a wide range of frequencies, making them suitable for most listening environments.\n\nHeadphones are miniature speakers worn on, around, or in the ears. All headphones feature passive noise cancelling by design (since they cover the ears), but there are also different types of active noise cancelling.\n\nActuators are devices that create vibrations, typically in solid objects. The vibrations are typically not audible by itself, it is the resonances in the objects excited by the actuator that produces the sound. They are used in haptic feedback systems or to turn surfaces, such as tables or windows, into speakers. They are also used in \n\nbone-conducting headsets.\n\nNote\n\nDid you know that \n\nactive noise cancelling is based on inverting the phase of the sound signal?\n\n","type":"content","url":"/week3#loudspeakers","position":61},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Signal processing","lvl2":"Electro-Acoustics"},"type":"lvl3","url":"/week3#signal-processing","position":62},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Signal processing","lvl2":"Electro-Acoustics"},"content":"Signal processing is the field concerned with analyzing, modifying, and synthesizing signals such as sound, images, and scientific measurements. In acoustics and audio engineering, signal processing is essential for improving sound quality, extracting information, and adapting audio for specific applications.\n\nSignal processing can be done in both the analog and digital domain. Key aspects of signal processing in audio include:\n\nAmplification: Increasing the strength of electrical signals so they can drive loudspeakers or be recorded at usable levels.\n\nFiltering: Removing unwanted frequencies (such as noise or hum) or enhancing desired frequency ranges. Filters can be low-pass, high-pass, band-pass, or notch filters, each serving different purposes.\n\nEqualization (EQ): Adjusting the balance between frequency components to shape the tonal quality of audio signals.\n\nDynamic Range Compression: Reducing the difference between the loudest and quietest parts of a signal to make audio more consistent and prevent distortion.\n\nNoise Reduction: Techniques such as gating, spectral subtraction, or adaptive filtering to minimize background noise.\n\nEffects Processing: Adding reverberation, delay, chorus, distortion, or other effects to enhance or creatively alter the sound.\n\nModulation: Changing aspects of the signal such as amplitude, frequency, or phase for transmission or synthesis.\n\nSignal processing is used in microphones, mixing consoles, audio interfaces, hearing aids, mobile devices, and music production software. It enables clear communication, high-fidelity music reproduction, and creative sound design. In the following, we will briefly look into digital sound.\n\n","type":"content","url":"/week3#signal-processing","position":63},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Digital audio"},"type":"lvl2","url":"/week3#digital-audio","position":64},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Digital audio"},"content":"Digital audio is a huge field, so here we will only be able to scratch the surface.","type":"content","url":"/week3#digital-audio","position":65},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sound vs audio","lvl2":"Digital audio"},"type":"lvl3","url":"/week3#sound-vs-audio","position":66},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sound vs audio","lvl2":"Digital audio"},"content":"It is important to understand the difference between sound and audio. Sound refers to vibrations traveling through materials, while audio is the technology or electrical representation used to capture, store, and reproduce those vibrations. Audio can exist in both analog and digital forms, such as LPs, cassettes, or digital files, and serves as the intermediary between devices like microphones and speakers. The distinction is similar to that between light (a physical phenomenon) and video (its recorded representation).","type":"content","url":"/week3#sound-vs-audio","position":67},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Digitization","lvl2":"Digital audio"},"type":"lvl3","url":"/week3#digitization","position":68},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Digitization","lvl2":"Digital audio"},"content":"Digitization is the process of converting physical sound to digital audio signals using an \n\nAnalog-to-Digital Converter (ADC). This involves two main steps: sampling and quantization.\n\nFirst, the continuous sound wave is measured at regular intervals (samples per second), known as the sampling rate (SR). Next, each sampled value is rounded to the nearest value that can be represented by a fixed number of bits (the bit depth). Higher bit depths allow for more precise representation of amplitude, resulting in higher audio quality and lower quantization noise. The result is a stream of numbers that can be stored, processed, and transmitted by computers and digital devices.\n\nBelow is a visualization of how a continuous sound wave (such as a sine wave or white noise) is sampled and quantized. The blue curve represents the original continuous wave. The red dots show the sampled points at a specific sampling rate. Quantization further rounds these sampled values to discrete levels, determined by the bit depth.\n\n# Define parameters for the sine wave\nfrequency = 1  # Frequency in Hz\namplitude = 1  # Amplitude of the sine wave\nsampling_rates = [5, 10, 20, 50]  # Different sampling rates in Hz\n\n# Generate the continuous sine wave\ncontinuous_wave = amplitude * np.sin(2 * np.pi * frequency * x)\n\n# Create the figure\nplt.figure(figsize=(12, 2))\n\n# Plot the continuous sine wave\nplt.plot(x, continuous_wave, label='Continuous Sine Wave', color='blue', alpha=0.7)\n\n# Plot sampled points for each sampling rate\nfor i, rate in enumerate(sampling_rates):\n    sampled_x = np.linspace(0, 2 * np.pi, rate, endpoint=False)\n    sampled_wave = amplitude * np.sin(2 * np.pi * frequency * sampled_x)\n    plt.scatter(sampled_x, sampled_wave, label=f'Sampled Points (Rate = {rate} Hz)', zorder=5)\n\nplt.title('Effect of Different Sampling Rates on a Sine Tone')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\n","type":"content","url":"/week3#digitization","position":69},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sampling rate","lvl2":"Digital audio"},"type":"lvl3","url":"/week3#sampling-rate","position":70},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Sampling rate","lvl2":"Digital audio"},"content":"The sampling rate is the number of samples per second taken from a continuous signal to create a discrete signal. According to the \n\nNyquist–Shannon sampling theorem, the sampling rate must be at least twice the highest frequency present in the signal to accurately reconstruct it. This minimum rate is known as the \n\nNyquist Frequency. When the CD was introduced, it was decided that it should have a sampling rate of 44,100 Hz, which can capture frequencies up to 22,050 Hz. As we shall see next week, this is sufficient to cover all humanly audible sounds.\n\nNowadays, it is often common to have much higher sampling rates, such as 96 kHz and 192 kHz. This is far beyond human hearing, but there are several benefits in audio processing:\n\nExtended Frequency Response: While humans cannot hear frequencies above ~20 kHz, higher sampling rates allow for accurate recording and reproduction of ultrasonic content, which can affect the audible range through nonlinear processing or analog equipment.\n\nReduced Aliasing: Aliasing is distortion caused when high-frequency signals are misrepresented as lower frequencies. Higher sampling rates push the aliasing artifacts further above the audible range, making them easier to filter out.\n\nImproved Phase Accuracy: Digital filters and processing algorithms can operate with greater precision at higher sampling rates, resulting in more accurate phase response and less pre-ringing or artifacts.\n\nBetter Headroom for Processing: Audio editing, mixing, and effects (such as pitch shifting or time stretching) can be performed with fewer artifacts and greater fidelity at higher sampling rates.\n\nArchival and Mastering Quality: Recording at higher rates preserves more information for future remastering or conversion to other formats, ensuring the highest possible quality.\n\nHowever, higher sampling rates also result in larger file sizes and increased CPU usage, so they are typically used in professional recording, mixing, and mastering environments rather than for consumer playback.\n\n","type":"content","url":"/week3#sampling-rate","position":71},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Bit depth","lvl2":"Digital audio"},"type":"lvl3","url":"/week3#bit-depth","position":72},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Bit depth","lvl2":"Digital audio"},"content":"The \n\naudio bit depth is the amount of data used to represent each individual sample in a digital audio signal. Bit depth determines the “resolution” or precision of the amplitude values that can be stored. Higher bit depths allow for more accurate representation of the original sound, resulting in lower quantization noise and a greater dynamic range.\n\nLow bit depth (e.g., 8-bit): Only a small number of amplitude levels are available, which can introduce audible distortion and noise, especially in quiet passages.\n\nMedium bit depth (e.g., 16-bit): CD-quality audio uses 16 bits per sample, allowing for 65,536 possible amplitude values.\n\nHigh bit depth (e.g., 24-bit): Professional audio recordings nowadays often use 24 bits per sample, providing over 16 million possible values and a wider dynamic range. Many more amplitude levels are available, resulting in smoother, more natural sound and the ability to capture subtle details.\n\nQuite recently, there are 32-bit recorders available, offering an extremely high dynamic range, far beyond what human hearing or traditional analog equipment can capture. With 32-bit float, you can record both very quiet and extremely loud sounds without worrying about clipping (distortion from signals being too loud) or noise floor issues (signals being too quiet). This means you can adjust levels after recording without losing audio quality, making it nearly impossible to ruin a recording due to incorrect gain settings. While most playback systems and final mixes use 24-bit or 16-bit audio, 32-bit float is a powerful tool for capturing and processing audio with maximum flexibility and safety during production.\n\nThe effect of bit depth can be visualized by quantizing a waveform at different bit depths. Lower bit depths produce a “stepped” appearance and more distortion, while higher bit depths closely follow the original waveform.\n\n# Define parameters for the sine wave\nfrequency = 1  # Frequency in Hz\namplitude = 1  # Amplitude of the sine wave\nsampling_rate = 50  # Sampling rate in Hz (samples per second)\n\n# Generate the continuous sine wave\ncontinuous_wave = amplitude * np.sin(2 * np.pi * frequency * x)\n\n# Generate the sampled points\nsampled_x = np.linspace(0, 2 * np.pi, sampling_rate, endpoint=False)\nsampled_wave = amplitude * np.sin(2 * np.pi * frequency * sampled_x)\n\n# Quantize the sampled wave at different bit depths\nbit_depths = [2, 4, 8]  # Bit depths to demonstrate\nquantized_waves = [np.round(sampled_wave * (2**(b-1) - 1)) / (2**(b-1) - 1) for b in bit_depths]\n\n# Plot the continuous wave, sampled points, and quantized waves\nplt.figure(figsize=(12, 2))\n\n# Plot the continuous sine wave\nplt.plot(x, continuous_wave, label='Continuous Sine Wave', color='blue', alpha=0.7)\n\n# Plot the sampled points\nplt.scatter(sampled_x, sampled_wave, color='red', label='Sampled Points', zorder=5)\n\n# Plot quantized waves\nfor i, b in enumerate(bit_depths):\n    plt.step(sampled_x, quantized_waves[i], where='mid', label=f'Quantized ({b}-bit)', alpha=0.8)\n\nplt.title('Effect of Bit Depth on Sine Wave Quantization')\nplt.xlabel('x')\nplt.ylabel('Amplitude')\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n\n","type":"content","url":"/week3#bit-depth","position":73},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Audio Compression and File Formats","lvl2":"Digital audio"},"type":"lvl3","url":"/week3#audio-compression-and-file-formats","position":74},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl3":"Audio Compression and File Formats","lvl2":"Digital audio"},"content":"The final topic in today’s class is audio file formats and compression, essential for storing, sharing, and streaming digital sound.\n\nAudio containers are file formats that store digital audio data along with metadata (such as track info, album art, and artist details). Common containers include \n\nWAV (Windows) and \n\nAIFF (Apple), which both offer uncompressed, high-quality audio.\n\nSeveral container types, e.g. \n\nMKV, can contain raw (uncompressed) and/or compressed audio data alongside video and metadata. Others, like \n\nMP4, store compressed audio, often in combination with video.\n\nIt is important to note that the container used for storing the file is (often) independent from the audio compression used. Note that audio file compression is not the same as the dynamics compression often used to improve the balance in recordings. Audio file compression is based on reducing the file size of digital audio, making it easier to store and transmit. There are two main types:\n\nLossless compression: Preserves all original audio data, allowing perfect reconstruction, such as \n\nFLAC and \n\nALAC.\n\nLossy compression: Removes some audio data, typically those less perceptible to human hearing, to achieve smaller file sizes, such as \n\nMP3 and \n\nAAC. Here is a quick overview of what to use:\n\nFormat\n\nCompression Type\n\nTypical Use\n\nQuality\n\nWAV, AIFF\n\nNone\n\nRecording, editing\n\nExcellent\n\nFLAC\n\nLossless\n\nArchiving, hi-fi\n\nExcellent\n\nAAC\n\nLossy\n\nStreaming\n\nGood\n\nWhile MP3 files can provide good quality, AAC compression generally produces smaller file size and are therefore preferred. For those concerned about using open formats, such as the \n\nOgg container and related compression formats.\n\nTry to store an uncompressed audio file in a highly compressed format (MP3 or AAC). Listen to how it degrades.\n\n","type":"content","url":"/week3#audio-compression-and-file-formats","position":75},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Questions"},"type":"lvl2","url":"/week3#questions","position":76},{"hierarchy":{"lvl1":"Week 3: Acoustics","lvl2":"Questions"},"content":"What is the difference between longitudinal and transverse waves, and how do these wave types relate to the propagation of sound in different media?\n\nExplain the concepts of frequency, amplitude, and phase in sound waves. How do changes in each property affect the perception of sound?\n\nDescribe the role of room acoustics in shaping sound quality. What are standing waves, room modes, and flutter echo, and how can acoustic treatment improve a room’s acoustics?\n\nCompare and contrast the main categories of musical instruments in the Hornbostel–Sachs system. How does each category produce sound, and what are some examples?\n\nWhat is the process of digitizing sound, and how do sampling rate and bit depth influence the quality of digital audio?","type":"content","url":"/week3#questions","position":77},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics"},"type":"lvl1","url":"/week4","position":0},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics"},"content":"This week we will explore the field of psychoacoustics, the study of how humans perceive and interpret sound, bridging the gap between physical sound properties and subjective auditory experiences. Psychoacoustics examines the psychological and physiological responses associated with sound, including how we detect, differentiate, and interpret auditory stimuli. It seeks to answer questions such as: Why do some sounds seem louder than others, even if they have the same physical intensity? How do we distinguish between different musical instruments playing the same note? What makes certain sounds pleasant or unpleasant? This will form the foundation for more detailed investigations of “vertical” and “horizontal” sound perception in the coming weeks.","type":"content","url":"/week4","position":1},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Introduction to Psychoacoustics"},"type":"lvl2","url":"/week4#introduction-to-psychoacoustics","position":2},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Introduction to Psychoacoustics"},"content":"Psychoacoustics is a specialized area within the broader field of psychophysics, which studies the relationships between physical stimuli and the sensations and perceptions they produce. While psychophysics investigates general principles of sensory perception across all senses, psychoacoustics focuses specifically on hearing. Both fields use experimental methods to quantify how changes in physical properties (like frequency, intensity, or duration) correspond to changes in perception (such as pitch, loudness, or timbre), helping to bridge the gap between objective measurements and subjective experience.\n\nPsychoacoustics explores the relationship between the measurable, physical properties of sound (such as frequency, amplitude, and harmonic relationships) and the way these sounds are perceived by the human ear and brain (such as pitch, loudness, and timbre). Insights from psychoacoustics are applied in audio engineering, music production, hearing aid design, and the development of audio codecs (such as MP3), which exploit perceptual limitations to compress audio data efficiently.\n\nThe roots of psychoacoustics can be traced back to the 19th century, with foundational work by scientists such as \n\nHermann von Helmholtz (1821–1894), who investigated the sensations of tone and the physical basis of music. Later, \n\nS. S. Stevens  (1906–1973) contributed to the development of psychophysical scaling, introducing methods to quantify the relationship between stimulus and perception (e.g., the Stevens’ Power Law for loudness perception). Over time, psychoacoustics has evolved into a multidisciplinary field, integrating insights from physics, psychology, neuroscience, and engineering.\n\nHow does psychoacoustics relate to auditory perception and cognition? Psychoacoustics forms the scientific foundation for understanding how we perceive and interpret sound. Auditory perception encompasses the processes by which the ear and brain detect, analyze, and make sense of acoustic signals—transforming vibrations in the air into meaningful experiences such as speech, music, or environmental sounds. Psychoacoustics explains why certain sounds are perceived as louder, higher, or more pleasant, and how we can distinguish between different sources in complex auditory scenes.\n\nCognition involves higher-level mental functions such as attention, memory, learning, and decision-making. While psychoacoustics focuses on the sensory and perceptual mechanisms of hearing, cognition shapes how we interpret, remember, and respond to sounds. For example, cognitive processes help us focus on a single voice in a noisy room (the “cocktail party effect”), recognize familiar melodies, or associate sounds with emotions and memories. Together, psychoacoustics and auditory cognition provide a comprehensive understanding of how we experience and interact with the world of sound.","type":"content","url":"/week4#introduction-to-psychoacoustics","position":3},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"The Human Auditory System"},"type":"lvl2","url":"/week4#the-human-auditory-system","position":4},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"The Human Auditory System"},"content":"The human ear is a remarkably complex organ that enables us to detect sounds and maintain our sense of balance. It is divided into three main sections: the outer ear, middle ear, and inner ear. Each part plays a distinct and crucial role in the auditory process.","type":"content","url":"/week4#the-human-auditory-system","position":5},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The outer ear","lvl2":"The Human Auditory System"},"type":"lvl3","url":"/week4#the-outer-ear","position":6},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The outer ear","lvl2":"The Human Auditory System"},"content":"The outer ear consists of the visible part called the pinna and the ear canal. Its primary function is to collect sound waves from the environment and funnel them toward the eardrum (sometimes call the tympanic membrane). The unique shape of the pinna helps us localize the direction of sounds. When sound waves reach the eardrum, they cause it to vibrate.\n\nImage Source: \n\nWikipedia - Outer Ear\n\nThe eardrum  in the human ear functions much like the membrane in a microphone. Both act as sensitive barriers that vibrate in response to incoming sound waves. When sound waves enter the ear canal, they strike the eardrum, causing it to vibrate. These vibrations are then transmitted through the ossicles to the inner ear, where they are converted into electrical signals for the brain to interpret. In a microphone, sound waves hit a thin, flexible membrane (diaphragm), causing it to vibrate. These vibrations are converted into electrical signals, which can then be amplified, recorded, or transmitted. As such, both the ear drum and the microphone membrane convert air pressure variations (sound waves) into mechanical vibrations. Both also serve as the first step in transforming acoustic energy into a form that can be further processed; biologically in the ear, electronically in the microphone.","type":"content","url":"/week4#the-outer-ear","position":7},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The middle ear","lvl2":"The Human Auditory System"},"type":"lvl3","url":"/week4#the-middle-ear","position":8},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The middle ear","lvl2":"The Human Auditory System"},"content":"Beyond the eardrum lies the middle ear, which contains three tiny bones known as the ossicles: the malleus, incus, and stapes. These bones act as a mechanical lever system, amplifying the vibrations from the eardrum and transmitting them to the oval window, a membrane-covered opening to the inner ear. This amplification is essential for efficiently transferring sound energy from air to the fluid-filled inner ear.\n\nImage Source: \n\nWikipedia - Middle Ear\n\nThis process is analogous to how a microphone connected to a preamplifier works. The vibrations from a microphone membrane are weak and are typically run through a preamplifier to boost the signal to a level suitable for further processing or recording. Similarly, the ossicles amplify the mechanical vibrations from the eardrum, ensuring that the signal is strong enough to be effectively transmitted into the inner ear for further processing by the auditory system.","type":"content","url":"/week4#the-middle-ear","position":9},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The inner ear","lvl2":"The Human Auditory System"},"type":"lvl3","url":"/week4#the-inner-ear","position":10},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"The inner ear","lvl2":"The Human Auditory System"},"content":"The inner ear is where the mechanical vibrations are transformed into electrical signals that the brain can interpret as sound. The main structure responsible for this is the cochlea, a spiral-shaped, fluid-filled organ lined with thousands of tiny hair cells. As vibrations travel through the cochlear fluid, they cause the hair cells to move, generating nerve impulses that are sent to the brain via the auditory nerve.\n\nImage Source: \n\nWikipedia - Inner Ear\n\nThe cochlea in the inner ear functions similarly to an analog-to-digital converter (ADC) in audio technology. Just as an ADC transforms continuous analog sound waves into discrete digital signals that can be processed by computers, the cochlea converts mechanical vibrations from sound into electrical nerve impulses that the brain can interpret. Inside the cochlea, thousands of hair cells respond to specific frequencies, effectively performing a biological form of frequency analysis and encoding the intensity and timing of sounds. This process is analogous to how an ADC samples and quantizes audio signals, enabling the transmission and interpretation of complex auditory information in a form the brain can understand.\n\nThe inner ear also contains the \n\nvestibular system, which is crucial for maintaining balance and spatial orientation. We will get back to this when we get to music-related body motion.","type":"content","url":"/week4#the-inner-ear","position":11},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Human vs machine perception","lvl2":"The Human Auditory System"},"type":"lvl3","url":"/week4#human-vs-machine-perception","position":12},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Human vs machine perception","lvl2":"The Human Auditory System"},"content":"Although the human auditory system and machine-based audio systems (like microphones and ADCs) are fundamentally different in their biological and technological makeup, drawing analogies between them can help us better understand both. Each system consists of a series of components that transform and process sound, ultimately converting physical vibrations in the air into meaningful information—whether as neural signals in the brain or digital data in a computer.graph LR\n    A[Sound Waves] --> B[Transduction]\n    B --> C[Frequency analysis]\n    C --> D[Encoding]\n    D --> E[Transmission]\n    E --> F[Interpretation]\n\nThe different parts are as follows:\n\nTransduction: In humans, the ear drum and ossicles convert air pressure variations into mechanical vibrations, while the cochlea transduces these vibrations into electrical nerve impulses. In machines, a microphone membrane converts sound waves into electrical signals, which are then amplified and digitized.\n\nFrequency Analysis: The cochlea performs a kind of real-time frequency analysis, with different regions responding to different frequencies (a biological “filter bank”). Similarly, digital systems use mathematical transforms (like the Fourier Transform) to analyze frequency content.\n\nEncoding and Transmission: The auditory nerve encodes and transmits information about sound to the brain, where it is further processed and interpreted. In digital systems, the ADC encodes the analog signal into binary data, which can be stored, transmitted, and processed by computers.\n\nDespite these similarities, there are important differences:\n\nThe human auditory system is adaptive, context-sensitive, and influenced by attention, learning, and memory. It can focus on specific sounds in noisy environments (the “cocktail party effect”) and fill in missing information based on prior experience.\n\nMachine perception is limited by hardware specifications (such as microphone quality and sampling rate) and the algorithms used for analysis. While modern systems can achieve impressive results, they lack the flexibility, nuance, and subjective interpretation of human hearing.\n\nUnderstanding these parallels and differences is essential for fields like audio engineering, hearing aid design, and music information retrieval, where the goal is often to bridge the gap between physical sound and perceptual experience.\n\n","type":"content","url":"/week4#human-vs-machine-perception","position":13},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Loudness"},"type":"lvl2","url":"/week4#loudness","position":14},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Loudness"},"content":"If you recall from last week, loudness is different from sound pressure level (SPL). While sound pressure level (SPL) is an objective, physical measurement of the intensity of a sound wave (expressed in decibels, dB), loudness is a subjective perception—how “loud” a sound feels to a human listener. Loudness perception is influenced by context, such as background noise and recent exposure to other sounds. This distinction is fundamental in psychoacoustics and underlies many practical considerations in audio engineering, hearing science, and music production.","type":"content","url":"/week4#loudness","position":15},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Measuring loudness","lvl2":"Loudness"},"type":"lvl3","url":"/week4#measuring-loudness","position":16},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Measuring loudness","lvl2":"Loudness"},"content":"The most common unit for measuring perceived loudness is the phon, which is based on equal-loudness contours (which we will discuss soon). The phon scale is anchored to the loudness of a 1,000 Hz pure tone. For example, a sound that is perceived as equally loud as a 40 dB SPL, 1,000 Hz tone is said to have a loudness of 40 phons, regardless of its frequency or SPL.\n\nThere are several standards for estimating loudness in audio signals. ITU-R BS.1770 is widely used in broadcasting; this standard defines algorithms for measuring loudness and true-peak audio levels, forming the basis for loudness normalization in radio, TV, and streaming.\n\nModern audio software and digital audio workstations (DAWs) often include loudness meters that display values in LUFS: Loudness units relative to full scale. These tools help engineers and producers ensure that audio content meets broadcast standards and provides a comfortable listening experience. When mastering music for streaming platforms, engineers typically aim for an integrated loudness of around -14 LUFS, as recommended by services like Spotify and YouTube. In film and TV, loudness normalization ensures that dialogue, music, and effects are balanced and that viewers do not need to constantly adjust the volume.","type":"content","url":"/week4#measuring-loudness","position":17},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Threshold of hearing","lvl2":"Loudness"},"type":"lvl3","url":"/week4#threshold-of-hearing","position":18},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Threshold of hearing","lvl2":"Loudness"},"content":"The threshold of hearing is the quietest sound that the average human ear can detect in a silent environment. This threshold is typically defined as 0 decibels (dB SPL) at 1,000 Hz for a healthy young adult, but it varies with frequency and individual hearing ability. At very low or very high frequencies, sounds must be much louder to be heard.\n\nThe threshold of hearing is not a fixed value; it can be affected by age, exposure to loud noises, and even temporary conditions like ear infections or fatigue. As people age, their sensitivity to high frequencies usually decreases, raising the threshold at those frequencies.\n\nThe graph below illustrates the average hearing thresholds for different frequencies and age groups, showing how sensitivity changes across the audible spectrum.\n\n\nImage Source: \n\nWikipedia - Hearing Thresholds","type":"content","url":"/week4#threshold-of-hearing","position":19},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Safe listening levels","lvl2":"Loudness"},"type":"lvl3","url":"/week4#safe-listening-levels","position":20},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Safe listening levels","lvl2":"Loudness"},"content":"Here is a list of some different sound levels:\n\nSound Source\n\nApproximate Loudness (dB SPL)\n\nExample / Context\n\nThreshold of hearing\n\n0\n\nQuietest sound a healthy ear can detect\n\nRustling leaves\n\n10\n\nVery quiet, barely audible\n\nWhisper\n\n20–30\n\nSoft whisper at close distance\n\nQuiet library\n\n30–40\n\nTypical background noise\n\nNormal conversation\n\n60\n\nAt 1 meter distance\n\nBusy street traffic\n\n70–85\n\nInside a car with windows closed\n\nSubway train\n\n95–100\n\nInside the train\n\nRock concert\n\n110–120\n\nNear speakers\n\nThreshold of pain\n\n130\n\nJet engine at 30 meters\n\nFireworks / Gunshot\n\n140–150\n\nClose range\n\nAs a rule of thumb, prolonged exposure to sounds above 85 dB can cause hearing damage. For example, typical conversation is around 60 dB, city traffic can reach 85 dB, and concerts or clubs often exceed 100 dB. The higher the volume, the shorter the safe exposure time. At 100 dB, hearing damage can occur in as little as 15 minutes.\n\nThe World Health Organization (WHO) recommends keeping personal listening devices below 80 dB for adults (75 dB for children) and limiting exposure to loud environments. Using ear protection in noisy settings and taking listening breaks can help preserve hearing health.\n\nSounds above 120–130 dB (such as jet engines at close range or fireworks) can cause immediate pain and permanent hearing loss. Even brief exposure to extremely loud sounds can result in irreversible damage to the hair cells in the inner ear.\n\nQuestion\n\nHow do you protect your hearing?","type":"content","url":"/week4#safe-listening-levels","position":21},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Equal Loudness Contours","lvl2":"Loudness"},"type":"lvl3","url":"/week4#equal-loudness-contours","position":22},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Equal Loudness Contours","lvl2":"Loudness"},"content":"One important thing to consider is that there is not a direct relationship between \n\nsound pressure level (SPL) and \n\nloudness. Although they are related, the same SPL can be perceived as having a different loudness depending on the frequency of the sound and the listener’s hearing sensitivity.\n\nThis relationship was first systematically studied by \n\nHarvey Fletcher and \n\nWilden A. Munson in the 1930s. Their experiments led to the creation of the Fletcher-Munson curves, which is nowadays often also known as equal-loudness contours. These curves show that the human ear is not equally sensitive to all frequencies: we are most sensitive to frequencies between roughly 2,000 and 5,000 Hz, and less sensitive to very low or very high frequencies. As a result, a low-frequency sound must be played at a higher SPL than a mid-frequency sound to be perceived as equally loud.\n\n\nImage Source: \n\nWikipedia - Equal-loudness contour\n\nThere is evidence suggesting that our heightened sensitivity to frequencies between 2,000 and 5,000 Hz has evolutionary roots. This frequency range corresponds closely to the spectral content of human speech, particularly the consonant sounds that are crucial for understanding language. Being able to detect subtle differences in these frequencies would have provided a survival advantage by improving communication, social bonding, and the ability to detect important environmental sounds such as a baby’s cry or warning calls. Over time, natural selection may have favored individuals whose hearing was optimized for these frequencies, shaping the contours of human auditory perception.\n\nEqual-loudness contours are crucial in audio engineering, music production, and hearing science. They explain why music or speech can sound different at low versus high volumes, and why audio equipment often includes “loudness” compensation to adjust for these perceptual differences. Understanding these contours helps us design better audio systems and create more accurate sound reproductions that align with human hearing.\n\nSine tone loudness\n\nIn \n\nGlicol, try to play a sine tone with different frequencies, for example, 100, 500, 1000, 3000, 5000, 10,000 Hz. What do you notice in terms of how loud it sounds like?","type":"content","url":"/week4#equal-loudness-contours","position":23},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just noticeable differences for loudness","lvl2":"Loudness"},"type":"lvl3","url":"/week4#just-noticeable-differences-for-loudness","position":24},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just noticeable differences for loudness","lvl2":"Loudness"},"content":"The just noticeable difference (JND), also called the difference limen, is the smallest change in a physical stimulus that a listener can reliably detect. For loudness, the JND refers to the smallest change in sound intensity that can be perceived as a difference in loudness by a human listener.\n\nThe JND for loudness is often expressed as a percentage of the original sound level. For mid-level sounds, the JND is typically about 1 dB (decibel), meaning that a change of at least 1 dB is needed for most people to notice a difference in loudness. This threshold can vary depending on the frequency, the absolute loudness, and the listening environment.\n\nJND for loudness\n\nIn \n\nGlicol, try to play a sine tone with different amplitues. How little or much do you need to change to hear a difference?\n\n","type":"content","url":"/week4#just-noticeable-differences-for-loudness","position":25},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Pitch"},"type":"lvl2","url":"/week4#pitch","position":26},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Pitch"},"content":"When we hear tonal sounds, like musical instruments but also voices, we typically hear a pitch. The pitch is closely related to the physical property of frequency (the rate at which a sound wave vibrates), but it is ultimately a subjective experience shaped by the human auditory system.","type":"content","url":"/week4#pitch","position":27},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Pitch range","lvl2":"Pitch"},"type":"lvl3","url":"/week4#pitch-range","position":28},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Pitch range","lvl2":"Pitch"},"content":"The typical range of audible sound for humans spans from approximately \n\n20 Hz to \n\n20,000 Hz, encompassing the frequencies we perceive as \n\npitch. Sounds below 20 Hz are referred to as \n\ninfrasound and are generally imperceptible to the human ear, while those above 20,000 Hz are called \n\nultrasound and also lie beyond our hearing range. Within the audible spectrum, our sensitivity to pitch and frequency varies, with the greatest acuity in the mid-frequency range, which is crucial for understanding speech and music.\n\nExploring Pitch with Glicol\n\nUse \n\nGlicol to play sine tones at different frequencies across the human hearing range. Try frequencies at the extremes, below 100 Hz and above 15,000 Hz. What is the range of your hearing?\n\nIt is well-known that dogs have better hearing than humans, and the following graphs show that humans are in the middle of the pitch range among animals. Different animal species have evolved to detect sounds in frequency ranges that are most relevant to their survival and communication. For example, dogs can hear frequencies up to around 45,000 Hz, allowing them to detect high-pitched noises that are inaudible to humans. Bats and dolphins can perceive even higher frequencies, well into the ultrasonic range, which they use for echolocation. On the other hand, elephants and some whales can hear infrasound—very low frequencies below the human hearing threshold—which helps them communicate over long distances.\n\nThe chart below compares the hearing ranges of various animals, illustrating how human hearing fits within the broader spectrum of animal auditory perception. This diversity in hearing abilities reflects the different ecological needs and evolutionary pressures faced by each species. Understanding these differences not only highlights the uniqueness of human hearing but also provides insight into how other animals experience the world of sound.","type":"content","url":"/week4#pitch-range","position":29},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just Noticeable Differences for pitch","lvl2":"Pitch"},"type":"lvl3","url":"/week4#just-noticeable-differences-for-pitch","position":30},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just Noticeable Differences for pitch","lvl2":"Pitch"},"content":"We have a just noticeable difference (JND) also for pitch, which relates to the smallest change in frequency that a listener can reliably detect. The human ear is highly sensitive to small frequency changes, especially in the mid-frequency range (about 500–4000 Hz, where speech and music are most prominent). For example, at 1000 Hz, the JND for pitch is typically around 3 Hz for trained listeners, which is about 0.3% of the frequency. This means that if you play two tones at 1000 Hz and 1003 Hz, a trained listener can usually tell them apart. At very low or very high frequencies, the JND becomes larger, meaning it is harder to distinguish small pitch differences.\n\nExplore JND for Pitch\n\nTry using \n\nGlicol to play two sine tones in succession: start with a reference tone (e.g., 1000 Hz), then play a second tone slightly higher (e.g., 1002 Hz, 1005 Hz, 1010 Hz). At what frequency difference can you reliably hear a change in pitch? Try different frequency ranges and note how your sensitivity changes.\n\nThe JND for pitch vary depending on factors such as the listener’s age, hearing ability, training, the loudness of the tones, and whether the tones are played in isolation or in a complex sound environment.\n\n","type":"content","url":"/week4#just-noticeable-differences-for-pitch","position":31},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Timbre"},"type":"lvl2","url":"/week4#timbre","position":32},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Timbre"},"content":"Timbre is the quality of a sound that distinguishes different types of sound sources, even when they have the same pitch and loudness. It is what allows us to tell apart a piano and a violin playing the same note at the same volume. Timbre is shaped by the spectral content (the mix of the fundamental and the overtones), the temporal envelope (attack, decay, sustain, release), and other characteristics such as vibrato or noise components. In music and audio, timbre is essential for identifying instruments, voices, and sound textures.","type":"content","url":"/week4#timbre","position":33},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Overtones","lvl2":"Timbre"},"type":"lvl3","url":"/week4#overtones","position":34},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Overtones","lvl2":"Timbre"},"content":"When a musical instrument or voice produces a note, it doesn’t just generate a single frequency (the fundamental), but also a series of higher frequencies called overtones or harmonics. These overtones are integer multiples of the fundamental frequency and contribute to the richness and color of the sound.\n\nFundamental: The lowest frequency of a sound, typically perceived as its pitch.\n\nOvertones/Harmonics: Frequencies above the fundamental. The first overtone is the second harmonic (2× the fundamental), the second overtone is the third harmonic (3× the fundamental), and so on.\n\nA pure sine wave has no overtones and sounds “plain” or “pure,” while most musical sounds are complex and rich due to their overtone content. The specific pattern and strength of overtones determine the timbre of an instrument. For example, a clarinet and a violin playing the same note will have different overtone structures, making them sound distinct.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\n# Spectrogram for sound1 (log scale, limited to 0-5000 Hz)\nplt.subplot(2, 1, 1)\nplt.specgram(sound1, Fs=sr, NFFT=128, noverlap=64, cmap='viridis', scale='dB')\nplt.ylim(0, 5000)\nplt.title('Spectrogram (0-5000 Hz): Pure Tone')\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency (Hz)')\n\n# Spectrogram for sound2 (log scale, limited to 0-5000 Hz)\nplt.subplot(2, 1, 2)\nplt.specgram(sound2, Fs=sr, NFFT=128, noverlap=64, cmap='viridis', scale='dB')\nplt.ylim(0, 5000)\nplt.title('Spectrogram (0-5000 Hz): Tone with Overtones')\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency (Hz)')\n\nplt.tight_layout()\nplt.show()\n\n\n","type":"content","url":"/week4#overtones","position":35},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Envelope","lvl2":"Timbre"},"type":"lvl3","url":"/week4#envelope","position":36},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Envelope","lvl2":"Timbre"},"content":"The envelope of a sound describes how its amplitude changes over time, shaping the sound’s overall character and dynamics. The envelope determines how a sound starts, develops, and ends, and is crucial for distinguishing between different instruments and sound types.\n\nIn audio synthesis, the most common model is the ADSR envelope, which stands for Attack, Decay, Sustain, and Release:\n\nAttack: The time it takes for the sound to reach its maximum amplitude after being triggered.\n\nDecay: The time it takes for the amplitude to decrease from the peak level to the sustain level.\n\nSustain: The level at which the sound holds while the note is sustained.\n\nRelease: The time it takes for the sound to fade to silence after the note is released.\n\n\nImage source: \n\nADSR envelope on Wikipedia.","type":"content","url":"/week4#envelope","position":37},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just noticeable differences of timbre","lvl2":"Timbre"},"type":"lvl3","url":"/week4#just-noticeable-differences-of-timbre","position":38},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Just noticeable differences of timbre","lvl2":"Timbre"},"content":"How many partials do you need to recognize the timbre of a sound. Below you can find examples of a Sony Rollins saxophone tone that has been spectrally decomposed and with resynthesis of an increasing number of overtones.\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Audio\n\nfor fname in filenames:\n    y, sr = librosa.load(fname, sr=None)\n    plt.figure(figsize=(8, 3))\n    S = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n    librosa.display.specshow(S, sr=sr, x_axis='time', y_axis='log')\n    plt.title(f\"Spectrogram: {fname.split('/')[-1]}\")\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout()\n    plt.show()\n    display(Audio(y, rate=sr))\n\n\nHow many overtones do you need to hear the full richness of the saxophone?\n\n","type":"content","url":"/week4#just-noticeable-differences-of-timbre","position":39},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Spatial perception"},"type":"lvl2","url":"/week4#spatial-perception","position":40},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Spatial perception"},"content":"Spatial hearing refers to our ability to perceive the location and movement of sound sources in our environment. This skill allows us to determine where a sound is coming from—whether it is in front, behind, above, below, or to the side. Spatial hearing is essential for navigating the world, understanding speech in complex environments, and enjoying immersive audio experiences in music and virtual reality.\n\nBinaural hearing is the process of using both ears to perceive spatial cues and localize sounds. By comparing the differences in timing (ITD) and intensity (ILD) between the two ears, the brain constructs a three-dimensional auditory scene. Binaural hearing is essential for depth perception, sound localization, and separating different sound sources in complex environments.","type":"content","url":"/week4#spatial-perception","position":41},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Interaural time difference","lvl2":"Spatial perception"},"type":"lvl3","url":"/week4#interaural-time-difference","position":42},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Interaural time difference","lvl2":"Spatial perception"},"content":"The \n\ninteraural time difference (ITD) is the difference in the arrival time of a sound at each ear. When a sound source is positioned off-center, it reaches one ear slightly before the other. The brain uses this tiny time difference, especially for low-frequency sounds, to help localize the direction of the sound source on the horizontal plane. ITD is a primary cue for determining the azimuth (left-right position) of sounds.","type":"content","url":"/week4#interaural-time-difference","position":43},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Interaural level difference","lvl2":"Spatial perception"},"type":"lvl3","url":"/week4#interaural-level-difference","position":44},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Interaural level difference","lvl2":"Spatial perception"},"content":"Interaural level difference (ILD) refers to the difference in sound pressure level reaching each ear. When a sound comes from one side, the head acts as a barrier, causing the sound to be louder in the ear closer to the source and quieter in the far ear. ILD is most effective for high-frequency sounds, where the head shadow effect is more pronounced. The brain combines ILD with ITD to accurately localize sounds.","type":"content","url":"/week4#interaural-level-difference","position":45},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Head-related transfer function","lvl2":"Spatial perception"},"type":"lvl3","url":"/week4#head-related-transfer-function","position":46},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Head-related transfer function","lvl2":"Spatial perception"},"content":"The \n\nhead-related transfer function (HRTF) describes how an ear receives a sound from a specific point in space, taking into account the effects of the listener’s head, torso, and outer ear (pinna). HRTFs are unique to each individual and are crucial for perceiving elevation and front-back differences in sound localization. They are widely used in 3D audio and virtual reality to simulate realistic spatial audio experiences.\n\nThe \n\nprecedence effect is a phenomenon where the first-arriving sound dominates our perception of the sound’s location, even if reflections or echoes follow shortly after. This effect helps us focus on the direct sound source in reverberant environments, such as distinguishing a speaker’s voice in a large hall, and prevents confusion from multiple sound reflections.","type":"content","url":"/week4#head-related-transfer-function","position":47},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Cocktail party effect","lvl2":"Spatial perception"},"type":"lvl3","url":"/week4#cocktail-party-effect","position":48},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Cocktail party effect","lvl2":"Spatial perception"},"content":"The \n\ncocktail party effect describes our remarkable ability to focus on a single sound source, such as a conversation partner, in a noisy environment filled with competing sounds. This selective attention relies on spatial hearing cues, as well as cognitive processes, to filter out background noise and enhance the target sound. The cocktail party effect is a key aspect of auditory scene analysis and is fundamental to effective communication in social settings.\n\n","type":"content","url":"/week4#cocktail-party-effect","position":49},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Auditory illusions"},"type":"lvl2","url":"/week4#auditory-illusions","position":50},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Auditory illusions"},"content":"Auditory illusions are fascinating because they reveal the complex ways in which our brains interpret sound, often going beyond the raw physical properties of audio signals. By studying these illusions, we gain insight into the mechanisms of human auditory perception, how we organize, prioritize, and sometimes misinterpret acoustic information. This understanding is crucial for fields like music production, audio engineering, hearing aid design, and the development of perceptually efficient audio codecs.","type":"content","url":"/week4#auditory-illusions","position":51},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Hysteresis","lvl2":"Auditory illusions"},"type":"lvl3","url":"/week4#hysteresis","position":52},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Hysteresis","lvl2":"Auditory illusions"},"content":"Hysteresis refers to the phenomenon where the response of a system depends not only on its current state but also on its past states. In psychoacoustics, hysteresis can be observed in loudness perception: the perceived loudness of a sound may depend on the sequence of sounds that came before it. For example, if a listener is exposed to a loud sound and then to a softer sound, the softer sound may seem even quieter than if it were heard in isolation. Conversely, a gradual increase in volume may be perceived differently than a sudden jump, even if the final sound pressure level is the same. This effect highlights how our auditory system adapts to changing sound environments and why context and listening history can influence how loud a sound seems.\n\n\nImage Source: \n\nWikipedia - Hysteresis","type":"content","url":"/week4#hysteresis","position":53},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Masking Effects","lvl2":"Auditory illusions"},"type":"lvl3","url":"/week4#masking-effects","position":54},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Masking Effects","lvl2":"Auditory illusions"},"content":"Masking in psychoacoustics refers to the phenomenon where the presence of one sound (the masker) makes it more difficult to hear another sound (the maskee). This occurs because the auditory system has limited frequency and temporal resolution, so strong or similar sounds can “cover up” weaker or nearby sounds, making them less perceptible or even inaudible.\n\nThere are different types of masking. \n\nSimultaneous Masking refers to when two sounds are played at the same time, a louder sound (the masker) can make a softer sound (the maskee) inaudible, even if both are within the listener’s hearing range. This effect is strongest when the sounds are close in frequency. For example, in music production, a loud bass drum can mask a softer bass guitar note if they occur together and share similar frequencies. This principle is used in \n\naudio compression, such as \n\nMP3, which remove masked sounds to reduce file size without noticeably affecting perceived audio quality.\n\nFrequency Masking is most effective when the masker and maskee are close in frequency, typically within the same \n\ncritical band. The human ear divides the frequency spectrum into critical bands, and sounds within the same band are more likely to mask each other. This is why a high-pitched sound is less likely to be masked by a low-pitched sound, and vice versa. Masking is generally stronger for frequencies above the masker (upward spread) than for those below. This means a loud low-frequency sound can mask higher-frequency sounds more effectively than the reverse.\n\nTemporal Masking refers to when a loud sound can mask a softer sound that occurs immediately before (\n\npre-masking) or after (\n\npost-masking) it, even if the two sounds do not overlap in time. Pre-masking can last up to about 20 ms before the masker, while post-masking can persist for up to 100 ms after the masker. This demonstrates the temporal resolution limits of human hearing and is also exploited in \n\nperceptual audio coding.\n\nThe graph below illustrates how a masker tone at a certain frequency and intensity can raise the \n\nthreshold of hearing for nearby frequencies, making them inaudible unless they are louder than the masking threshold. This is a key concept in \n\npsychoacoustics and \n\naudio engineering.\n\n\nImage Source: \n\nWikipedia - Audio Masking\n\nSome real-world examples of auditory masking includes how the sound of a passing truck can mask a conversation, making it difficult to hear speech until the truck has passed. In music, masking can be used creatively to blend instruments or to hide imperfections in a recording. In \n\nhearing aids, understanding masking helps in designing algorithms that enhance speech while suppressing background noise.","type":"content","url":"/week4#masking-effects","position":55},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Shepard and Risset tones","lvl2":"Auditory illusions"},"type":"lvl3","url":"/week4#shepard-and-risset-tones","position":56},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Shepard and Risset tones","lvl2":"Auditory illusions"},"content":"One of the most famous auditory illusions is named Shepard Tones after \n\nRoger Shepard (1929–2022). This is a series of tones that seem to continually ascend or descend in pitch, yet never get higher or lower. This creates the auditory equivalent of a “sonic barber pole.”\n\nThe Shepard tone is created by layering several sine waves that are spaced one octave apart. As the sequence progresses, all the sine waves move up (or down) in pitch simultaneously. The highest-pitched sine wave fades out as it reaches the top of the range, while a new, low-pitched sine wave fades in at the bottom. The overall amplitude envelope is shaped so that the listener always hears a similar blend of frequencies, with no clear starting or ending point. This continuous overlap and cross-fading trick the brain into perceiving a never-ending rise (or fall) in pitch, even though the actual frequencies are cycling within a fixed range. The illusion exploits the way our auditory system groups harmonically related tones and interprets pitch changes, making it difficult to pinpoint when the scale “resets.”\n\nRisset tones, named after French composer and scientist \n\nJean-Claude Risset (1938–2016), are often confused with Shepard tones because they are similar. However, while the Shepard tones are discrete, Risset showed that it is possible to create a similar illusion also work with continuous sounds. This type of auditory illusion is similar to Shepard tones, where a continuously ascending or descending pitch seems to rise or fall endlessly. This effect is achieved by overlapping sine waves that fade in and out at different frequencies, creating the impression of a never-ending scale. Risset tones demonstrate how our perception of pitch can be manipulated by carefully controlling the spectral content and amplitude envelopes of sounds.\n\nBoth Shepard and Risset tones have been used in sound design and music. Here is one of many examples:","type":"content","url":"/week4#shepard-and-risset-tones","position":57},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Missing Fundamental","lvl2":"Auditory illusions"},"type":"lvl3","url":"/week4#missing-fundamental","position":58},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Missing Fundamental","lvl2":"Auditory illusions"},"content":"Another powerful auditory illusion is called missing fundamental. When a complex tone lacks its fundamental frequency but contains its harmonics, listeners still perceive the pitch corresponding to the missing fundamental. This shows that pitch perception is based on the pattern of overtones, not just the lowest frequency present.\n\nFor example, if you play a sound containing frequencies at 200 Hz, 300 Hz, and 400 Hz (the 2nd, 3rd, and 4th harmonics of 100 Hz), but omit the 100 Hz fundamental, most listeners will still perceive the pitch as if the 100 Hz tone were present. The auditory system analyzes the spacing between the harmonics and infers the fundamental frequency, even when it is physically absent.\n\nThis phenomenon is important in music and audio technology. For instance, small speakers (like those in smartphones) often cannot reproduce very low frequencies, but listeners can still perceive the intended bass notes due to the presence of higher harmonics. The missing fundamental effect is also used in telephony and audio compression to create the illusion of full-range sound with limited frequency content.\n\nThe missing fundamental illusion demonstrates that pitch perception is a constructive process, relying on the brain’s ability to detect regularities and patterns in the frequency spectrum, rather than simply responding to the presence of specific frequencies.\n\nExplore the Missing Fundamental with Glicol\n\nUse \n\nGlicol to create a sound that contains only the 2nd, 3rd, and 4th harmonics of a fundamental frequency (for example, 200 Hz, 300 Hz, and 400 Hz), but omits the fundamental itself (100 Hz). What pitch do you perceive? Try changing the harmonics and observe how your perception of pitch changes. Can you still “hear” the missing fundamental?","type":"content","url":"/week4#missing-fundamental","position":59},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Binaural Beats","lvl2":"Auditory illusions"},"type":"lvl3","url":"/week4#binaural-beats","position":60},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl3":"Binaural Beats","lvl2":"Auditory illusions"},"content":"When two slightly different frequencies are played separately to each ear (using headphones), the listener perceives a rhythmic beating at the frequency difference. This illusion arises from the brain’s processing of phase differences between the ears.\n\n","type":"content","url":"/week4#binaural-beats","position":61},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Questions"},"type":"lvl2","url":"/week4#questions","position":62},{"hierarchy":{"lvl1":"Week 4: Psychoacoustics","lvl2":"Questions"},"content":"What is the difference between sound pressure level (SPL) and loudness, and why is this distinction important in psychoacoustics?\n\nHow does the human ear convert sound waves into electrical signals, and what are the main roles of the outer, middle, and inner ear in this process?\n\nExplain the concept of masking in psychoacoustics. Give an example of how masking can affect our perception of sound in everyday life or music production.\n\nWhat are Shepard tones and Risset tones, and how do they demonstrate the brain’s role in pitch perception?\n\nHow do interaural time difference (ITD) and interaural level difference (ILD) help us localize sound sources in space?","type":"content","url":"/week4#questions","position":63},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm"},"type":"lvl1","url":"/week5","position":0},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm"},"content":"","type":"content","url":"/week5","position":1},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Introduction"},"type":"lvl2","url":"/week5#introduction","position":2},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Introduction"},"content":"Music unfolds in time. Unlike pitch or timbre, which can be considered at a single point in time, rhythm and meter depend on duration, order, and recurrence. They provide the structures that allow us to predict, move, and coordinate.\n\nIn this chapter, we begin with broad concepts of time and rhythm before considering the frameworks of meter, pulse, and subdivisions. We then explore microrhythmic nuance and the phenomenon of groove as it pertains to rhythmic structures.\n\nThroughout, you will see that musical time is not only a matter of clock measurement. It is shaped by perception, culture, and performance practice. Rhythms and meters may be notated on the page, but their feel depends on how musicians and listeners interpret and embody them.","type":"content","url":"/week5#introduction","position":3},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Time"},"type":"lvl2","url":"/week5#time","position":4},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Time"},"content":"Time in music refers to the organization of sounds and silences in a temporal framework. It is the foundation upon which rhythm, beat, and meter are built.","type":"content","url":"/week5#time","position":5},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Onset Timing","lvl2":"Time"},"type":"lvl3","url":"/week5#onset-timing","position":6},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Onset Timing","lvl2":"Time"},"content":"When we speak of the “timing” of instruments in performance, it usually refers to the onset timing more specifically. When analysing the sound signals of rhythms (e.g., waveform analysis), the (physical) onset represents the moment a sound “begins,” usually defined as the moment the signal rises above a certain amplitude threshold (zero dB or noise floor baseline), or begins to rise with a slope above a certain pre-determined value. Various onset-detection algorithms exist, and each can differ slightly, though the principle remains the same.\n\nIn addition to physical onset, sound events have other relevant timing-related features such as:\n\nPerceptual Onset: the earliest a sound is perceived to begin\n\nPerceptual Attack or Center: the “perceived moment of rhythmic placement”\n\nEnergy Peak: the highest peak of energy or intensity\n\nAttack Time (or Duration): time from Physical Onset to Energy Peak\n\nTemporal Centroid: the temporal “balancing point” of the sound\n\nImage source: adapted from Nymoen et al. (2017)","type":"content","url":"/week5#onset-timing","position":7},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Perceptual Center and Attack Time","lvl2":"Time"},"type":"lvl3","url":"/week5#perceptual-center-and-attack-time","position":8},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Perceptual Center and Attack Time","lvl2":"Time"},"content":"When listening to rhythms, the “moment of occurrence” of a sound—the point we use to place it in time—often does not coincide with its physical onset. This percept is often called the perceptual center (P-center) or perceived attack time (PAT): a subjective reference point that typically falls somewhere after physical onset and before the peak of the sound’s energy. As a result, sequences that are physically synchronous or physically isochronous may be heard as misaligned or uneven.\n\nStudies show that certain sound parameters affect where P-centers are heard. Attack Duration or Time (time from onset to peak) is the strongest factor: faster attacks yield earlier P-centers; slower attacks yield later ones. Total duration (from onset to offset) exerts a weaker but similar influence—shorter sounds shift P-centers earlier; longer sounds shift them later.\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nTake two guitar chords for example, one played by slowly arpeggiating the strings (“swept”) and one played fast by stroking all the strings in quick succession (“swift”). Although both are produced by the same instrument, the swept stroke will tend to sound as if occurring slightly later relative to its physical onset compared to the swift stroke, where the P-center will tend to be much closer to its onset timing.\n\nImage source: adapted from Câmara et al. (2020)\n\nTaken altogether, this means aligning tracks by physical onsets does not always guarantee perceptual synchrony in a rhythmic sequence. When two identical sounds are physically synchronous (physical onsets evenly spaced) and their P-centers closely align with their physical onsets, they will tend to sound perceptually isochronous (example A). However, when different sounds with different attack duration profiles are physically isochronous but their P-centers do not closely align with their physical onsets, then they will tend to sound perceptually non-isochronous (example B). An analogous case can be imagined with sounds that occur simultaneously in a vertical fashion instead: if their P-centers are similar, they will tend to sound more synchronous, if P-centers are misaligned, more asynchronous.\n\nImage source: adapted from Villing, R. C. (2010). Note: IPI = Inter-P-Center-Interval, IOI] = Inter-Onset-Interval","type":"content","url":"/week5#perceptual-center-and-attack-time","position":9},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Rhythm"},"type":"lvl2","url":"/week5#rhythm","position":10},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Rhythm"},"content":"A rhythm can be considered a pattern of durations. When a series of sound events occurs, the intervals between them form recognizable shapes in time. In everyday use, we often call anything regular “rhythmic,” but in music the term usually refers more specifically to a pattern of durations formed by intervals between the perceived time points of notes.\n\nRhythms can be periodic (based on cycles, such as a march rhythm) or aperiodic (irregular, as in speech or improvisation). Both can be meaningful: a drum groove in 4/4 and a free jazz solo may each have rhythm, but in different senses.\n\nRhythms also have cultural meanings. A clave rhythm in Afro-Cuban music is more than a sequence of durations—it is a structural reference for musicians and dancers. Similarly, speech rhythms shape the flow of rap and spoken word performance.","type":"content","url":"/week5#rhythm","position":11},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Meter"},"type":"lvl2","url":"/week5#meter","position":12},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Meter"},"content":"Meter provides a hierarchical framework for the organization’s rhythm. It is often described as a nested system of pulses at multiple periodicities, from fast subdivisions to slower bar lengths:\n\nBeat (Pulse/Tactus): the fundamental beat to which we tend to synchronize/entrain to\n\nDivision levels: subdivisions of the beat, 8ths, 16ths, 32nds, and slower groupings.\n\nMultiple levels: slower groupings of the beat, half-notes, bar-lengths, etc.\n\nImage Source: \n\nWikipedia - Meter (music)","type":"content","url":"/week5#meter","position":13},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Pulse / Beat","lvl2":"Meter"},"type":"lvl3","url":"/week5#pulse-beat","position":14},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Pulse / Beat","lvl2":"Meter"},"content":"The beat (often also referred to as the tactus or pulse) is the basic time unit of a meter. A pulse can also describe an ongoing stream of beats (one can speak of 8th note pulses just as much as quarter note pulses). Regardless, the beat level of music is what we naturally tap our feet to, providing stability and predictability for both performers and listeners.\n\nResearch suggests that we comfortably perceive and move to beats when their IOI rate lies between about 500-700ms (120-86 BPM). Pulse is thus closely tied to our sensorimotor system—we tend to move along with it. Faster events are heard as subdivisions, and slower ones are grouped into larger spans.\n\nIn performance, the pulse may not always be physically marked by sound. Even when silent, it can be inferred as a virtual reference that organizes events. Musicians often “feel” the pulse even when they play around it.","type":"content","url":"/week5#pulse-beat","position":15},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Subdivision","lvl2":"Meter"},"type":"lvl3","url":"/week5#subdivision","position":16},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Subdivision","lvl2":"Meter"},"content":"Subdivisions divide each beat into smaller units. Perceptually, subdivisions tend to be heard when their IOI duration is between ca. 100 to 500 milliseconds. Subdivisions provide detail and density, and often act as stylistic markers (e.g., straight eighths in rock; sixteenths in funk; triplet-based 12/8 in blues).","type":"content","url":"/week5#subdivision","position":17},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Metric Accent","lvl2":"Meter"},"type":"lvl3","url":"/week5#metric-accent","position":18},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Metric Accent","lvl2":"Meter"},"content":"Traditionally, Western theory frames meter as a pattern of strong and weak accented beats (e.g., ONE-two-THREE-four in 4/4). Accents are thought to align where metrical levels overlap. But in practice, this is not always how music is felt or performed. Many groove-based styles emphasize off-beat positions. In many popular genres such as rock, soul, and R&B, the snare backbeats on 2 and 4 may feel more salient than beat 1. In reggae, the offbeats on the “ands” may dominate more than the downbeats. This is to show that metrical accents are not fixed but can be redefined by practice and cultural convention.","type":"content","url":"/week5#metric-accent","position":19},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Asymmetric Meter","lvl2":"Meter"},"type":"lvl3","url":"/week5#asymmetric-meter","position":20},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Asymmetric Meter","lvl2":"Meter"},"content":"Not all meters divide evenly into twos (binary) and threes (tertiary). Asymmetric meters (such as 5/8 or 7/8) feature beats of different durations, organized into uneven groupings of equal subdivisions, like 3+2 or 2+2+3. They are common in Balkan, Middle Eastern, and contemporary art music traditions.\n\nPerformers often feel such meters as cycles of unequal steps rather than a uniform grid. The asymmetry can produce a sense of lilt or forward drive distinct from even meters.\n\nIn addition, the inter-onset-interval (IOI), or time duration, between the main beats in a meter may either be isochronous (equal), or non-isochronous (unequal). In many popular groove-based styles, instruments tend to create a highly isochronous beat structure. In Scandinavian folk styles such as Telespringar, the main beats of the meter are not always equal in duration, but instead can vary (e.g, long-medium-short), creating a signature lilting feeling to the music.\n\nListen to an excerpt of a springar fiddle tune by Jon Vestafe (Audio Ex. 1 – the first beat is marked by a synthetic voice for three measures to help you entrain to the beat). Can you hear how the beats are irregular (non-isochronous)? Compare this to John Coltrane’s My Favorite Things (Audio Ex. 2), where the beats are highly regular.\n\nNow try tapping to both using the Maître Gnome online tapping platform:\n\nFirst, tap along to \n\nMy Favorite Things.\n\nThen, tap along to \n\nspringar.\n\nAfter tapping at least 3 full bars in each, follow the instructions at the bottom of the page (copy/paste the saved dump code into the right field in the provided Google form link). The group results will be discussed in class.\n\nReflect: Compared to just listening, how did it feel to try and tap to the beat? Was it easier to perceive the non-isochrony in the springar? Briefly describe your experience.","type":"content","url":"/week5#asymmetric-meter","position":21},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Audio Source Separation"},"type":"lvl2","url":"/week5#audio-source-separation","position":22},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Audio Source Separation"},"content":"Auditory scene analysis is the process by which the human auditory system organizes sound into perceptually meaningful elements. In computer audition, \n\naudio source separation is the process of extracting individual sound sources (e.g., a single flute) from a mixture of sounds (e.g., a recording of an entire concert band) utilizing a variety of computational techniques that consider various features in the extraction process (rhythm, pitch, harmony, notational score). It is a highly useful tool for analysis of rhythm and timing, making the detection of onsets’ microrhythm far easier today than a decade ago.\n\nImage Source: Manilow et al. (2020)\n\nA range of audio source separation tools is available, each with unique advantages and trade-offs:\n\nCommercial Online Tools (e.g., \n\nmoises.ai, \n\nlalal.ai)\n\nStrengths: Fast, cloud-based processing with intuitive, DAW-like interfaces. These services typically provide high-quality separation for common instruments in Western popular music, making them accessible to non-experts and suitable for most general use cases.\n\nLimitations: User control over separation parameters is limited, as algorithms are proprietary and not user-adjustable. Free versions often restrict export quality or the number of processed tracks, while full functionality usually requires a paid subscription.\n\nOpen Source / Code-Based Tools (e.g., \n\nSpleeter by Deezer, \n\nDemucs)\n\nStrengths: Highly customizable—users can tweak parameters, retrain models for specific instruments or genres, and integrate separation into research or custom workflows. These tools are free and adaptable for advanced or specialized needs.\n\nLimitations: Processing speed and quality depend on local hardware and configuration. They require technical skills (command-line or programming experience), and achieving optimal results may involve additional setup or parameter tuning.\n\nChoosing the right tool depends on your technical background, the complexity of your separation task, and whether you prioritize ease of use or customization.\n\nSign up for a free account at \n\nmoises.ai. Upload a track of your choice (preferably something groove-based, or with at least drums and a few other rhythm instruments). Listen to the individual stems produced by the source separation.\n\nDo you notice any distortions, glitches, or loss of frequency information in the separated stems?\n\nReflect on how useful such a separation would be for transcription or signal analysis purposes (both rhythm and pitch).\n\nConsider the limitations of current source separation algorithms and how they might affect your analysis.\n\nReflect: How did the quality of the separated stems compare to your expectations? In what ways could these tools assist or hinder your musical analysis?","type":"content","url":"/week5#audio-source-separation","position":23},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Microrhythm"},"type":"lvl2","url":"/week5#microrhythm","position":24},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Microrhythm"},"content":"Microrhythm refers to fine-scale timing and shaping of events around our subjective reference structures (beat, subdivision). Unlike beat and subdivision-level rhythms, microrhythm concerns variations in timing at a scale below 100 ms.\n\nIn terms of timing, one can speak generally of two related, yet different forms of microrhythm: asynchrony and non-isochrony. Asynchrony refers to the way musicians can play in either a more synchronized (“on-beat”) or asynchronous (early/“pushed” or late/“laid-back”) manner relative to one another. Non-isochrony relates to how they might skew the durational ratio of metrical subdivision levels (e.g., 8th or 16th notes) to varying degrees from isochronous (‘straight’) to non-isochronous (‘swung’).\n\nWhereas asynchrony and non-isochrony both denote departure in timing from synchrony and isochrony in rhythmic contexts (Figure 1, example 1), they do so along different dimensions: asynchrony refers to vertical non-alignment between simultaneously sounding events, whereas non-isochrony denotes non-equal horizontal timing relationships between successive events.\n\nImage Source:* \n\nCâmara et al. (2025), Figure 1*\n\nAsynchrony tends to be measured as the absolute positive (late) or negative (early) value of signal onset displacement (d) relative to a fixed timing reference grid (often isochronous) in milliseconds (example 2a and 2b below). Swing is commonly quantified in two ways: either as simply the absolute onset displacement (d) from a grid of an even-numbered/off-beat subdivision event (in milliseconds), or as the ‘swing ratio’ (R) between the relative duration of an odd-numbered subdivision event (m + d) and a subsequent even subdivision (m − d), where m is the mean inter-onset-interval (IOI) rate of the subdivision level (example 3). Swing ratios are thus calculated by the formula: 𝑅 = 𝑚 + 𝑑 / 𝑚 − 𝑑, and may be expressed as a ratio (e.g., 1.5:1 [‘medium swing’]) or more simply as a decimal (e.g., 1.5).\n\nOverall, microrhythm can encompass both expressive moment-to-moment nuance and systematic patterns that recur across a piece or style. In many musical performance traditions, those systematic patterns often constitute the norm themselves, rather than representing “deviations” from ideal or perfect metronomic/isochronous timing.","type":"content","url":"/week5#microrhythm","position":25},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Beat Delay / Anticipation","lvl2":"Microrhythm"},"type":"lvl3","url":"/week5#beat-delay-anticipation","position":26},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Beat Delay / Anticipation","lvl2":"Microrhythm"},"content":"Musicians may sometimes play behind or ahead of the beat in a systematic fashion—whether consciously or not. In rock and soul styles, snare drum strokes on beats 2 and 4 of a 4/4 meter are often claimed to be delayed, and in funk, downbeats are said to regularly anticipate “One”. It’s important to remember that what counts as “late” or “early” depends on a reference in any given context—whether kick, snare, or overall grid.\n\nDifferent degrees of beat delay/anticipation are said to convey different types of ‘feels’. For example, delayed (late) back-beats in groove-styles are often described as “Laid-back”, and elicit a more “relaxed” feel when only slightly delayed, but a “heavy” or “dragging” feel if delayed too much. Anticipated (early) beats, on the other hand, are regularly described as “On-top/Pushed”, which at lower magnitudes might sound “snappy” and “driving”, but too much may sound “nervous” or “rushing”. Many scholars and musicians have attempted to develop their own heuristics as to what is the right degree of asynchrony in a given style. For example, MIDI-based electronic music producer Michael Stewart’s (in Prögler 1995) “feel spectrum,” which illustrates his prescriptions for an assortment of rhythmic feels at 130 beats per minute, for use with synthesizers and drum machines:\n\nImage Source: adapted from Stewart (in Prögler 1995)\n\nWe can look at an example where Bootsy Collins (bassist from James Brown Band and Parliament-Funkadelic) is playing along to a highly isochronous drum machine groove, which serves as a convenient stable time reference to measure bass onset asynchronies against (listen to Audio Ex. 3, where the 2-bar bass and drum riff is looped 3 times). An onset timing analysis reveals that Bootsy systematically plays slightly behind the beat at around +15 ms on average (dashed white line in plot)—what Stewart might call “in the pocket” playing), and furthermore, he does so in the same systematic way in both bars (red and blue lines).\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nListen to an excerpt of D’Angelo’s Left and Right (Audio Ex. 4). Can you hear any systematic delay or anticipation in the comp section? Does it sound “snappy”/“driving”, or “dragging”/“heavy”? Now listen to an excerpt from James Brown’s Get Up Offa That Thang (Audio Ex. 5). Do you hear any asynchronies here?\n\nNow try tapping to both tracks using the Maître Gnome online tapping platform:\n\nFirst, tap along to \n\nGet Up Offa That Thang.\n\nThen, tap along to \n\nLeft and Right.\n\nAfter tapping at least 3 full bars for each, follow the instructions at the bottom of the page (copy/paste the saved dump code into the right field in the provided Google form link). The group results will be discussed in class.\n\nReflect: Compared to just listening, how did it feel to tap along to the beat? Did it change how the groove felt? Did you perceive the asynchronies more or less the same?","type":"content","url":"/week5#beat-delay-anticipation","position":27},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Swing","lvl2":"Microrhythm"},"type":"lvl3","url":"/week5#swing","position":28},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Swing","lvl2":"Microrhythm"},"content":"The duration of subdivision notes need not be isochronous (equal), such as in a straight 8ths hi-hat pattern in rock. Swing - a form of \"non-isochrony (non-equal durations) describes systematic long–short patterns at the subdivision level.\n\nIn jazz, musicians typically do not swing at a perfectly mechanical ‘triplet swing’ ratio of 2:1, but rather swing ratios in performance can vary from closer to 1:1 (straight) all the way up to 3:1 (dotted swing) and beyond, depending on tempo, sub-genre, and personal style, amongst other factors. In funk/hip-hop, on the other hand, sixteenth notes tend to be swung at more subtle degrees (e.g., 1.2:1) unless played in explicit funk-shuffle styles (closer to 2:1, triplet).\n\nDifferent swing ratios are thought to impart different degrees of “motional energy” to rhythms. Rhythms with lower swing ratios (closer to 1:1) are described as more “continuous” “driving”, or “propulsive”, whereas higher swing ratios (up to 2:1 and beyond) tend to impart “bounce” to “choppiness”, often accentuating the downbeats/on-beats of the meter.\n\nLooking at Bootsy Collins’ performance again, instead of simply measuring how early or late the timing of bass onsets is on every note, we can calculate the swing ratio between all pairs of odd- and even-numbered subdivisions. We find that the bass swings rather significantly (mean: 1.4) over the course of the two bars, sometimes swinging more and sometimes less—arguably supplying the groove with a certain degree of “bounce”. Again, the swing pattern is applied systematically across both bars. (Audio Ex. 6 gives the bass pattern isolated, without drum machine, looped 3 times).\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nListen to two manipulated versions of James Brown’s Get Up (I Feel Like Being a Sex Machine), where the vocals have been source separated out:\n\nIn Audio Ex. 7, the timing of all instruments has been modified to be perfectly aligned with a straight 16th grid (1:1 - isochronous).\n\nIn Audio Ex. 8:, the timing is aligned with a triplet sixteenth grid (2:1 - quantized triplet swing).\n\nReflect: Does one version feel more “choppy” and the other more “driving/continuous”? Why might this be the case?\n\nFinally, listen to the original version (Audio Ex. 9, no timing manipulation).\n\nReflect: Can you tell which instruments are swinging, and if so, are they swinging to the same degree?","type":"content","url":"/week5#swing","position":29},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Perceptual Thresholds of Microrhythm","lvl2":"Microrhythm"},"type":"lvl3","url":"/week5#perceptual-thresholds-of-microrhythm","position":30},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Perceptual Thresholds of Microrhythm","lvl2":"Microrhythm"},"content":"Recall from last week that the just-noticeable difference (JND) thresholds of asynchrony and non-isochrony—the smallest change in onset timing that a listener can reliably detect depend on various factors. In general, however, asynchrony JNDs tend to depend strongly on instrument attacks and texture. With overlapping, sustained or blended tones (e.g., piano, strings, winds, voices), listeners typically need onsets to differ by roughly 20–50 ms before they hear them as asynchronous. In groove contexts that often involve sharper, more transient sounds (e.g., drums, picked bass), smaller onset displacements can be detected, around 10–20 ms for trained musicians.\n\nAs for swing JND thresholds, non-musicians tend to need around 30 ms (ca. 1.4) to hear rhythms as irregular (not straight/isochronous), while trained musicians can detect lower degrees of swing at around 10 ms displacement (about 1.1 - 1.2).\n\nOther contextual factors can affect the extent to which we hear microrhythmic nuances. In general, the faster the tempo, the greater the onset displacement magnitude, and the more displaced events (higher note density), as well as musical training, all improve sensitivity.\n\nFor more on JNDs of microrhythm in music, see \n\nCâmara et al. (2025)\n\nDownload and install \n\nSonic Visualiser, a free tool for visualizing and analyzing audio files. Also download and install the \n\nVamp Plugin Pack.\n\nOpen sound files of separate instruments from the same track (such as those you source-separated in Exercise 2) and experiment with marking onsets onto the waveforms:\n\nLoad one instrument audio file into Sonic Visualiser first (File → Open), then load the other instrument audio files using File → Import More Audio.\n\nSelect an instrument layer pane, and then run, for example, the BBC onset plugin:\n\nTransform → Analysis by Maker → BBC → Rhythm: Onset\n\nTweak sensitivity (especially threshold), then run (press OK).\n\nRe-run with different threshold/sensitivity if it over/under-detects onsets; keep only the best layer (right-click layer name → Remove Layer) to avoid clutter.\n\nDo the same for other instrument stems.\n\n(Optional) To measure timing differences elsewhere, select a layer and File → Export Annotation Layer... to TSV/CSV. You can then compute inter-onset intervals or between-layer offsets in a spreadsheet.\n\nReflect: Do you notice asynchronies between different instrument layers (e.g., bass to drums), or systematic patterns of swing in the subdivisions of a given instrument? Are they steady or random?","type":"content","url":"/week5#perceptual-thresholds-of-microrhythm","position":31},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Entrainment"},"type":"lvl2","url":"/week5#entrainment","position":32},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Entrainment"},"content":"Entrainment refers to the synchronization of an individual’s movements or internal rhythms with an external rhythm, such as a musical beat.\n\nMeter can be thought of as a musically specific form of entrainment: we synchronize our attention (and often our movements) to periodicities in the sound. This synchronization sets up periodic peaks of attention—expectancy moments where events are most salient—and these peaks are arranged in hierarchies (subdivision, tactus/beat, bar, larger cycles). In this view, meter is less a printed grid and more a behavior of attention that locks to temporal invariants and shapes how we group notes and hear accents.\n\nDynamic attending theory attempts to explain where metrical accent “comes from” from a cognitive perspective. Not all accents are metrical: phenomenal accents (loudness, timbre, leaps) and structural accents (harmonic/melodic goals) can occur anywhere. Metrical accent, however, is said to arise when a rhythmic event lands inside an attentional peak—that is, it is marked by consciousness, not just made louder. Thus, a rock backbeat (dynamic emphasis on 2 and 4) need not move the metrical accent away from 1 and 3 for enculturated listeners; the listener’s entrainment maintains the meter while reading the backbeat as an idiomatic reinforcement.\n\nThink of metrical accent strength as the height and narrowness of an attention peak. Stronger regularity and clearer cueing tighten the window for “on-time,” while looser surfaces widen it without losing the beat.","type":"content","url":"/week5#entrainment","position":33},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Groove"},"type":"lvl2","url":"/week5#groove","position":34},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Groove"},"content":"While groove can be understood in many ways, here we can focus on groove as “pattern” and as “performance approach”, with focus on aspects of time and rhythm.\n\nAs a pattern, a groove can be simply defined as a persistently repeated rhythm—often spanning one or two bars—whose events establish a clear beat and a characteristic subdivision layer. The beat may be externalized (e.g., kick/snare backbeat in 4/4) or implied by cyclic “isoperiodic” figures that recur in a predictable fashion. Style identity often hinges on the basic unit (the smallest repeating chunk) and its density referent (the shortest practically used subdivision).\n\nAs performance, a groove is the coordinated realization of that basic unit across parts (e.g., drums, bass, guitar/keys, vocals). Players distribute roles: some layers reinforce the beat (e.g., kicks on beats 1 and 3, snare on 2 and 4), while others supply tension by playing on off-beat subdivisions (e.g., bass accentuating the ‘2-and’). Often, grooves balance stability (easily entrainable pulse) with off-beat rhythmic devices that supply interest and complexity, such as syncopation and cross-/counter-rhythm (see below).","type":"content","url":"/week5#groove","position":35},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Syncopation","lvl2":"Groove"},"type":"lvl3","url":"/week5#syncopation","position":36},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Syncopation","lvl2":"Groove"},"content":"Syncopation is often described as a local contradiction of a metrical expectation scheme: a note occurring on a weak-position accent, a tie across a strong beat, or the omission of a strong beat followed by sound on a weak one. In 4/4, common cases include stressing the “and” of 2 or 4, tying into 1, or placing a salient event on 3-and while 3 itself is silent. Because the beat is usually firmly established in groove styles, such contradictions tend to add tension without destabilizing the meter.\n\nSyncopation can arguably also reinforce meter—when an expected strong beat is left unarticulated, listeners feel the “missing” beat more vividly through anticipation; the follow-up weak-beat event then locks back to the underlying cycle. Backbeat-heavy styles illustrate this: dynamic accents on 2 and 4 do not move the downbeat to 2; instead, they clarify a beat-level ‘hocketing’ against the strong–weak–strong–weak scheme.\n\nOnce again, using Bootsy Collins’ bass performance as an example (bar 1 of the two-bar transcribed pattern below), looking at a simpler riff this time (Audio Ex. 10), we can see that he plays almost exclusively on the off-beat positions of the meter, hitting mainly the “-a” and “'-and” notes (think: 1-e-and-a, 2-e-and-a, etc.) with the exception of the “One”—a typical trait in funk music, where the downbeat is clearly marked in every repetition of the basic groove unit. Against the drums—which, conversely, are highly anchored to the main beats, featuring no syncopation on the 8th and 16th note subdivision level—the bass provides ample tension and rhythmic interest, without challenging the meter.\n\nImage Source: Guilherme Schmidt Camara 2025 ©","type":"content","url":"/week5#syncopation","position":37},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Counter-/Cross-Rhythm","lvl2":"Groove"},"type":"lvl3","url":"/week5#counter-cross-rhythm","position":38},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl3":"Counter-/Cross-Rhythm","lvl2":"Groove"},"content":"When syncopated events recur systematically within the basic unit, they may form larger-scale, patterned off-beat groupings that suggest alternative periodicities to the main beat.\n\nCross-rhythm (often also called “polyrhythm”) occurs when these groupings display a systematic overlap of rhythmic streams whose periodicities (i.e., ‘metrical levels’) are noninteger multiples. Typical examples in 4/4 meter would be when two evenly spaced events are superimposed over three beats (2:3 cross-rhythms) or four events over three or six beats (4:3 or 4:6 cross-rhythms). The regular pattern of overlapping accents regularly contradicts the beats of the prevailing meter, challenging it and engendering greater metrical ambiguity—to the extent that one might hear the pulse as either binary or triplet.\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nIn most groove styles, however, such overlapping rhythms are usually bounded—they tend to span less than a bar, and usually coincide again with beat-confirming positions of the meter before repeating. As such, they tend to generate lesser degrees of metrical ambiguity without fundamentally challenging the main pulse. Typical counter-rhythmic groupings in 4/4 are 3+3+2 over eight eighth notes, or 3+3+3+3+2+2 over sixteen sixteenths; listeners may hear secondary “pulses” riding on top of the main meter.\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nOnce again, returning to the more complex Bootsy Collins riff we saw earlier, this time, what appears on the surface to be a dense pattern with many 16th notes could be construed as roughly accenting a typical counter-rhythm often found in groove-based music: a 3-3-2 (repeated twice) (see below for transcription, and listen to Audio Ex. 11, where the counter-rhythm is overlaid by a clave). Although this counter-rhythm produces constant 2:3 cross-rhythms, they never fully create the sense of a triplet pulse above the main binary 4/4 beat, as the bass groove always returns to accent the main meter beats before this happens.\n\nImage Source: Guilherme Schmidt Camara 2025 ©\n\nListen to the chorus of Jackie Wilson’s Your Love Keeps Lifting Me (Higher and Higher) (Audio Ex. 12, see transcription below).\n\nIdentify which notes in the melody or accompaniment are syncopated (i.e., placed on weak beats or off-beats).\n\nListen for instruments that accentuate cross- or counter-rhythmic figures. Can you detect any grouping patterns (such as 3+3+2)?\n\nNote where and how the instruments reassert the main beats of the meter.\n\nReflect: How do syncopation and cross-rhythms contribute to the groove? Does the reassertion of the main beats help maintain a sense of meter despite rhythmic complexity?","type":"content","url":"/week5#counter-cross-rhythm","position":39},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Tempo"},"type":"lvl2","url":"/week5#tempo","position":40},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Tempo"},"content":"Tempo is the rate of a given pulse, usually measured in BPM as the pulse rate of the fundamental metrical level (e.g. quarter notes in 4/4). Without a reference, people tend to tap within a spontaneous tempo range (around 100–120 BPM). This overlaps with walking pace and many dance genres. It suggests that musical tempo is linked to bodily rhythms. We will get back to that later in this course.\n\nTry determining the tempo of a song by tapping along to the beat using an online \n\nTap-BPM tool.\n\nSelect a song and tap along to the beat; note the BPM value shown.\n\nTry this with two songs at different tempi (one slow, one fast).\n\nListen for how the feel of swing or straight subdivisions changes with tempo.\n\nReflect: Did you notice that swing is easier to perceive at slower tempos? How does the tempo affect your perception of rhythmic feel?","type":"content","url":"/week5#tempo","position":41},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Questions"},"type":"lvl2","url":"/week5#questions","position":42},{"hierarchy":{"lvl1":"Week 5: Time and Rhythm","lvl2":"Questions"},"content":"Define rhythm and meter, each in one sentence. Give a musical example where phenomenal accents contradict the notated metrical accents—and explain why the meter still “holds.”\n\nWhy do researchers measure onset timing rather than P-centers when analyzing performances?\n\nWhy do many listeners move most comfortably to beats with IOIs around 500–700 ms (≈120–86 BPM)? What typically happens to beat perception above and below this range?\n\nA snare with a very sharp attack and a bass note with a slower attack strike together at the same physical onset time. Why might they not sound synchronous? Name two sound features that can shift perceived placement.\n\nHow do straight and swung subdivisions differ? Describe with words and numbers.\n\nExplain laid-back and pushed timing. Why does the choice of reference layer matter?\n\nWhat is entertainment, and how does it explain that we can maintain a sense of a steady pulse despite microrhythmic variations?\n\nWhat makes a groove “work”? Explain how a stable beat and subdivision interact with off-beat devices (syncopation, counter-/cross-rhythm, swing) and performance choices to create feel.\n\nKristian Nymoen, Anne Danielsen & Justin London (2017). Validating Attack Phase Descriptors Obtained by the Timbre Toolbox and MIRtoolbox, In Tapio Lokki; Jukka Pätynen & Vesa Välimäki (ed.), Proceedings of the 14th Sound and Music Computing Conference 2017. Aalto University. ISBN 978-952-60-3729-5, s 214 - 219. \n\nhttps://​smc2017​.aalto​.fi​/media​/materials​/proceedings​/SMC17​_p214​.pdf\n\nCâmara, G. S., Nymoen K., Lartillot O., and Danielsen, A. (2020). Effects of instructed timing on electric guitar and bass sound in groove performance. Journal of the Acoustical Society of America 147(2), 1028–1041. \n\nhttps://​doi​.org​/10​.1121​/10​.0000685\n\nVilling, R. C. (2010). Hearing the moment: Measures and models of the perceptual centre. Ph.D. dissertation, National University of Ireland. \n\nhttps://​doras​.dcu​.ie​/16254/\n\nManilow, E., Seetharman, P., & Salamon, J. (2020, October). Open source tools & data for music source separation. \n\nhttps://​source​-separation​.github​.io​/tutorial\n\nCâmara, G. S., Spiech, C., Solli, S., Bang, B., Rogulina, O., Laeng, B., & Danielsen, A. (2025, September 1). Just Noticeable Difference Thresholds of Asynchrony and Non-isochrony in a Multi-Instrumental Groove-based Context. \n\nCâmara et al. (2025)\n\nPrögler, J. A. (1995). Searching for swing: Participatory discrepancies in the jazz rhythm section. Ethnomusicology, 39, 21-54. \n\nProgler (1995)\n\nDancing in the Street, episode 9, “Make It Funky” (BBC/WBGH, 1996)\n\nCâmara, G. S., Spiech, C., Solli, S., Bang, B., Rogulina, O., Laeng, B., & Danielsen, A. (2025, September 1). Just Noticeable Difference Thresholds of Asynchrony and Non-isochrony in a Multi-Instrumental Groove-based Context. \n\nCâmara et al. (2025)","type":"content","url":"/week5#questions","position":43},{"hierarchy":{"lvl1":"Week 6: Harmony and melody"},"type":"lvl1","url":"/week6","position":0},{"hierarchy":{"lvl1":"Week 6: Harmony and melody"},"content":"Last week, we explored the importance of timing and rhythm in sound and music. This week, our focus shifts to how frequencies interact both “horizontally” and “vertically.” Horizontal organization of frequencies gives rise to melodies; sequences of pitches unfolding over time, often shaped by rhythmic patterns. Vertical organization, on the other hand, leads to the formation of intervals, harmonies, and textures, as multiple pitches sound together or overlap. Understanding these dimensions is essential for analyzing how music conveys structure, emotion, and complexity. First, however, we need to define the concept of “tone”.","type":"content","url":"/week6","position":1},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Tones"},"type":"lvl2","url":"/week6#tones","position":2},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Tones"},"content":"In music psychology, a tone is understood as a sound with a specific frequency and timbral quality that the auditory system interprets as having a definite pitch. Our perception of tones is shaped by both the physical properties of the sound wave (such as frequency, amplitude, and harmonic content) and the way our brains process these signals. Tones are the building blocks of musical perception, allowing us to distinguish melodies, harmonies, and timbres.\n\nFrom a technological perspective, tones can be generated, analyzed, and manipulated using digital tools. Synthesizers create tones by combining waveforms, while audio analysis software can extract pitch and timbre features from recordings. Technologies such as pitch detection algorithms and spectral analysis are essential for applications in music information retrieval, automatic transcription, and digital instrument design.\n\nimport numpy as np\n\nsr = 22050  # or whatever sample rate you are using\nduration = 2.0  # seconds, or whatever duration you want\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\n\n# Define a fundamental frequency\nf0 = 440  # A4, 440 Hz\n\n# Pure tone (sine wave)\npure = np.sin(2 * np.pi * f0 * t)\n\n# Harmonics (sum of first 5 harmonics)\nharmonics = sum([np.sin(2 * np.pi * f0 * (n+1) * t) / (n+1) for n in range(5)])\n\n# Complex tone (sum of odd harmonics, like a square wave)\ncomplex_tone = sum([np.sin(2 * np.pi * f0 * (2*n+1) * t) / (2*n+1) for n in range(5)])\n\n# Set new envelope times (in seconds)\nattack_time = 0.01   # very rapid attack\ndecay_time = 0.02    # very rapid decay\nsustain_time = 0.9   # much longer sustain\nrelease_time = 0.2   # longer release\n\n# Convert times to sample lengths\nattack_len = int(attack_time * sr)\ndecay_len = int(decay_time * sr)\nsustain_len = int(sustain_time * sr)\nrelease_len = int(release_time * sr)\n\n# Create envelope with decay going down to 0.2\nattack = np.linspace(0, 1, attack_len, endpoint=False)                    # Attack\ndecay = np.linspace(1, 0.2, decay_len, endpoint=False)                    # Decay (to 0.2)\nsustain = np.full(sustain_len, 0.2)                                       # Sustain\nrelease = np.linspace(0.2, 0, release_len, endpoint=True)                 # Release\n\nenvelope = np.concatenate([attack, decay, sustain, release])\n\n# Ensure envelope matches signal length\nif len(envelope) < len(t):\n    envelope = np.concatenate([envelope, np.zeros(len(t) - len(envelope))])\nelse:\n    envelope = envelope[:len(t)]\n\n# Apply envelope to signals (assuming pure, harmonics, complex_tone are numpy arrays)\npure_env = pure * envelope\nharmonics_env = harmonics * envelope\ncomplex_tone_env = complex_tone * envelope\nsignals_env = [pure_env, harmonics_env, complex_tone_env]\n\nTones are not the same as notes: while a note refers to a symbolic representation in musical notation (with defined pitch, duration, and sometimes dynamics), a tone refers to the auditory experience itself.\n\nNote\n\nIn the following, we will use an \n\nanalysis-by-synthesis technique to understand more about the different concepts. This is a method used in sound and music research to understand auditory perception by recreating sounds and analyzing their properties. This approach is widely used in areas like speech synthesis, sound design, and music analysis.\n\nimport music21\n\nimport matplotlib.pyplot as plt\n\n# Plot the complex tone waveform\nfig, axs = plt.subplots(1, 2, figsize=(12, 3), gridspec_kw={'width_ratios': [2, 1]})\n\n# Waveform plot\naxs[0].plot(t, complex_tone_env)\naxs[0].set_title('Complex Tone (with Envelope)')\naxs[0].set_xlabel('Time (s)')\naxs[0].set_ylabel('Amplitude')\n\n# Create a single musical note (e.g., A4)\nnote = music21.note.Note('A4')\nnote.quarterLength = 1.0\nstream = music21.stream.Stream([note])\n\n# Render the musical note in the second subplot (as an image)\n# Save the note as an image and display it\nimg_path = stream.write('lily.png')\nimg = plt.imread(img_path)\naxs[1].imshow(img)\naxs[1].axis('off')\naxs[1].set_title('Musical Note (A4)')\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/week6#tones","position":3},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Pitch"},"type":"lvl2","url":"/week6#pitch","position":4},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Pitch"},"content":"Pitch is the psycho-physiological correlate of frequency that lets us hear one sound as higher or lower than another. It is closely related to frequency---the rate at which a waveform repeats---but the relationship is not one-to-one. As you may recall from the Acoustics chapter, most instrument tones are not pure sine waves. Musical, ‘pitched’ instruments generally produce a fundamental frequency and many additional frequencies (overtones). This is because pitched musical instruments are often based on an acoustic resonator, which oscillates at numerous frequencies simultaneously, mostly limited to integer multiples, or harmonics, of the lowest (fundamental) frequency, and such multiples form a harmonic series. However, the individual partials of a complex sound are typically not perceived as separate; our perceptual system fuses them together, leading us to experience a unitary sound\n\nThe pitch of harmonic tones generally corresponds to the fundamental frequency (f₀). However, the brain can infer a fundamental frequency (and thus perceive pitch) from complex tones even when a fundamental component is absent. This is called virtual pitch or the missing fundamental and it is related to the phenomenon whereby one’s brain extracts tones from everyday signals, even if parts of the signal are masked by other sounds\n\nBelow is a sequence of simple (sine) tones distinct at different frequencies (\n\n[Audio Ex. 1]{.underline}).\n\nIf we add some partials (multiples) below each tone (\n\nAudio example 2), can you start to hear a familiar melody?\n\nIf we then rearrange these partials slightly (\n\n[Audio Example 3]{.underline}), we can induce an even stronger sense of virtual pitch---various missing fundamental frequencies which are not physically present in the tones themselves:\n\nImage source: Toiviainen, P. (2015). Lecture materials for Music Perception. University of Jyväskylä.","type":"content","url":"/week6#pitch","position":5},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Pitch class","lvl2":"Pitch"},"type":"lvl3","url":"/week6#pitch-class","position":6},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Pitch class","lvl2":"Pitch"},"content":"In music, a pitch class (or “chroma”) is a set of pitches that are a whole number of octaves apart, e.g., the pitch class C consists of the Cs in all octaves. Humans perceive the notes in a tonal scale as repeating once per octave. This provides the basis for producing & perceiving melodic patterns based on relative pitch relationships---that is, relative to a pitch class. Absolute pitch ability, on the other hand, is the ability to recognize (or reproduce) specific pitches without the help of a reference pitch and pitch class.\n\nImage Source: Trainor, Laurel & Unrau, A.J.. (2012). Development of pitch and music perception. Springer Handbook of Auditory Research: Human Auditory Development. 223-254.","type":"content","url":"/week6#pitch-class","position":7},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Tonality and harmonic expectation","lvl2":"Pitch"},"type":"lvl3","url":"/week6#tonality-and-harmonic-expectation","position":8},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Tonality and harmonic expectation","lvl2":"Pitch"},"content":"Tonality refers to the tendency for tones to resolve to a fundamental tonic note---a hierarchy of tones centered on a tonic (scale). It is pne of the main conceptual categories in Western music & musical thought, and corresponds to key signatures in musical notation (C major, C minor). Some tones feel stable in a melodic-harmonic sequence or scale, others seem to want to resolve. Knowledge of tonality does not require formal training in music theory---it is learned implicitly, by virtue of exposure to the music of one’s culture. This learning-by-exposure causes us to store knowledge about key as a cognitive schema. Tonality is also supported by acoustics, in the sense that the most crucial notes in a scale (3rd and 5th) tend to share partials with the tonic.\n\n","type":"content","url":"/week6#tonality-and-harmonic-expectation","position":9},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Harmony"},"type":"lvl2","url":"/week6#harmony","position":10},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Harmony"},"content":"Harmony involves the combination of discernible tones to create intervals and chords. It is the simultaneous sounding of different pitches, which can evoke a wide range of emotional responses. Harmony plays a crucial role in the emotional tone and complexity of music. Harmony encompasses the interaction of pitches through intervals and chords, shaped by timbre and texture.","type":"content","url":"/week6#harmony","position":11},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Intervals","lvl2":"Harmony"},"type":"lvl3","url":"/week6#intervals","position":12},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Intervals","lvl2":"Harmony"},"content":"An interval is the distance between two pitches, measured in steps or frequency ratios. Intervals are the building blocks of harmony, as they define the relationships between notes played together or in succession. The human brain is sensitive to the relationships between pitches, perceiving certain combinations as consonant (pleasant or stable) and others as dissonant (tense or unstable). These perceptual responses are influenced by cultural exposure, musical training, and innate auditory processing mechanisms.\n\nTypes of Intervals: Intervals are named by counting the number of letter names from the lower to the higher note (e.g., C to E is a third). They can be major, minor, perfect, augmented, or diminished.\n\nConsonance and Dissonance: Some intervals, like octaves and perfect fifths, are perceived as consonant, while others, like minor seconds or tritones, are more dissonant.\n\nRole in Harmony: Intervals form the basis for chords and harmonic progressions. The combination of intervals within a chord determines its character and function.","type":"content","url":"/week6#intervals","position":13},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Chords","lvl2":"Harmony"},"type":"lvl3","url":"/week6#chords","position":14},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Chords","lvl2":"Harmony"},"content":"A chord is a group of three or more notes played simultaneously. Chords are the foundation of Western harmony and are used to create progressions that define the structure and mood of a piece.\n\nTriads: The most basic chords, consisting of three notes (root, third, fifth). Types include major, minor, diminished, and augmented triads.\n\nSeventh Chords and Extensions: Adding more notes (such as sevenths, ninths, elevenths, and thirteenths) creates richer harmonies.\n\nChord Progressions: Sequences of chords that create movement and tension-resolution patterns in music (e.g., I–IV–V–I in classical music, or ii–V–I in jazz).\n\nFunctional Harmony: Chords have roles (tonic, dominant, subdominant) that guide the listener’s expectations.","type":"content","url":"/week6#chords","position":15},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Timbre","lvl2":"Harmony"},"type":"lvl3","url":"/week6#timbre","position":16},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Timbre","lvl2":"Harmony"},"content":"Recall that \n\ntimbre, often called “sound color,” is the quality of a sound that distinguishes different instruments or voices, even when they produce the same pitch and loudness.\n\n“Sound Color”: The unique quality that makes a violin sound different from a flute, even if both play the same note at the same loudness.\n\nSpectral Content: Timbre is shaped by the harmonic content (overtones) and the way energy is distributed across frequencies.\n\nTemporal Alignment: The timing of sound waves, including attack, decay, sustain, and release, contributes to timbre perception.\n\nJust Noticeable Differences (JND): The smallest change in a sound property (such as frequency, amplitude, or spectral content) that can be perceived.","type":"content","url":"/week6#timbre","position":17},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Texture","lvl2":"Harmony"},"type":"lvl3","url":"/week6#texture","position":18},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Texture","lvl2":"Harmony"},"content":"Texture describes how multiple layers of sound interact in a musical composition. It ranges from monophonic (a single melody) to polyphonic (multiple independent melodies).\n\nMonophonic Texture: A single melodic line without accompaniment (e.g., solo singing).\n\nHomophonic Texture: A main melody supported by chords or accompaniment (e.g., singer with guitar).\n\nPolyphonic Texture: Two or more independent melodies played simultaneously (e.g., fugues, counterpoint).\n\nHeterophonic Texture: Multiple performers play variations of the same melody at the same time.\n\nCombination of Timbres: The blending of different sound qualities to create a rich texture.\n\n","type":"content","url":"/week6#texture","position":19},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Melody"},"type":"lvl2","url":"/week6#melody","position":20},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Melody"},"content":"Melody is the sequence of musical notes that are perceived as a single entity. It is often the most recognizable and memorable aspect of a musical piece. Melody plays a crucial role in emotional engagement and memory recall. Tools like MIDI editors and pitch detection algorithms are used to analyze and manipulate melodies.\n\nimport music21\nmusic21.environment.UserSettings()['musescoreDirectPNGPath'] = '/usr/bin/mscore3'\n\n# Create a simple melody: C D E F G F E D C\nmelody_notes = ['C4', 'D4', 'E4', 'F4', 'G4', 'F4', 'E4', 'D4', 'C4']\nmelody = music21.stream.Stream()\nfor n in melody_notes:\n    melody.append(music21.note.Note(n, quarterLength=0.5))\n\n# Show the musical score (this will render in Jupyter if MuseScore or similar is installed)\nmelody.show()\n\nFrom a psychological perspective, melody is the perception of a coherent sequence of tones that form a recognizable musical line. Melodies are central to musical memory and emotional response, as the brain tracks pitch contours, intervals, and rhythmic patterns to identify and recall tunes. Research in music cognition explores how listeners segment, remember, and anticipate melodic sequences.\n\nTechnological advances have enabled detailed analysis and manipulation of melody. Pitch tracking algorithms extract melodic lines from audio, while MIDI editors and music notation software allow for precise editing and visualization. In music generation and AI composition, models learn melodic patterns from large datasets to create new, stylistically consistent melodies. Melody extraction and similarity algorithms are also used in music search and recommendation systems.","type":"content","url":"/week6#melody","position":21},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Auditory stream segregation","lvl2":"Melody"},"type":"lvl3","url":"/week6#auditory-stream-segregation","position":22},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Auditory stream segregation","lvl2":"Melody"},"content":"Auditory stream segregation is the process by which the human auditory system organizes complex mixtures of sounds into perceptually meaningful elements, or “streams.” In music, this allows listeners to distinguish between different melodic lines, instruments, or voices, even when they are played simultaneously. This perceptual organization is influenced by factors such as pitch, timbre, spatial location, and timing. For example, melodies that move in different pitch ranges or have distinct timbres are more likely to be perceived as separate streams. Understanding auditory stream segregation is essential for analyzing polyphonic music, designing effective music information retrieval systems, and developing algorithms for source separation and automatic transcription. Advances in computational modeling and machine learning have enabled researchers to simulate and study how the brain separates and tracks multiple musical streams in real-world listening scenarios.\n\nThe foundational work of psychologist Albert S. Bregman, particularly his book Auditory Scene Analysis (1990), established the theoretical framework for understanding how the auditory system parses complex acoustic environments. Bregman introduced the concept of “auditory scene analysis” (ASA), describing how the brain groups and segregates sounds based on cues such as frequency proximity, temporal continuity, common onset/offset, and timbral similarity. According to Bregman, these grouping principles allow us to perceive coherent musical lines and separate voices in polyphonic music, even when their acoustic signals overlap. His research has had a profound influence on music psychology, cognitive science, and the development of computational models for music and audio processing.","type":"content","url":"/week6#auditory-stream-segregation","position":23},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Gestalt theory","lvl2":"Melody"},"type":"lvl3","url":"/week6#gestalt-theory","position":24},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Gestalt theory","lvl2":"Melody"},"content":"Gestalt theory, originating from early 20th-century psychology, describes how humans naturally organize sensory information into meaningful patterns and wholes. In music perception, Gestalt principles help explain how listeners group sequences of notes into coherent melodies, phrases, and motifs, even when the underlying acoustic signals are complex or ambiguous.\n\nKey Gestalt principles relevant to music include:\n\nProximity: Notes that are close together in time or pitch are perceived as belonging to the same melodic group.\n\nSimilarity: Notes with similar timbre, loudness, or articulation are grouped together.\n\nContinuity: The brain tends to perceive smooth, continuous melodic lines rather than abrupt jumps.\n\nClosure: Listeners mentally “fill in” gaps to perceive complete musical phrases, even if some notes are missing.\n\nFigure-Ground: The ability to focus on a primary melody (figure) while treating accompaniment or background sounds as secondary (ground).\n\nThese principles interact with auditory stream segregation, enabling us to follow individual voices in polyphonic music, recognize recurring themes, and make sense of complex musical textures. Gestalt theory has influenced both music psychology and computational models for music analysis, providing a framework for understanding how we perceive structure and form in music.","type":"content","url":"/week6#gestalt-theory","position":25},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Auditory illusions","lvl2":"Melody"},"type":"lvl3","url":"/week6#auditory-illusions","position":26},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Auditory illusions","lvl2":"Melody"},"content":"Diana Deutsch is a renowned psychologist and researcher who has made significant contributions to the study of auditory illusions and the psychology of music. Her experiments have uncovered a variety of perceptual phenomena that reveal how our brains organize and interpret complex sound patterns. Some of her most famous auditory illusions include:\n\nThe Tritone Paradox: When two tones separated by a tritone (half an octave) are played in succession, some listeners perceive the sequence as ascending in pitch, while others hear it as descending. The direction of the perceived pitch change can vary depending on the listener’s linguistic background and even their geographical origin, suggesting that pitch perception is influenced by both biology and experience.\n\nThe Octave Illusion: When two tones an octave apart are alternately played to each ear (for example, high tone to the right ear, low tone to the left, then switching), listeners often perceive a single tone that alternates between ears and changes pitch, even though both tones are always present. This illusion demonstrates how the brain integrates and separates auditory information from both ears.\n\nThe Scale Illusion: When ascending and descending musical scales are split between the two ears (with some notes sent to the left ear and others to the right), listeners tend to perceive coherent melodic lines that do not correspond to the actual physical input. The brain “reconstructs” the most plausible musical pattern, illustrating its tendency to organize sounds into familiar structures.\n\nPhantom Words: In this illusion, repeating ambiguous speech sounds can cause listeners to “hear” words or phrases that are not actually present. The specific words perceived can vary between individuals, highlighting the role of expectation, language, and context in auditory perception.\n\nDeutsch’s work demonstrates that auditory perception is not a simple reflection of the physical properties of sound, but an active process shaped by cognitive, cultural, and neural factors. Her illusions are widely used in research, education, and demonstrations to illustrate the complexities of how we hear and interpret sound.\n\nFor more examples and audio demonstrations, visit \n\nDiana Deutsch’s Auditory Illusions website.\n\n","type":"content","url":"/week6#auditory-illusions","position":27},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Audio Visualizations"},"type":"lvl2","url":"/week6#audio-visualizations","position":28},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Audio Visualizations"},"content":"We have previously looked at \n\nwaveform displays and \n\nspectrograms. However, there are also several other visualization forms that try to better capture what humans hear.\n\nVisualizing audio is crucial for understanding both the physical properties of sound and how humans perceive it. Different representations highlight various aspects of the audio signal, making them useful for analysis, classification, and creative applications.","type":"content","url":"/week6#audio-visualizations","position":29},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Waveform","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#waveform","position":30},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Waveform","lvl2":"Audio Visualizations"},"content":"A waveform is a simple plot of amplitude versus time. It shows how the air pressure (or voltage, in digital audio) changes over time. While useful for seeing the overall shape and dynamics of a sound, it does not provide detailed information about frequency content.","type":"content","url":"/week6#waveform","position":31},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Spectrogram","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#spectrogram","position":32},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Spectrogram","lvl2":"Audio Visualizations"},"content":"A spectrogram displays how the frequency content of a signal changes over time. It is created by applying the Short-Time Fourier Transform (STFT) to the audio, resulting in a 2D image where the x-axis is time, the y-axis is frequency, and the color represents amplitude (often in decibels). Spectrograms are widely used for audio analysis, speech recognition, and music research.","type":"content","url":"/week6#spectrogram","position":33},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Log Mel Spectrogram","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#log-mel-spectrogram","position":34},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Log Mel Spectrogram","lvl2":"Audio Visualizations"},"content":"The \n\nLog Mel Spectrogram mimics human hearing by applying the STFT, mapping the frequencies to the Mel scale (which is more perceptually relevant), and then applying a logarithmic transformation to represent the amplitude on a decibel scale. This representation compresses the frequency axis to better match how humans perceive pitch differences, making it especially useful in machine learning and audio classification tasks.","type":"content","url":"/week6#log-mel-spectrogram","position":35},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MFCCs (Mel-Frequency Cepstral Coefficients)","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#mfccs-mel-frequency-cepstral-coefficients","position":36},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MFCCs (Mel-Frequency Cepstral Coefficients)","lvl2":"Audio Visualizations"},"content":"MFCCs are a compact representation of the spectral envelope of a sound. They are computed by taking the Log Mel Spectrogram and applying the Discrete Cosine Transform (DCT), which decorrelates the features and compresses the information. MFCCs are widely used in speech recognition, music classification, and audio similarity tasks because they capture timbral characteristics that are important for distinguishing different sounds.","type":"content","url":"/week6#mfccs-mel-frequency-cepstral-coefficients","position":37},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"CQT (Constant-Q Transform)","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#cqt-constant-q-transform","position":38},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"CQT (Constant-Q Transform)","lvl2":"Audio Visualizations"},"content":"The \n\nConstant-Q Transform uses a logarithmic frequency scale, with exponentially spaced center frequencies and varying filter bandwidths. This makes it ideal for musical applications, as it aligns with the way musical notes are spaced (e.g., each octave is divided into equal steps). The CQT is particularly useful for tasks like pitch tracking, chord recognition, and music transcription, as it provides a more musically meaningful frequency representation than the linear STFT.","type":"content","url":"/week6#cqt-constant-q-transform","position":39},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Summary Table","lvl2":"Audio Visualizations"},"type":"lvl3","url":"/week6#summary-table","position":40},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Summary Table","lvl2":"Audio Visualizations"},"content":"Visualization\n\nWhat it shows\n\nTypical Use Cases\n\nWaveform\n\nAmplitude vs. time\n\nEditing, dynamics, onset detection\n\nSpectrogram\n\nFrequency vs. time (linear)\n\nAudio analysis, speech/music research\n\nLog Mel Spectrogram\n\nPerceptual frequency vs. time\n\nMachine learning, audio classification\n\nMFCCs\n\nCompressed spectral envelope\n\nSpeech/music recognition, feature extraction\n\nCQT\n\nMusical pitch vs. time\n\nPitch tracking, chord recognition, MIR\n\nThese visualizations are implemented in Python using libraries such as librosa and matplotlib, as shown in the code below. Each representation provides unique insights into the structure and content of audio signals, supporting both scientific analysis and creative exploration.\n\nimport numpy as np\nimport librosa\n\nimport matplotlib.pyplot as plt\nimport librosa.display\n\n# Generate a test audio signal (sine wave + harmonics)\nsr = 22050  # sample rate\nduration = 2.0  # seconds\nt = np.linspace(0, duration, int(sr * duration), endpoint=False)\nf_start = 0\nf_end = 20000\naudio = 0.5 * np.sin(2 * np.pi * ((f_start + (f_end - f_start) * t / duration) * t))\n\nfig, axs = plt.subplots(3, 2, figsize=(14, 12))\nfig.suptitle('Audio Representations', fontsize=16)\n\n# Waveform\nlibrosa.display.waveshow(audio, sr=sr, ax=axs[0, 0])\naxs[0, 0].set_title('Waveform')\naxs[0, 0].set_xlabel('')\naxs[0, 0].set_ylabel('Amplitude')\n\n# Spectrogram\nS = np.abs(librosa.stft(audio, n_fft=1024, hop_length=256))\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='hz', ax=axs[0, 1])\naxs[0, 1].set_title('Spectrogram (dB)')\n\n# Mel Spectrogram\nS_mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=256, n_mels=64)\nlibrosa.display.specshow(librosa.power_to_db(S_mel, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='mel', ax=axs[1, 0])\naxs[1, 0].set_title('Mel Spectrogram (dB)')\n\n# MFCC\nmfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=1024, hop_length=256)\nlibrosa.display.specshow(mfccs, x_axis='time', ax=axs[1, 1])\naxs[1, 1].set_title('MFCC')\n\n# CQT\nC = np.abs(librosa.cqt(audio, sr=sr, hop_length=256, n_bins=60))\nlibrosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max), sr=sr, hop_length=256, x_axis='time', y_axis='cqt_note', ax=axs[2, 0])\naxs[2, 0].set_title('CQT (dB)')\n\n# Hide the last empty subplot\naxs[2, 1].axis('off')\n\nplt.tight_layout(rect=[0, 0, 1, 0.97])\nplt.show()\n\n\n","type":"content","url":"/week6#summary-table","position":41},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Symbolic Representations"},"type":"lvl2","url":"/week6#symbolic-representations","position":42},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Symbolic Representations"},"content":"Symbolic representations are structured, human- and machine-readable formats that encode musical information such as pitch, rhythm, dynamics, and articulation. These representations are essential for music analysis, composition, generation, and interoperability between software tools. Below are some of the most widely used symbolic formats:","type":"content","url":"/week6#symbolic-representations","position":43},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MIDI","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#midi","position":44},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MIDI","lvl2":"Symbolic Representations"},"content":"MIDI (Musical Instrument Digital Interface) is a standard protocol for communicating musical performance data between electronic instruments and computers. It encodes information such as note pitch, velocity (how hard a note is played), duration, instrument type, and control changes (e.g., modulation, sustain pedal). MIDI files do not contain actual audio but rather instructions for how music should be played, making them compact and widely compatible. MIDI is the backbone of most digital music production environments and is used for sequencing, editing, and playback.\n\nKey features:\n\nEncodes note events (on/off), pitch, velocity, and timing.\n\nSupports multiple channels (instruments) and tracks.\n\nWidely supported by DAWs, synthesizers, and notation software.","type":"content","url":"/week6#midi","position":45},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"ABC Notation","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#abc-notation","position":46},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"ABC Notation","lvl2":"Symbolic Representations"},"content":"ABC Notation is a text-based music notation system that uses ASCII characters to represent musical scores. It is especially popular for folk and traditional music due to its simplicity and ease of sharing via plain text. ABC notation can encode melody, rhythm, lyrics, and basic chords.\n\nKey features:\n\nHuman-readable and easy to edit in any text editor.\n\nSupports simple melodies, chords, and lyrics.\n\nMany online tools exist for converting ABC to sheet music or MIDI.","type":"content","url":"/week6#abc-notation","position":47},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"REMI","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#remi","position":48},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"REMI","lvl2":"Symbolic Representations"},"content":"REMI (REvamped MIDI-derived events) is an enhanced representation of MIDI data designed for deep learning-based music generation. REMI introduces additional event types such as Note Duration, Bar, Position, and Tempo, allowing for a more structured and musically meaningful encoding of rhythm and meter.\n\nKey features:\n\nDesigned for symbolic music generation with neural networks.\n\nEncodes timing, structure, and expressive elements.\n\nFacilitates learning of musical form and rhythm.","type":"content","url":"/week6#remi","position":49},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MusicXML","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#musicxml","position":50},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"MusicXML","lvl2":"Symbolic Representations"},"content":"MusicXML is an XML-based format for representing Western music notation. It encodes detailed musical elements such as notes, rests, articulations, dynamics, lyrics, and layout information. MusicXML is ideal for sharing, analyzing, and archiving sheet music, and is supported by most notation software.\n\nKey features:\n\nEncodes full sheet music, including layout and expressive markings.\n\nSupports complex scores with multiple staves, voices, and instruments.\n\nEnables interoperability between notation programs (e.g., Finale, Sibelius, MuseScore).","type":"content","url":"/week6#musicxml","position":51},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Piano Roll","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#piano-roll","position":52},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Piano Roll","lvl2":"Symbolic Representations"},"content":"The \n\nPiano Roll is a visual representation of music, where time is displayed on the horizontal axis and pitch on the vertical axis. Notes are shown as rectangles, with their position indicating onset, their length indicating duration, and their vertical placement indicating pitch. Piano rolls are commonly used in DAWs for editing MIDI data and visualizing performances.\n\nKey features:\n\nIntuitive, graphical interface for editing MIDI notes.\n\nUseful for sequencing, quantization, and visualization.\n\nDoes not encode expressive markings or complex notation.","type":"content","url":"/week6#piano-roll","position":53},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Note Graph","lvl2":"Symbolic Representations"},"type":"lvl3","url":"/week6#note-graph","position":54},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl3":"Note Graph","lvl2":"Symbolic Representations"},"content":"A \n\nNote Graph is a graph-based representation of musical scores, where nodes represent notes and edges capture relationships such as sequence, onset, and sustain. This approach provides a structured way to analyze and model complex musical relationships, such as polyphony, voice leading, and harmonic context.\n\nKey features:\n\nCaptures relationships between notes beyond simple sequences.\n\nUseful for music analysis, generation, and machine learning.\n\nEnables modeling of complex structures like counterpoint and harmony.\n\nSummary Table\n\nRepresentation\n\nType\n\nTypical Use Cases\n\nStrengths\n\nMIDI\n\nEvent-based\n\nSequencing, playback, editing\n\nCompact, widely supported\n\nABC Notation\n\nText-based\n\nFolk/traditional music, sharing\n\nSimple, human-readable\n\nREMI\n\nEvent-based\n\nAI music generation\n\nStructured, rhythm-aware\n\nMusicXML\n\nXML-based\n\nSheet music, notation, analysis\n\nDetailed, expressive, interoperable\n\nPiano Roll\n\nVisual\n\nMIDI editing, sequencing\n\nIntuitive, easy to manipulate\n\nNote Graph\n\nGraph-based\n\nAnalysis, AI, complex relationships\n\nCaptures structure and context\n\n","type":"content","url":"/week6#note-graph","position":55},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Questions"},"type":"lvl2","url":"/week6#questions","position":56},{"hierarchy":{"lvl1":"Week 6: Harmony and melody","lvl2":"Questions"},"content":"What is the difference between a tone and a note in music psychology?\n\nHow does a spectrogram differ from a waveform in audio visualization?\n\nWhat is the purpose of the Mel Spectrogram and why is it perceptually relevant?\n\nDescribe the concept of harmony and how technology can be used to analyze it.\n\nWhat are the main differences between symbolic representations such as MIDI, ABC Notation, and MusicXML?","type":"content","url":"/week6#questions","position":57},{"hierarchy":{"lvl1":"Week 7: Body Motion"},"type":"lvl1","url":"/week7","position":0},{"hierarchy":{"lvl1":"Week 7: Body Motion"},"content":"","type":"content","url":"/week7","position":1},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"The body in music"},"type":"lvl2","url":"/week7#the-body-in-music","position":2},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"The body in music"},"content":"Music performance is inherently physical, involving coordinated movements of the body to produce sound and convey expression. Musicians rely on posture, gesture, and fine motor control to interact with their instruments, shape musical phrases, and communicate with audiences and fellow performers.","type":"content","url":"/week7#the-body-in-music","position":3},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Physicality and expression","lvl2":"The body in music"},"type":"lvl3","url":"/week7#physicality-and-expression","position":4},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Physicality and expression","lvl2":"The body in music"},"content":"Gesture and emotion: Body movements, such as hand gestures, facial expressions, and posture, play a crucial role in expressing musical ideas and emotions. These gestures can enhance the communicative power of a performance.\n\nTechnique and ergonomics: Proper body alignment and movement are essential for efficient technique and injury prevention. Musicians often train to optimize their posture and motion for better sound production and endurance.\n\nInteraction: In ensemble settings, musicians use visual cues and body language to synchronize timing, dynamics, and phrasing, fostering group cohesion.","type":"content","url":"/week7#physicality-and-expression","position":5},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Research perspectives","lvl2":"The body in music"},"type":"lvl3","url":"/week7#research-perspectives","position":6},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Research perspectives","lvl2":"The body in music"},"content":"Studying the body in music helps researchers understand the relationship between movement and sound, the biomechanics of performance, and the ways physical gestures contribute to musical interpretation. Technologies like motion capture provide valuable insights into these aspects, enabling detailed analysis of how musicians use their bodies to create and shape music.","type":"content","url":"/week7#research-perspectives","position":7},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"The vestibular system"},"type":"lvl2","url":"/week7#the-vestibular-system","position":8},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"The vestibular system"},"content":"The vestibular system is a sensory system located in the inner ear that plays a crucial role in balance, spatial orientation, and coordination of movement. It helps the brain process information about motion, head position, and body posture, allowing individuals to maintain equilibrium and navigate their environment effectively. In the context of music and performance, the vestibular system contributes to a musician’s sense of balance and body awareness, which are essential for precise and expressive movement. For more information, see the \n\nWikipedia article on the vestibular system.","type":"content","url":"/week7#the-vestibular-system","position":9},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Motion capture"},"type":"lvl2","url":"/week7#motion-capture","position":10},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Motion capture"},"content":"Motion capture (often abbreviated as “mocap”) is a technique used to record the movement of objects or people. In software and animation, it’s commonly used to capture the movements of actors and apply them to digital characters.","type":"content","url":"/week7#motion-capture","position":11},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"How it works","lvl2":"Motion capture"},"type":"lvl3","url":"/week7#how-it-works","position":12},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"How it works","lvl2":"Motion capture"},"content":"Preparation: Sensors or reflective markers are placed on key points of a subject, such as joints or limbs. These markers help track the precise movement of each part of the body.\n\nRecording: Multiple cameras or specialized tracking devices are set up around the subject. As the subject moves, the cameras capture the positions of the markers from different angles.\n\nData processing: The recorded data is sent to software that reconstructs the 3D positions of the markers over time. This data is then mapped onto a digital skeleton or model, allowing the virtual character to mimic the real-life movements.\n\nApplications: Motion capture is widely used in film, video games, sports analysis, biomechanics, and virtual reality. It enables realistic animation, performance analysis, and interactive experiences.","type":"content","url":"/week7#how-it-works","position":13},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Types of motion capture","lvl2":"Motion capture"},"type":"lvl3","url":"/week7#types-of-motion-capture","position":14},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Types of motion capture","lvl2":"Motion capture"},"content":"Optical systems: Use cameras and reflective markers to track movement. These systems are common in film and game production.\n\nInertial systems: Use wearable sensors (such as accelerometers and gyroscopes) to measure movement without the need for cameras.\n\nMagnetic systems: Use magnetic fields and sensors to determine position and orientation.\n\nMotion capture technology continues to evolve, making it easier to capture complex movements and apply them to digital environments with high accuracy.","type":"content","url":"/week7#types-of-motion-capture","position":15},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Mocap in music research"},"type":"lvl2","url":"/week7#mocap-in-music-research","position":16},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Mocap in music research"},"content":"Motion capture is increasingly used in music research to study the physical gestures and movements of performers. By tracking body motion, researchers can analyze how musicians interact with their instruments, coordinate with other performers, and express musical ideas through movement.","type":"content","url":"/week7#mocap-in-music-research","position":17},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl4":"Applications","lvl2":"Mocap in music research"},"type":"lvl4","url":"/week7#applications","position":18},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl4":"Applications","lvl2":"Mocap in music research"},"content":"Performance analysis: Mocap helps researchers understand the biomechanics of playing instruments, such as finger, hand, and arm movements in pianists or violinists.\n\nGesture recognition: By capturing expressive gestures, mocap enables the study of how movement relates to musical phrasing, dynamics, and emotion.\n\nInteractive systems: Motion data can be used to control electronic sounds or visual effects in real time, allowing for new forms of musical expression and live performance.\n\nEnsemble coordination: Mocap can reveal how musicians synchronize their movements during group performances, providing insights into communication and timing.","type":"content","url":"/week7#applications","position":19},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl4":"Example studies","lvl2":"Mocap in music research"},"type":"lvl4","url":"/week7#example-studies","position":20},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl4":"Example studies","lvl2":"Mocap in music research"},"content":"Researchers have used mocap to investigate topics such as:\n\nThe relationship between bowing technique and sound production in string players.\n\nThe role of body posture in wind instrument performance.\n\nHow conductors use gestures to communicate tempo and dynamics to an orchestra.\n\nMotion capture provides a powerful tool for bridging the gap between physical movement and musical expression, supporting both scientific research and creative exploration.","type":"content","url":"/week7#example-studies","position":21},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Tools","lvl2":"Mocap in music research"},"type":"lvl3","url":"/week7#tools","position":22},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl3":"Tools","lvl2":"Mocap in music research"},"content":"","type":"content","url":"/week7#tools","position":23},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"References"},"type":"lvl2","url":"/week7#references","position":24},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"References"},"content":"","type":"content","url":"/week7#references","position":25},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Questions"},"type":"lvl2","url":"/week7#questions","position":26},{"hierarchy":{"lvl1":"Week 7: Body Motion","lvl2":"Questions"},"content":"","type":"content","url":"/week7#questions","position":27},{"hierarchy":{"lvl1":"Week 8: The Brain"},"type":"lvl1","url":"/week8","position":0},{"hierarchy":{"lvl1":"Week 8: The Brain"},"content":"","type":"content","url":"/week8","position":1},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Cognition"},"type":"lvl3","url":"/week8#cognition","position":2},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Cognition"},"content":"","type":"content","url":"/week8#cognition","position":3},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl4":"The Brain","lvl3":"Cognition"},"type":"lvl4","url":"/week8#the-brain","position":4},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl4":"The Brain","lvl3":"Cognition"},"content":"Processes auditory information via the cerebral cortex. \n\nLearn more","type":"content","url":"/week8#the-brain","position":5},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl4":"Auditory Cortex","lvl3":"Cognition"},"type":"lvl4","url":"/week8#auditory-cortex","position":6},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl4":"Auditory Cortex","lvl3":"Cognition"},"content":"Responsible for sound processing. \n\nLearn more","type":"content","url":"/week8#auditory-cortex","position":7},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"The brain"},"type":"lvl2","url":"/week8#the-brain-1","position":8},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"The brain"},"content":"The brain is a highly complex organ that serves as the control center for the body’s functions and behaviors. It is composed of billions of neurons that communicate through electrical and chemical signals. These neural networks enable the brain to process sensory information, regulate bodily functions, generate thoughts and emotions, and coordinate movement.\n\nKey regions of the brain include the cerebral cortex, which is responsible for higher-order functions such as reasoning, perception, and decision-making; the limbic system, which manages emotions and memory; and the brainstem, which controls vital functions like breathing and heart rate. The brain’s plasticity allows it to adapt and reorganize in response to experiences, learning, and injury.\n\nOverall, the brain integrates information from both internal and external environments, enabling humans to interact with the world, solve problems, and experience consciousness.","type":"content","url":"/week8#the-brain-1","position":9},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Key Brain Regions for Sound and Music Listening","lvl2":"The brain"},"type":"lvl3","url":"/week8#key-brain-regions-for-sound-and-music-listening","position":10},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Key Brain Regions for Sound and Music Listening","lvl2":"The brain"},"content":"Several specialized regions of the brain are crucial for processing sound and music:\n\nAuditory Cortex: Located in the temporal lobe, the auditory cortex is responsible for decoding basic sound features such as pitch, loudness, and timbre. It plays a central role in recognizing and interpreting musical elements.\n\nPrefrontal Cortex: This area is involved in higher-order cognitive functions, including attention, pattern recognition, and prediction of musical structure.\n\nMotor Cortex: Even when passively listening to music, the motor cortex can be activated, reflecting the brain’s response to rhythm and beat, and its role in coordinating movement.\n\nLimbic System: Comprising structures like the amygdala and hippocampus, the limbic system is essential for emotional responses to music and linking music to memories.\n\nNucleus Accumbens: Part of the brain’s reward system, this region is associated with the pleasurable feelings and motivation that music can evoke.\n\nTogether, these regions enable the perception, emotional experience, and cognitive processing of sound and music.","type":"content","url":"/week8#key-brain-regions-for-sound-and-music-listening","position":11},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Psychology"},"type":"lvl2","url":"/week8#psychology","position":12},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Psychology"},"content":"","type":"content","url":"/week8#psychology","position":13},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Music Perception and Cognition","lvl2":"Psychology"},"type":"lvl3","url":"/week8#music-perception-and-cognition","position":14},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"Music Perception and Cognition","lvl2":"Psychology"},"content":"Music perception and cognition involve complex processes in the brain that integrate sensory, emotional, and cognitive functions. When listening to music, the auditory cortex processes sound waves, identifying pitch, rhythm, and timbre. The prefrontal cortex contributes to recognizing patterns and predicting musical structure, while the motor cortex is often activated, even when passively listening, due to the rhythmic elements of music.\n\nThe limbic system, including the amygdala and hippocampus, plays a key role in the emotional response to music, linking melodies to memories and evoking feelings. Additionally, the reward system, involving the release of dopamine in the nucleus accumbens, is activated during pleasurable musical experiences.\n\nMusic cognition also engages higher-order brain functions, such as attention, memory, and decision-making. Studies have shown that musical training can enhance neuroplasticity, improving cognitive abilities and even aiding in language processing and problem-solving.\n\nResearch continues to explore how these interconnected brain regions work together, shedding light on the profound impact of music on human cognition and emotion.","type":"content","url":"/week8#music-perception-and-cognition","position":15},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Capturing brain activity"},"type":"lvl2","url":"/week8#capturing-brain-activity","position":16},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Capturing brain activity"},"content":"","type":"content","url":"/week8#capturing-brain-activity","position":17},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"EEG (Electroencephalography)","lvl2":"Capturing brain activity"},"type":"lvl3","url":"/week8#eeg-electroencephalography","position":18},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"EEG (Electroencephalography)","lvl2":"Capturing brain activity"},"content":"Electroencephalography (EEG) is a method used to record electrical activity of the brain. It is commonly used in neuroscience, cognitive psychology, and clinical diagnostics. EEG measures voltage fluctuations resulting from ionic current within the neurons of the brain.\n\n\n\nLearn more about EEG on Wikipedia","type":"content","url":"/week8#eeg-electroencephalography","position":19},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"fNIRS (Functional Near-Infrared Spectroscopy)","lvl2":"Capturing brain activity"},"type":"lvl3","url":"/week8#fnirs-functional-near-infrared-spectroscopy","position":20},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"fNIRS (Functional Near-Infrared Spectroscopy)","lvl2":"Capturing brain activity"},"content":"Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive imaging technique that uses near-infrared light to measure brain activity. It is often used in cognitive neuroscience and neurorehabilitation studies.\n\n\n\nLearn more about fNIRS on Wikipedia","type":"content","url":"/week8#fnirs-functional-near-infrared-spectroscopy","position":21},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"MEG (Magnetoencephalography)","lvl2":"Capturing brain activity"},"type":"lvl3","url":"/week8#meg-magnetoencephalography","position":22},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"MEG (Magnetoencephalography)","lvl2":"Capturing brain activity"},"content":"Magnetoencephalography (MEG) is a neuroimaging technique for mapping brain activity by recording magnetic fields produced by electrical currents occurring naturally in the brain. It is particularly useful for studying brain function and connectivity.\n\n\n\nLearn more about MEG on Wikipedia","type":"content","url":"/week8#meg-magnetoencephalography","position":23},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"fMRI (Functional Magnetic Resonance Imaging)","lvl2":"Capturing brain activity"},"type":"lvl3","url":"/week8#fmri-functional-magnetic-resonance-imaging","position":24},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl3":"fMRI (Functional Magnetic Resonance Imaging)","lvl2":"Capturing brain activity"},"content":"Functional Magnetic Resonance Imaging (fMRI) is a technique that measures and maps brain activity by detecting changes associated with blood flow. It is widely used in neuroscience and psychology to study brain function.\n\n\n\nLearn more about fMRI on Wikipedia","type":"content","url":"/week8#fmri-functional-magnetic-resonance-imaging","position":25},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"References"},"type":"lvl2","url":"/week8#references","position":26},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"References"},"content":"","type":"content","url":"/week8#references","position":27},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Questions"},"type":"lvl2","url":"/week8#questions","position":28},{"hierarchy":{"lvl1":"Week 8: The Brain","lvl2":"Questions"},"content":"","type":"content","url":"/week8#questions","position":29},{"hierarchy":{"lvl1":"Week 9: Vision"},"type":"lvl1","url":"/week9","position":0},{"hierarchy":{"lvl1":"Week 9: Vision"},"content":"","type":"content","url":"/week9","position":1},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Audiovisuality"},"type":"lvl2","url":"/week9#audiovisuality","position":2},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Audiovisuality"},"content":"","type":"content","url":"/week9#audiovisuality","position":3},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Physics","lvl2":"Audiovisuality"},"type":"lvl3","url":"/week9#physics","position":4},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Physics","lvl2":"Audiovisuality"},"content":"Light: Light is electromagnetic radiation within the visible spectrum, enabling vision and the perception of color, shape, and movement. It is fundamental to how we interpret our surroundings. Light behaves both as a wave and as a particle (photon), and its properties—such as wavelength and intensity—determine how we perceive brightness and color.\n\nSound: Sound consists of mechanical vibrations that travel through a medium (such as air or water) and are perceived by the auditory system. It underpins communication, music, and environmental awareness. Sound is characterized by properties like frequency (pitch), amplitude (loudness), and timbre (quality).","type":"content","url":"/week9#physics","position":5},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Representation","lvl2":"Audiovisuality"},"type":"lvl3","url":"/week9#representation","position":6},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Representation","lvl2":"Audiovisuality"},"content":"Audio: Audio encompasses the capture, transmission, and reproduction of sound. It is essential in media, communication, and interactive technologies, allowing for the sharing and analysis of auditory information. Audio signals can be analog or digital, and are often processed for clarity, compression, or creative effect.\n\nVideo: Video involves recording, processing, and displaying sequences of moving images. It is a versatile medium for conveying information, storytelling, education, and entertainment. Video signals can be analog or digital, and are defined by parameters such as frame rate, resolution, and color depth.\n\nThese fundamental physical concepts and their representations form the basis for understanding audiovisual systems and technologies, as well as the ways in which humans perceive and interact with their environment.","type":"content","url":"/week9#representation","position":7},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Auditory-visual","lvl3":"Representation","lvl2":"Audiovisuality"},"type":"lvl4","url":"/week9#auditory-visual","position":8},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Auditory-visual","lvl3":"Representation","lvl2":"Audiovisuality"},"content":"Auditory-visual: Auditory-visual integration refers to the brain’s ability to combine auditory and visual information to enhance perception and understanding. This phenomenon is essential in activities like speech comprehension and multimedia experiences. \n\nLearn more on Wikipedia.","type":"content","url":"/week9#auditory-visual","position":9},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Multimodal","lvl3":"Representation","lvl2":"Audiovisuality"},"type":"lvl4","url":"/week9#multimodal","position":10},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Multimodal","lvl3":"Representation","lvl2":"Audiovisuality"},"content":"Multimodal: Multimodal perception involves the integration of information from multiple sensory modalities, such as sight, sound, and touch, to create a cohesive understanding of the environment. \n\nLearn more on Wikipedia.","type":"content","url":"/week9#multimodal","position":11},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Crossmodal","lvl3":"Representation","lvl2":"Audiovisuality"},"type":"lvl4","url":"/week9#crossmodal","position":12},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Crossmodal","lvl3":"Representation","lvl2":"Audiovisuality"},"content":"McGurk effect: The McGurk effect is a perceptual phenomenon where conflicting auditory and visual stimuli result in a third, distinct perception. It highlights the complex interplay between sensory modalities. \n\nLearn more on Wikipedia.","type":"content","url":"/week9#crossmodal","position":13},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Psychology"},"type":"lvl2","url":"/week9#psychology","position":14},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Psychology"},"content":"","type":"content","url":"/week9#psychology","position":15},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Gaze","lvl2":"Psychology"},"type":"lvl3","url":"/week9#gaze","position":16},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Gaze","lvl2":"Psychology"},"content":"Gaze refers to the direction in which a person is looking, often used as an indicator of attention and focus. It plays a crucial role in understanding human behavior, communication, and cognitive processes. Gaze tracking is commonly used in psychological studies to analyze visual attention and social interactions. \n\nLearn more on Wikipedia.","type":"content","url":"/week9#gaze","position":17},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Pupillometry","lvl2":"Psychology"},"type":"lvl3","url":"/week9#pupillometry","position":18},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Pupillometry","lvl2":"Psychology"},"content":"Pupillometry is the measurement of pupil size and reactivity, often used to study cognitive and emotional processes. Changes in pupil size can indicate arousal, attention, and mental effort. This technique is widely applied in psychology, neuroscience, and human-computer interaction research. \n\nLearn more on Wikipedia.","type":"content","url":"/week9#pupillometry","position":19},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Technology"},"type":"lvl2","url":"/week9#technology","position":20},{"hierarchy":{"lvl1":"Week 9: Vision","lvl2":"Technology"},"content":"","type":"content","url":"/week9#technology","position":21},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Eye-Trackers","lvl2":"Technology"},"type":"lvl3","url":"/week9#eye-trackers","position":22},{"hierarchy":{"lvl1":"Week 9: Vision","lvl3":"Eye-Trackers","lvl2":"Technology"},"content":"Eye-trackers are devices used to measure eye positions and movements. They are widely used in research fields such as psychology, neuroscience, marketing, and human-computer interaction. Eye-trackers can be categorized into two main types: mobile and stationary.","type":"content","url":"/week9#eye-trackers","position":23},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Mobile Eye-Trackers","lvl3":"Eye-Trackers","lvl2":"Technology"},"type":"lvl4","url":"/week9#mobile-eye-trackers","position":24},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Mobile Eye-Trackers","lvl3":"Eye-Trackers","lvl2":"Technology"},"content":"Mobile eye-trackers are wearable devices that allow for the tracking of eye movements in real-world environments. These devices are often used in studies that require participants to move freely, such as sports performance analysis, usability testing, and outdoor experiments. \n\nLearn more on Wikipedia.\n\n\nFigure 1: Example of mobile eye-tracking glasses. Image credit: Wikimedia Commons.","type":"content","url":"/week9#mobile-eye-trackers","position":25},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Stationary Eye-Trackers","lvl3":"Eye-Trackers","lvl2":"Technology"},"type":"lvl4","url":"/week9#stationary-eye-trackers","position":26},{"hierarchy":{"lvl1":"Week 9: Vision","lvl4":"Stationary Eye-Trackers","lvl3":"Eye-Trackers","lvl2":"Technology"},"content":"Stationary eye-trackers are fixed devices typically used in controlled laboratory settings. They are often mounted on a desk or integrated into a monitor and are used for tasks such as reading studies, visual search experiments, and website usability testing. \n\nLearn more on Wikipedia.\n\n\nFigure 2: Example of stationary eye-tracking software. Image credit: Wikimedia Commons.","type":"content","url":"/week9#stationary-eye-trackers","position":27}]}